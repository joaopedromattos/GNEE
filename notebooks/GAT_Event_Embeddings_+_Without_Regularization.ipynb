{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAT Event Embeddings + Without Regularization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "29d239a1a54e47daad86c14eb50c1964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_edb9768e7462436298c3ac40458723cb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_11725a65f2f740f699af0f66aeb1e66f",
              "IPY_MODEL_0ececd84539544cb9231df27b6efb5db"
            ]
          }
        },
        "edb9768e7462436298c3ac40458723cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "11725a65f2f740f699af0f66aeb1e66f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_60c9d1ad6d2a4eae8e2620c9070760b7",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 50,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 50,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ac91fe70cf194c558a1325f1dd353557"
          }
        },
        "0ececd84539544cb9231df27b6efb5db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cbd0ae4643d14d38948e466fde3bfe9b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 50/50 [05:33&lt;00:00,  6.66s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5667d4a876b442f2bc3215b5be3db7b5"
          }
        },
        "60c9d1ad6d2a4eae8e2620c9070760b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ac91fe70cf194c558a1325f1dd353557": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cbd0ae4643d14d38948e466fde3bfe9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5667d4a876b442f2bc3215b5be3db7b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNq4clhCly9v"
      },
      "source": [
        "# GAT Event Embedding\n",
        "This notebook establishes a training pipeline for our Event Embedding model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs6EH6Ufysax"
      },
      "source": [
        "## Installing our libraries and required scripts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EuLsjGYmA8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee7f5e2-9e82-44ee-b20f-afec664dfbd9"
      },
      "source": [
        "!git clone https://github.com/joaopedromattos/pyGAT\n",
        "!pip install --quiet spektral"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pyGAT'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 213 (delta 26), reused 34 (delta 13), pack-reused 163\u001b[K\n",
            "Receiving objects: 100% (213/213), 328.27 KiB | 325.00 KiB/s, done.\n",
            "Resolving deltas: 100% (115/115), done.\n",
            "\u001b[K     |████████████████████████████████| 112kB 11.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF-BO7e-AUqA",
        "outputId": "595f9832-f8ff-473c-8427-2e7ba53e137b"
      },
      "source": [
        "!pip install git+https://github.com/rmarcacini/sentence-transformers\r\n",
        "!pip install gdown\r\n",
        "!gdown https://drive.google.com/uc?id=1NV5t1YhyyOzMF5zAovfbSLdZZLvqrfZ_\r\n",
        "!unzip distiluse-base-multilingual-cased.zip -d language_model\r\n",
        "from sentence_transformers import SentenceTransformer, LoggingHandler\r\n",
        "language_model = SentenceTransformer('distiluse-base-multilingual-cased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/rmarcacini/sentence-transformers\n",
            "  Cloning https://github.com/rmarcacini/sentence-transformers to /tmp/pip-req-build-id3vp4vc\n",
            "  Running command git clone -q https://github.com/rmarcacini/sentence-transformers /tmp/pip-req-build-id3vp4vc\n",
            "Collecting transformers<3.2.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.6) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.6) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.6) (1.19.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.6) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.6) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.6) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (0.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 31.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (20.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (2019.12.20)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 48.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->sentence-transformers==0.3.6) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->sentence-transformers==0.3.6) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers==0.3.6) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers==0.3.6) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (2.4.7)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.6-cp36-none-any.whl size=101875 sha256=f414b67ed4fc3ed0f83be8fa1c5bf306a53ddcf7be0ef2697d7e8a82c0afeb9e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z5tn53kn/wheels/88/3c/66/55ee9fb698480d5a5116a8257c15dc363323e4922fb8ad361b\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=542636057da873ec7cce7240d016a9b169a75f3340de4f0d01a08b5015459936\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.3.6 sentencepiece-0.1.94 tokenizers-0.8.1rc2 transformers-3.1.0\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.10)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NV5t1YhyyOzMF5zAovfbSLdZZLvqrfZ_\n",
            "To: /content/distiluse-base-multilingual-cased.zip\n",
            "504MB [00:04, 125MB/s]\n",
            "Archive:  distiluse-base-multilingual-cased.zip\n",
            "   creating: language_model/0_DistilBERT/\n",
            " extracting: language_model/0_DistilBERT/added_tokens.json  \n",
            "  inflating: language_model/0_DistilBERT/config.json  \n",
            "  inflating: language_model/0_DistilBERT/pytorch_model.bin  \n",
            "  inflating: language_model/0_DistilBERT/sentence_distilbert_config.json  \n",
            "  inflating: language_model/0_DistilBERT/special_tokens_map.json  \n",
            "  inflating: language_model/0_DistilBERT/tokenizer_config.json  \n",
            "  inflating: language_model/0_DistilBERT/vocab.txt  \n",
            "   creating: language_model/1_Pooling/\n",
            "  inflating: language_model/1_Pooling/config.json  \n",
            "   creating: language_model/2_Dense/\n",
            "  inflating: language_model/2_Dense/config.json  \n",
            "  inflating: language_model/2_Dense/pytorch_model.bin  \n",
            " extracting: language_model/config.json  \n",
            "  inflating: language_model/modules.json  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 504M/504M [00:08<00:00, 56.0MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7djUzxZiokMc"
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir('./pyGAT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjRxJgFJmK0r"
      },
      "source": [
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import bigquery_storage\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, LabelEncoder\n",
        "from event_graph_utils import mount_graph, regularization, process_event_dataset_from_networkx\n",
        "\n",
        "# auth.authenticate_user()\n",
        "# print('Authenticated')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5-euv7HvyYd"
      },
      "source": [
        "## 5W1H Graph Events"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qysvvsEI5xMQ",
        "outputId": "6355370e-49b6-4523-c45d-6dd0b1d1bcf1"
      },
      "source": [
        "!gdown --id 1RF_bIo5ndxPhu9SJw-T8HBcuHyaGQGL0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RF_bIo5ndxPhu9SJw-T8HBcuHyaGQGL0\n",
            "To: /content/pyGAT/datasets.tar.gz\n",
            "22.7MB [00:00, 37.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro2OXREKBTp3",
        "outputId": "2b93bf35-0c77-491e-f5cb-d6da2b8682cd"
      },
      "source": [
        "!tar -xzvf datasets.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets_runs/\n",
            "datasets_runs/run_1_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_6_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_4_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_8_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_5_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_9_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_5_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_2_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_9_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_7_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_9_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_8_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_10_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_8_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_2_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_8_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_6_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_4_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_2_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_7_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_4_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_5_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_3_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_4_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_5_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_10_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_10_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_9_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_10_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_6_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_1_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_3_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_5_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_4_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_7_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_1_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_7_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_10_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_6_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_3_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_1_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_2_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_3_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_9_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_2_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_8_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_6_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_1_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_7_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_3_40er_5w1h_graph_hin.nx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOcBquMl0JvW"
      },
      "source": [
        "# Features from Adj Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcHszT5YJ0mu"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "import networkx as nx\n",
        "\n",
        "\n",
        "def features_by_adj(G):\n",
        "\n",
        "    nodes = []\n",
        "\n",
        "    for node in G.nodes():\n",
        "        nodes.append(node)\n",
        "\n",
        "    adj_matrix = nx.adjacency_matrix(G, nodelist=nodes).todense()\n",
        "\n",
        "    counter = 0\n",
        "    for node in nodes:\n",
        "      G.nodes[node]['f'] = np.array(adj_matrix[counter].tolist()[0])\n",
        "      counter += 1\n",
        "\n",
        "    return G"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5uOBWeVnaKG"
      },
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import logging\n",
        "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def process_event_dataset_from_networkx(G, features_attr=\"f\"):\n",
        "    \"\"\"\n",
        "    Builds an event graph dataset used in GAT model\n",
        "    Parameters:\n",
        "        G -> Graph representation of the event network (Networkx graph)\n",
        "        df_labels -> user labeled data\n",
        "        features_att -> Feature attribute of each node (str)\n",
        "        random_state -> A random seed to train_test_split\n",
        "    Returns:\n",
        "        adj -> Sparse and symmetric adjacency matrix of our graph.\n",
        "        features -> A NumPy matrix with our graph features.\n",
        "        idx_train -> A NumPy array with the indexes of the training nodes.\n",
        "        idx_val -> A NumPy array with the indexes of the validation nodes.\n",
        "        idx_test -> A NumPy array with the indexes of the test nodes.\n",
        "    \"\"\"\n",
        "\n",
        "    num_nodes = len(G.nodes)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    # validation_split_percentage = val_split / (1 - train_split)\n",
        "\n",
        "    # df_val, df_test = train_test_split(\n",
        "    #     df_test_and_val, train_size=validation_split_percentage, random_state=random_state)\n",
        "\n",
        "    # Organizing our feature matrix...\n",
        "    # feature_matrix = np.array([ G.nodes[i]['embedding'] if 'embedding' in G.nodes[i].keys() else G.nodes[i][features_attr] for i in G.nodes()])\n",
        "    #features = np.array([G.nodes[i][features_attr] for i in G.nodes()])\n",
        "    L_features = []\n",
        "    L_train = []\n",
        "    L_test = []\n",
        "    L_labels = []\n",
        "    label_codes = {}\n",
        "    for node in G.nodes():\n",
        "      L_features.append( (G.nodes[node]['id'], G.nodes[node]['f']) )\n",
        "      if 'train' in G.nodes[node]: L_train.append(G.nodes[node]['id'])\n",
        "      if 'test' in G.nodes[node]: L_test.append(G.nodes[node]['id'])\n",
        "      if 'label' in G.nodes[node]:\n",
        "        if G.nodes[node]['label'] not in label_codes: label_codes[G.nodes[node]['label']] = len(label_codes) \n",
        "        L_labels.append( [G.nodes[node]['id'],G.nodes[node]['label'],label_codes[G.nodes[node]['label']]] )\n",
        "    df_features = pd.DataFrame(L_features)\n",
        "    df_features.columns = ['node_id','embedding']\n",
        "    features = np.array(df_features.sort_values(by=['node_id'])['embedding'].to_list())\n",
        "\n",
        "    idx_train = L_train\n",
        "    idx_test = L_test\n",
        "    labels = [-1]*num_nodes\n",
        "    df_labels = pd.DataFrame(L_labels)\n",
        "    df_labels.columns = ['event_id','label','label_code']\n",
        "    for index,row in df_labels.iterrows():\n",
        "      labels[row['event_id']] = row['label_code']\n",
        "\n",
        "    adj = nx.adjacency_matrix(G)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_test, df_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75xhOPSqpoBh"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFKgSa0S5ftA"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRFC2tNBwBpR"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBZoIrwm0mbZ"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import scipy.sparse as sp\n",
        "\n",
        "\n",
        "from models import GAT, SpGAT\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "def normalize_adj(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "class Namespace(object):\n",
        "    def __init__(self, adict):\n",
        "        self.__dict__.update(adict)\n",
        "\n",
        "class GAT_wrapper():\n",
        "    def __init__(self, args={\"alpha\": 0.2, \"cuda\": True, \"dropout\": 0.6, \"epochs\": 10, \"fastmode\": False, \"hidden\": 8, \"lr\": 0.005, \"nb_heads\": 8, \"no_cuda\": False, \"patience\": 100, \"seed\": 72, \"sparse\": False, \"weight_decay\": 0.0005}):\n",
        "\n",
        "        if (type(args) == dict):\n",
        "            args = Namespace(args)\n",
        "\n",
        "        self.args = args\n",
        "\n",
        "        self.model = None\n",
        "\n",
        "        self.loss_test = 0.0\n",
        "        self.acc_test = 0.0\n",
        "\n",
        "        self.adj = None\n",
        "        self.features = None\n",
        "        self.labels = None\n",
        "        self.idx_train = None\n",
        "        self.idx_val = None\n",
        "        self.idx_test = None\n",
        "\n",
        "    def compute_test(self):\n",
        "        self.model.eval()\n",
        "        output = self.model(self.features, self.adj)\n",
        "        loss_test = F.nll_loss(\n",
        "            output[self.idx_test], self.labels[self.idx_test])\n",
        "        acc_test = accuracy(output[self.idx_test], self.labels[self.idx_test])\n",
        "        print(\"Test set results:\",\n",
        "              \"loss= {:.4f}\".format(loss_test.item()),\n",
        "              \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "        self.loss_test = loss_test\n",
        "        self.acc_test = acc_test\n",
        "\n",
        "        return loss_test, acc_test, output[self.idx_test].max(1)[1]\n",
        "\n",
        "    def train_pipeline(self, adj, features, labels, idx_train, idx_val, idx_test, *args):\n",
        "\n",
        "        adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "        if (sp.issparse(adj)):\n",
        "            adj = adj.todense()\n",
        "\n",
        "        if (sp.issparse(features)):\n",
        "            features = features.todense()\n",
        "\n",
        "        # With networkx, we no longer need to convert from one-hot encoding...\n",
        "        #labels = np.where(labels)[1]\n",
        "\n",
        "        adj = torch.FloatTensor(adj)\n",
        "        features = torch.FloatTensor(features)\n",
        "        labels = torch.LongTensor(labels)\n",
        "        idx_train = torch.LongTensor(idx_train)\n",
        "        idx_val = torch.LongTensor(idx_val)\n",
        "        idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "        random.seed(self.args.seed)\n",
        "        np.random.seed(self.args.seed)\n",
        "        torch.manual_seed(self.args.seed)\n",
        "        if self.args.cuda:\n",
        "            torch.cuda.manual_seed(self.args.seed)\n",
        "\n",
        "        # Load data\n",
        "        # adj, features, labels, idx_train, idx_val, idx_test = new_load_data(\n",
        "        #     *args, custom_function=custom_function, function=function)\n",
        "\n",
        "        # Model and optimizer\n",
        "        if self.args.sparse:\n",
        "            model = SpGAT(nfeat=features.shape[1],\n",
        "                          nhid=self.args.hidden,\n",
        "                          nclass=int(labels.max()) + 1,\n",
        "                          dropout=self.args.dropout,\n",
        "                          nheads=self.args.nb_heads,\n",
        "                          alpha=self.args.alpha)\n",
        "        else:\n",
        "            model = GAT(nfeat=features.shape[1],\n",
        "                        nhid=self.args.hidden,\n",
        "                        nclass=int(labels.max()) + 1,\n",
        "                        dropout=self.args.dropout,\n",
        "                        nheads=self.args.nb_heads,\n",
        "                        alpha=self.args.alpha)\n",
        "        optimizer = optim.Adam(model.parameters(),\n",
        "                               lr=self.args.lr,\n",
        "                               weight_decay=self.args.weight_decay)\n",
        "\n",
        "        if self.args.cuda:\n",
        "            model.cuda()\n",
        "            features = features.cuda()\n",
        "            adj = adj.cuda()\n",
        "            labels = labels.cuda()\n",
        "            idx_train = idx_train.cuda()\n",
        "            idx_val = idx_val.cuda()\n",
        "            idx_test = idx_test.cuda()\n",
        "\n",
        "        features, adj, labels = Variable(\n",
        "            features), Variable(adj), Variable(labels)\n",
        "\n",
        "        # TODO: Test if these lines could be written below line 41.\n",
        "        self.adj = adj\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.idx_train = idx_train\n",
        "        self.idx_val = idx_val\n",
        "        self.idx_test = idx_test\n",
        "\n",
        "        def train(epoch):\n",
        "            t = time.time()\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            output = model(features, adj)\n",
        "            loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "            acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if not self.args.fastmode:\n",
        "                # Evaluate validation set performance separately,\n",
        "                # deactivates dropout during validation run.\n",
        "                model.eval()\n",
        "                output = model(features, adj)\n",
        "\n",
        "            loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "            acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "            print('Epoch: {:04d}'.format(epoch+1),\n",
        "                  'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
        "                  'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
        "                  'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
        "                  'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
        "                  'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "            return loss_val.data.item()\n",
        "\n",
        "        # Train model\n",
        "        t_total = time.time()\n",
        "        loss_values = []\n",
        "        bad_counter = 0\n",
        "        best = self.args.epochs + 1\n",
        "        best_epoch = 0\n",
        "        for epoch in range(self.args.epochs):\n",
        "            loss_values.append(train(epoch))\n",
        "\n",
        "            torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "            if loss_values[-1] < best:\n",
        "                best = loss_values[-1]\n",
        "                best_epoch = epoch\n",
        "                bad_counter = 0\n",
        "            else:\n",
        "                bad_counter += 1\n",
        "\n",
        "            if bad_counter == self.args.patience:\n",
        "                break\n",
        "\n",
        "            files = glob.glob('*.pkl')\n",
        "            for file in files:\n",
        "                epoch_nb = int(file.split('.')[0])\n",
        "                if epoch_nb < best_epoch:\n",
        "                    os.remove(file)\n",
        "\n",
        "        files = glob.glob('*.pkl')\n",
        "        for file in files:\n",
        "            epoch_nb = int(file.split('.')[0])\n",
        "            if epoch_nb > best_epoch:\n",
        "                os.remove(file)\n",
        "\n",
        "        print(\"Optimization Finished!\")\n",
        "        print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "        # Restore best model\n",
        "        print('Loading {}th epoch'.format(best_epoch))\n",
        "        model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpuSQrky0srn",
        "outputId": "20a9d7f4-185e-4c78-868a-8d8b00363fa6"
      },
      "source": [
        "from os import listdir\r\n",
        "from os.path import isfile, join\r\n",
        "path_datasets = 'datasets_runs/'\r\n",
        "network_files = [f for f in listdir(path_datasets) if isfile(join(path_datasets, f))]\r\n",
        "print(network_files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['run_1_google_news_5w1h_graph_hin.nx', 'run_6_40er_5w1h_graph_hin.nx', 'run_4_bbc_5w1h_graph_hin.nx', 'run_8_gold_standard_5w1h_graph_hin.nx', 'run_5_bbc_5w1h_graph_hin.nx', 'run_9_google_news_5w1h_graph_hin.nx', 'run_5_gold_standard_5w1h_graph_hin.nx', 'run_2_bbc_5w1h_graph_hin.nx', 'run_9_news_cluster_5w1h_graph_hin.nx', 'run_7_40er_5w1h_graph_hin.nx', 'run_9_gold_standard_5w1h_graph_hin.nx', 'run_8_google_news_5w1h_graph_hin.nx', 'run_10_bbc_5w1h_graph_hin.nx', 'run_8_news_cluster_5w1h_graph_hin.nx', 'run_2_news_cluster_5w1h_graph_hin.nx', 'run_8_40er_5w1h_graph_hin.nx', 'run_6_bbc_5w1h_graph_hin.nx', 'run_4_google_news_5w1h_graph_hin.nx', 'run_2_google_news_5w1h_graph_hin.nx', 'run_7_gold_standard_5w1h_graph_hin.nx', 'run_4_gold_standard_5w1h_graph_hin.nx', 'run_5_40er_5w1h_graph_hin.nx', 'run_3_gold_standard_5w1h_graph_hin.nx', 'run_4_40er_5w1h_graph_hin.nx', 'run_5_google_news_5w1h_graph_hin.nx', 'run_10_news_cluster_5w1h_graph_hin.nx', 'run_10_40er_5w1h_graph_hin.nx', 'run_9_40er_5w1h_graph_hin.nx', 'run_10_google_news_5w1h_graph_hin.nx', 'run_6_google_news_5w1h_graph_hin.nx', 'run_1_news_cluster_5w1h_graph_hin.nx', 'run_3_news_cluster_5w1h_graph_hin.nx', 'run_5_news_cluster_5w1h_graph_hin.nx', 'run_4_news_cluster_5w1h_graph_hin.nx', 'run_7_bbc_5w1h_graph_hin.nx', 'run_1_gold_standard_5w1h_graph_hin.nx', 'run_7_google_news_5w1h_graph_hin.nx', 'run_10_gold_standard_5w1h_graph_hin.nx', 'run_6_gold_standard_5w1h_graph_hin.nx', 'run_3_bbc_5w1h_graph_hin.nx', 'run_1_bbc_5w1h_graph_hin.nx', 'run_2_gold_standard_5w1h_graph_hin.nx', 'run_3_google_news_5w1h_graph_hin.nx', 'run_9_bbc_5w1h_graph_hin.nx', 'run_2_40er_5w1h_graph_hin.nx', 'run_8_bbc_5w1h_graph_hin.nx', 'run_6_news_cluster_5w1h_graph_hin.nx', 'run_1_40er_5w1h_graph_hin.nx', 'run_7_news_cluster_5w1h_graph_hin.nx', 'run_3_40er_5w1h_graph_hin.nx']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6fcOAgy1B9k",
        "outputId": "0c364d50-409d-4dd6-9c2f-75eefaa4a4fe"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pyGAT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxsy4972pyvK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "29d239a1a54e47daad86c14eb50c1964",
            "edb9768e7462436298c3ac40458723cb",
            "11725a65f2f740f699af0f66aeb1e66f",
            "0ececd84539544cb9231df27b6efb5db",
            "60c9d1ad6d2a4eae8e2620c9070760b7",
            "ac91fe70cf194c558a1325f1dd353557",
            "cbd0ae4643d14d38948e466fde3bfe9b",
            "5667d4a876b442f2bc3215b5be3db7b5"
          ]
        },
        "outputId": "0c98a798-7936-4be9-b1d5-8f6522bcb63c"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "experimental_results = []\n",
        "\n",
        "for network_file in tqdm(network_files):\n",
        "\n",
        "  if 'news_cluster_5w1h_graph_hin.nx' in network_file: continue # usando toda a RAM as vezes???\n",
        "  print('Networkfile',network_file)\n",
        "  \n",
        "\n",
        "  G = nx.read_gpickle(path_datasets+network_file)\n",
        "\n",
        "  features_by_adj(G)\n",
        "  adj, features, labels, idx_train, idx_test, df_labels = process_event_dataset_from_networkx(G)\n",
        "  print(adj.shape,features.shape,len(idx_train),len(idx_test))\n",
        "  gat = GAT_wrapper({\"alpha\": 0.2, \"cuda\": False, \"dropout\": 0.5, \"epochs\": 20, \"fastmode\": False, \"hidden\": 8, \"lr\": 0.005, \"nb_heads\": 8, \"no_cuda\": False, \"patience\": 100, \"seed\": 72, \"sparse\": False, \"weight_decay\": 0.0005})\n",
        "  gat.train_pipeline(adj, features, labels, idx_train, idx_train, idx_test)\n",
        "  loss, acc, output = gat.compute_test()\n",
        "  y_pred = output.numpy()\n",
        "  y_true = []\n",
        "  for event_id in idx_test:\n",
        "    for node in G.nodes():\n",
        "      if ':event' in node:\n",
        "        if G.nodes[node]['id'] == event_id:\n",
        "          y_true.append(df_labels[df_labels.event_id==event_id].label_code.values[0])\n",
        "\n",
        "  f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "  print('--->' ,network_file,'f1_macro',f1_macro,'acc',acc)\n",
        "  experimental_results.append((network_file,'f1_macro',f1_macro,'acc',acc,y_true,y_pred))\n",
        "  del gat\n",
        "  del adj\n",
        "  del features\n",
        "  del G"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29d239a1a54e47daad86c14eb50c1964",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Networkfile run_1_google_news_5w1h_graph_hin.nx\n",
            "(227, 227) (227, 227) 7 25\n",
            "Epoch: 0001 loss_train: 2.0023 acc_train: 0.1429 loss_val: 1.8686 acc_val: 0.4286 time: 0.3335s\n",
            "Epoch: 0002 loss_train: 1.9403 acc_train: 0.1429 loss_val: 1.8033 acc_val: 0.5714 time: 0.1902s\n",
            "Epoch: 0003 loss_train: 1.5676 acc_train: 0.5714 loss_val: 1.7377 acc_val: 0.7143 time: 0.1729s\n",
            "Epoch: 0004 loss_train: 1.9834 acc_train: 0.1429 loss_val: 1.6764 acc_val: 0.7143 time: 0.1896s\n",
            "Epoch: 0005 loss_train: 1.7537 acc_train: 0.2857 loss_val: 1.6108 acc_val: 0.7143 time: 0.1563s\n",
            "Epoch: 0006 loss_train: 1.7703 acc_train: 0.2857 loss_val: 1.5440 acc_val: 0.7143 time: 0.1723s\n",
            "Epoch: 0007 loss_train: 1.6440 acc_train: 0.5714 loss_val: 1.4786 acc_val: 0.7143 time: 0.1897s\n",
            "Epoch: 0008 loss_train: 1.4287 acc_train: 0.4286 loss_val: 1.4157 acc_val: 0.7143 time: 0.1725s\n",
            "Epoch: 0009 loss_train: 1.7376 acc_train: 0.5714 loss_val: 1.3508 acc_val: 0.7143 time: 0.1727s\n",
            "Epoch: 0010 loss_train: 1.3853 acc_train: 0.4286 loss_val: 1.2851 acc_val: 0.7143 time: 0.1724s\n",
            "Epoch: 0011 loss_train: 1.1456 acc_train: 0.5714 loss_val: 1.2204 acc_val: 0.7143 time: 0.1709s\n",
            "Epoch: 0012 loss_train: 1.3963 acc_train: 0.5714 loss_val: 1.1590 acc_val: 0.7143 time: 0.1699s\n",
            "Epoch: 0013 loss_train: 1.6350 acc_train: 0.2857 loss_val: 1.0991 acc_val: 0.7143 time: 0.1763s\n",
            "Epoch: 0014 loss_train: 0.9369 acc_train: 0.7143 loss_val: 1.0410 acc_val: 0.8571 time: 0.1647s\n",
            "Epoch: 0015 loss_train: 1.2444 acc_train: 0.7143 loss_val: 0.9869 acc_val: 0.8571 time: 0.1657s\n",
            "Epoch: 0016 loss_train: 1.1514 acc_train: 0.7143 loss_val: 0.9350 acc_val: 1.0000 time: 0.1514s\n",
            "Epoch: 0017 loss_train: 1.2853 acc_train: 0.7143 loss_val: 0.8839 acc_val: 1.0000 time: 0.1846s\n",
            "Epoch: 0018 loss_train: 1.1868 acc_train: 0.7143 loss_val: 0.8354 acc_val: 1.0000 time: 0.1704s\n",
            "Epoch: 0019 loss_train: 1.2804 acc_train: 0.7143 loss_val: 0.7891 acc_val: 1.0000 time: 0.1634s\n",
            "Epoch: 0020 loss_train: 1.5004 acc_train: 0.8571 loss_val: 0.7439 acc_val: 1.0000 time: 0.1674s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.8753s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.4916 accuracy= 0.4800\n",
            "---> run_1_google_news_5w1h_graph_hin.nx f1_macro 0.4428571428571428 acc 0.48\n",
            "Networkfile run_6_40er_5w1h_graph_hin.nx\n",
            "(249, 249) (249, 249) 8 32\n",
            "Epoch: 0001 loss_train: 1.1261 acc_train: 0.2500 loss_val: 0.9851 acc_val: 0.5000 time: 0.2119s\n",
            "Epoch: 0002 loss_train: 1.2466 acc_train: 0.2500 loss_val: 0.9328 acc_val: 0.7500 time: 0.2005s\n",
            "Epoch: 0003 loss_train: 1.5138 acc_train: 0.5000 loss_val: 0.8845 acc_val: 0.7500 time: 0.1943s\n",
            "Epoch: 0004 loss_train: 1.3276 acc_train: 0.5000 loss_val: 0.8340 acc_val: 0.8750 time: 0.1877s\n",
            "Epoch: 0005 loss_train: 1.0152 acc_train: 0.5000 loss_val: 0.7814 acc_val: 0.8750 time: 0.1748s\n",
            "Epoch: 0006 loss_train: 1.0760 acc_train: 0.3750 loss_val: 0.7300 acc_val: 0.8750 time: 0.2016s\n",
            "Epoch: 0007 loss_train: 0.9168 acc_train: 0.6250 loss_val: 0.6789 acc_val: 0.8750 time: 0.2079s\n",
            "Epoch: 0008 loss_train: 0.8147 acc_train: 0.5000 loss_val: 0.6301 acc_val: 0.8750 time: 0.1770s\n",
            "Epoch: 0009 loss_train: 0.8106 acc_train: 0.7500 loss_val: 0.5836 acc_val: 0.8750 time: 0.1878s\n",
            "Epoch: 0010 loss_train: 1.0019 acc_train: 0.5000 loss_val: 0.5444 acc_val: 0.8750 time: 0.1860s\n",
            "Epoch: 0011 loss_train: 0.8839 acc_train: 0.6250 loss_val: 0.5049 acc_val: 0.8750 time: 0.2006s\n",
            "Epoch: 0012 loss_train: 0.8479 acc_train: 0.5000 loss_val: 0.4671 acc_val: 0.8750 time: 0.1944s\n",
            "Epoch: 0013 loss_train: 0.9851 acc_train: 0.7500 loss_val: 0.4327 acc_val: 0.8750 time: 0.1932s\n",
            "Epoch: 0014 loss_train: 0.3992 acc_train: 0.8750 loss_val: 0.4000 acc_val: 0.8750 time: 0.1839s\n",
            "Epoch: 0015 loss_train: 0.7149 acc_train: 0.7500 loss_val: 0.3690 acc_val: 1.0000 time: 0.1833s\n",
            "Epoch: 0016 loss_train: 0.4276 acc_train: 1.0000 loss_val: 0.3403 acc_val: 1.0000 time: 0.2090s\n",
            "Epoch: 0017 loss_train: 0.3639 acc_train: 1.0000 loss_val: 0.3138 acc_val: 1.0000 time: 0.2034s\n",
            "Epoch: 0018 loss_train: 0.4389 acc_train: 0.8750 loss_val: 0.2895 acc_val: 1.0000 time: 0.2259s\n",
            "Epoch: 0019 loss_train: 0.3600 acc_train: 1.0000 loss_val: 0.2679 acc_val: 1.0000 time: 0.2176s\n",
            "Epoch: 0020 loss_train: 0.4639 acc_train: 0.8750 loss_val: 0.2473 acc_val: 1.0000 time: 0.2232s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 4.2522s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 0.5675 accuracy= 0.8438\n",
            "---> run_6_40er_5w1h_graph_hin.nx f1_macro 0.7156659765355418 acc 0.84375\n",
            "Networkfile run_4_bbc_5w1h_graph_hin.nx\n",
            "(392, 392) (392, 392) 11 44\n",
            "Epoch: 0001 loss_train: 1.8115 acc_train: 0.2727 loss_val: 1.5432 acc_val: 0.4545 time: 0.4570s\n",
            "Epoch: 0002 loss_train: 1.5978 acc_train: 0.2727 loss_val: 1.4727 acc_val: 0.5455 time: 0.4700s\n",
            "Epoch: 0003 loss_train: 1.8156 acc_train: 0.1818 loss_val: 1.4055 acc_val: 0.6364 time: 0.4666s\n",
            "Epoch: 0004 loss_train: 1.5110 acc_train: 0.6364 loss_val: 1.3385 acc_val: 0.6364 time: 0.4802s\n",
            "Epoch: 0005 loss_train: 1.6360 acc_train: 0.2727 loss_val: 1.2701 acc_val: 0.7273 time: 0.4534s\n",
            "Epoch: 0006 loss_train: 1.3492 acc_train: 0.7273 loss_val: 1.1968 acc_val: 0.7273 time: 0.4776s\n",
            "Epoch: 0007 loss_train: 1.2759 acc_train: 0.4545 loss_val: 1.1245 acc_val: 0.8182 time: 0.5084s\n",
            "Epoch: 0008 loss_train: 1.1141 acc_train: 0.8182 loss_val: 1.0532 acc_val: 0.9091 time: 0.7023s\n",
            "Epoch: 0009 loss_train: 1.2742 acc_train: 0.6364 loss_val: 0.9826 acc_val: 0.9091 time: 0.4516s\n",
            "Epoch: 0010 loss_train: 1.2194 acc_train: 0.6364 loss_val: 0.9161 acc_val: 0.9091 time: 0.4828s\n",
            "Epoch: 0011 loss_train: 1.0872 acc_train: 0.9091 loss_val: 0.8519 acc_val: 0.9091 time: 0.4856s\n",
            "Epoch: 0012 loss_train: 0.9633 acc_train: 0.9091 loss_val: 0.7891 acc_val: 1.0000 time: 0.5180s\n",
            "Epoch: 0013 loss_train: 0.9302 acc_train: 0.8182 loss_val: 0.7298 acc_val: 1.0000 time: 0.4846s\n",
            "Epoch: 0014 loss_train: 1.0800 acc_train: 0.9091 loss_val: 0.6730 acc_val: 1.0000 time: 0.4646s\n",
            "Epoch: 0015 loss_train: 0.8095 acc_train: 0.9091 loss_val: 0.6195 acc_val: 1.0000 time: 0.5759s\n",
            "Epoch: 0016 loss_train: 0.7696 acc_train: 1.0000 loss_val: 0.5691 acc_val: 1.0000 time: 0.6750s\n",
            "Epoch: 0017 loss_train: 0.6317 acc_train: 0.9091 loss_val: 0.5222 acc_val: 1.0000 time: 0.4697s\n",
            "Epoch: 0018 loss_train: 0.7436 acc_train: 1.0000 loss_val: 0.4794 acc_val: 1.0000 time: 0.4768s\n",
            "Epoch: 0019 loss_train: 0.6492 acc_train: 1.0000 loss_val: 0.4397 acc_val: 1.0000 time: 0.4596s\n",
            "Epoch: 0020 loss_train: 0.4879 acc_train: 1.0000 loss_val: 0.4037 acc_val: 1.0000 time: 0.4601s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 10.4280s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.4808 accuracy= 0.4091\n",
            "---> run_4_bbc_5w1h_graph_hin.nx f1_macro 0.3971158571158571 acc 0.4090909090909091\n",
            "Networkfile run_8_gold_standard_5w1h_graph_hin.nx\n",
            "(579, 579) (579, 579) 20 76\n",
            "Epoch: 0001 loss_train: 2.6005 acc_train: 0.0500 loss_val: 2.4802 acc_val: 0.2500 time: 0.9998s\n",
            "Epoch: 0002 loss_train: 2.5124 acc_train: 0.2000 loss_val: 2.3737 acc_val: 0.3000 time: 0.9645s\n",
            "Epoch: 0003 loss_train: 2.4373 acc_train: 0.1500 loss_val: 2.2610 acc_val: 0.4500 time: 0.9641s\n",
            "Epoch: 0004 loss_train: 2.2731 acc_train: 0.4000 loss_val: 2.1508 acc_val: 0.6500 time: 0.9648s\n",
            "Epoch: 0005 loss_train: 2.1879 acc_train: 0.3500 loss_val: 2.0414 acc_val: 0.8500 time: 0.9250s\n",
            "Epoch: 0006 loss_train: 2.0467 acc_train: 0.5500 loss_val: 1.9310 acc_val: 0.8500 time: 0.9857s\n",
            "Epoch: 0007 loss_train: 2.0670 acc_train: 0.4500 loss_val: 1.8234 acc_val: 0.9000 time: 0.9362s\n",
            "Epoch: 0008 loss_train: 1.9600 acc_train: 0.6000 loss_val: 1.7178 acc_val: 0.9500 time: 0.9077s\n",
            "Epoch: 0009 loss_train: 1.7416 acc_train: 0.7500 loss_val: 1.6145 acc_val: 1.0000 time: 0.9333s\n",
            "Epoch: 0010 loss_train: 1.5316 acc_train: 0.8500 loss_val: 1.5125 acc_val: 1.0000 time: 0.9036s\n",
            "Epoch: 0011 loss_train: 1.5800 acc_train: 0.6000 loss_val: 1.4147 acc_val: 1.0000 time: 0.9348s\n",
            "Epoch: 0012 loss_train: 1.5279 acc_train: 0.8000 loss_val: 1.3215 acc_val: 1.0000 time: 0.9834s\n",
            "Epoch: 0013 loss_train: 1.4971 acc_train: 0.8500 loss_val: 1.2319 acc_val: 1.0000 time: 0.8787s\n",
            "Epoch: 0014 loss_train: 1.4106 acc_train: 0.9000 loss_val: 1.1471 acc_val: 1.0000 time: 0.9186s\n",
            "Epoch: 0015 loss_train: 1.2181 acc_train: 0.8500 loss_val: 1.0646 acc_val: 1.0000 time: 0.9169s\n",
            "Epoch: 0016 loss_train: 1.2228 acc_train: 0.8500 loss_val: 0.9875 acc_val: 1.0000 time: 1.3808s\n",
            "Epoch: 0017 loss_train: 1.1647 acc_train: 0.8000 loss_val: 0.9136 acc_val: 1.0000 time: 0.9552s\n",
            "Epoch: 0018 loss_train: 1.5380 acc_train: 0.8000 loss_val: 0.8466 acc_val: 1.0000 time: 0.9862s\n",
            "Epoch: 0019 loss_train: 1.2229 acc_train: 0.8500 loss_val: 0.7822 acc_val: 1.0000 time: 0.9673s\n",
            "Epoch: 0020 loss_train: 1.1917 acc_train: 0.7500 loss_val: 0.7207 acc_val: 1.0000 time: 0.9857s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 19.7081s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.6517 accuracy= 0.6184\n",
            "---> run_8_gold_standard_5w1h_graph_hin.nx f1_macro 0.4820592125739184 acc 0.618421052631579\n",
            "Networkfile run_5_bbc_5w1h_graph_hin.nx\n",
            "(392, 392) (392, 392) 11 44\n",
            "Epoch: 0001 loss_train: 1.8094 acc_train: 0.1818 loss_val: 1.6430 acc_val: 0.0909 time: 0.4780s\n",
            "Epoch: 0002 loss_train: 1.9353 acc_train: 0.0000 loss_val: 1.5809 acc_val: 0.1818 time: 0.4878s\n",
            "Epoch: 0003 loss_train: 1.8300 acc_train: 0.1818 loss_val: 1.5159 acc_val: 0.5455 time: 0.5055s\n",
            "Epoch: 0004 loss_train: 1.7010 acc_train: 0.3636 loss_val: 1.4427 acc_val: 0.7273 time: 0.4916s\n",
            "Epoch: 0005 loss_train: 1.6568 acc_train: 0.3636 loss_val: 1.3708 acc_val: 0.8182 time: 0.4696s\n",
            "Epoch: 0006 loss_train: 1.6090 acc_train: 0.1818 loss_val: 1.2956 acc_val: 0.8182 time: 0.4603s\n",
            "Epoch: 0007 loss_train: 1.2847 acc_train: 0.5455 loss_val: 1.2196 acc_val: 0.8182 time: 0.4754s\n",
            "Epoch: 0008 loss_train: 1.3237 acc_train: 0.4545 loss_val: 1.1463 acc_val: 0.8182 time: 0.4524s\n",
            "Epoch: 0009 loss_train: 1.4416 acc_train: 0.4545 loss_val: 1.0741 acc_val: 0.9091 time: 0.4254s\n",
            "Epoch: 0010 loss_train: 1.2020 acc_train: 0.8182 loss_val: 1.0049 acc_val: 0.9091 time: 0.4470s\n",
            "Epoch: 0011 loss_train: 1.0293 acc_train: 0.8182 loss_val: 0.9379 acc_val: 0.9091 time: 0.4568s\n",
            "Epoch: 0012 loss_train: 1.0042 acc_train: 0.8182 loss_val: 0.8740 acc_val: 0.9091 time: 0.4766s\n",
            "Epoch: 0013 loss_train: 0.9672 acc_train: 0.7273 loss_val: 0.8123 acc_val: 1.0000 time: 0.4585s\n",
            "Epoch: 0014 loss_train: 1.0029 acc_train: 0.9091 loss_val: 0.7521 acc_val: 1.0000 time: 0.4771s\n",
            "Epoch: 0015 loss_train: 0.8658 acc_train: 0.8182 loss_val: 0.6958 acc_val: 1.0000 time: 0.4886s\n",
            "Epoch: 0016 loss_train: 0.7936 acc_train: 0.8182 loss_val: 0.6427 acc_val: 1.0000 time: 0.4288s\n",
            "Epoch: 0017 loss_train: 0.7997 acc_train: 0.7273 loss_val: 0.5938 acc_val: 1.0000 time: 0.4306s\n",
            "Epoch: 0018 loss_train: 0.8509 acc_train: 0.9091 loss_val: 0.5486 acc_val: 1.0000 time: 0.4492s\n",
            "Epoch: 0019 loss_train: 0.6099 acc_train: 0.9091 loss_val: 0.5067 acc_val: 1.0000 time: 0.4424s\n",
            "Epoch: 0020 loss_train: 0.5973 acc_train: 1.0000 loss_val: 0.4670 acc_val: 1.0000 time: 0.4509s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 9.4079s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.3652 accuracy= 0.5227\n",
            "---> run_5_bbc_5w1h_graph_hin.nx f1_macro 0.5256959706959707 acc 0.5227272727272727\n",
            "Networkfile run_9_google_news_5w1h_graph_hin.nx\n",
            "(227, 227) (227, 227) 7 25\n",
            "Epoch: 0001 loss_train: 2.0741 acc_train: 0.1429 loss_val: 1.9401 acc_val: 0.2857 time: 0.1601s\n",
            "Epoch: 0002 loss_train: 2.0808 acc_train: 0.1429 loss_val: 1.8841 acc_val: 0.2857 time: 0.1559s\n",
            "Epoch: 0003 loss_train: 1.8083 acc_train: 0.2857 loss_val: 1.8215 acc_val: 0.2857 time: 0.1593s\n",
            "Epoch: 0004 loss_train: 2.1101 acc_train: 0.0000 loss_val: 1.7566 acc_val: 0.4286 time: 0.1613s\n",
            "Epoch: 0005 loss_train: 1.8712 acc_train: 0.2857 loss_val: 1.6909 acc_val: 0.5714 time: 0.1488s\n",
            "Epoch: 0006 loss_train: 1.9597 acc_train: 0.1429 loss_val: 1.6249 acc_val: 0.5714 time: 0.1563s\n",
            "Epoch: 0007 loss_train: 1.7376 acc_train: 0.4286 loss_val: 1.5572 acc_val: 0.7143 time: 0.1567s\n",
            "Epoch: 0008 loss_train: 1.3895 acc_train: 0.5714 loss_val: 1.4882 acc_val: 0.7143 time: 0.1490s\n",
            "Epoch: 0009 loss_train: 1.5684 acc_train: 0.7143 loss_val: 1.4211 acc_val: 0.7143 time: 0.1568s\n",
            "Epoch: 0010 loss_train: 1.7240 acc_train: 0.2857 loss_val: 1.3535 acc_val: 0.7143 time: 0.1504s\n",
            "Epoch: 0011 loss_train: 1.3126 acc_train: 0.5714 loss_val: 1.2879 acc_val: 0.7143 time: 0.1668s\n",
            "Epoch: 0012 loss_train: 1.2494 acc_train: 0.7143 loss_val: 1.2238 acc_val: 0.7143 time: 0.1536s\n",
            "Epoch: 0013 loss_train: 1.6693 acc_train: 0.4286 loss_val: 1.1619 acc_val: 0.8571 time: 0.1542s\n",
            "Epoch: 0014 loss_train: 1.0341 acc_train: 0.7143 loss_val: 1.0995 acc_val: 0.8571 time: 0.1632s\n",
            "Epoch: 0015 loss_train: 1.3319 acc_train: 0.4286 loss_val: 1.0388 acc_val: 0.8571 time: 0.1620s\n",
            "Epoch: 0016 loss_train: 1.0452 acc_train: 0.8571 loss_val: 0.9802 acc_val: 1.0000 time: 0.1643s\n",
            "Epoch: 0017 loss_train: 1.4871 acc_train: 0.7143 loss_val: 0.9248 acc_val: 1.0000 time: 0.1641s\n",
            "Epoch: 0018 loss_train: 1.2359 acc_train: 0.5714 loss_val: 0.8717 acc_val: 1.0000 time: 0.1494s\n",
            "Epoch: 0019 loss_train: 1.0087 acc_train: 0.7143 loss_val: 0.8174 acc_val: 1.0000 time: 0.1522s\n",
            "Epoch: 0020 loss_train: 1.4456 acc_train: 0.8571 loss_val: 0.7667 acc_val: 1.0000 time: 0.1568s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.2546s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.4897 accuracy= 0.5600\n",
            "---> run_9_google_news_5w1h_graph_hin.nx f1_macro 0.505028305028305 acc 0.56\n",
            "Networkfile run_5_gold_standard_5w1h_graph_hin.nx\n",
            "(579, 579) (579, 579) 20 76\n",
            "Epoch: 0001 loss_train: 2.3778 acc_train: 0.2000 loss_val: 2.4179 acc_val: 0.3500 time: 0.9026s\n",
            "Epoch: 0002 loss_train: 2.4582 acc_train: 0.1500 loss_val: 2.2963 acc_val: 0.4000 time: 0.8970s\n",
            "Epoch: 0003 loss_train: 2.2082 acc_train: 0.3000 loss_val: 2.1769 acc_val: 0.6500 time: 0.9076s\n",
            "Epoch: 0004 loss_train: 2.2452 acc_train: 0.4500 loss_val: 2.0581 acc_val: 0.7500 time: 0.9182s\n",
            "Epoch: 0005 loss_train: 2.0802 acc_train: 0.4500 loss_val: 1.9398 acc_val: 0.7500 time: 0.9089s\n",
            "Epoch: 0006 loss_train: 1.9442 acc_train: 0.6000 loss_val: 1.8223 acc_val: 0.9000 time: 0.8977s\n",
            "Epoch: 0007 loss_train: 1.9053 acc_train: 0.7500 loss_val: 1.7099 acc_val: 0.9000 time: 0.9135s\n",
            "Epoch: 0008 loss_train: 1.8240 acc_train: 0.6500 loss_val: 1.5991 acc_val: 0.9500 time: 0.9864s\n",
            "Epoch: 0009 loss_train: 1.7030 acc_train: 0.7500 loss_val: 1.4934 acc_val: 1.0000 time: 0.9732s\n",
            "Epoch: 0010 loss_train: 1.6567 acc_train: 0.7000 loss_val: 1.3924 acc_val: 1.0000 time: 0.9435s\n",
            "Epoch: 0011 loss_train: 1.4649 acc_train: 0.8000 loss_val: 1.2978 acc_val: 1.0000 time: 0.9928s\n",
            "Epoch: 0012 loss_train: 1.5070 acc_train: 0.8000 loss_val: 1.2081 acc_val: 1.0000 time: 0.9866s\n",
            "Epoch: 0013 loss_train: 1.3798 acc_train: 1.0000 loss_val: 1.1238 acc_val: 1.0000 time: 0.9394s\n",
            "Epoch: 0014 loss_train: 1.3862 acc_train: 0.9000 loss_val: 1.0455 acc_val: 1.0000 time: 0.9018s\n",
            "Epoch: 0015 loss_train: 1.0974 acc_train: 0.8500 loss_val: 0.9724 acc_val: 1.0000 time: 0.9384s\n",
            "Epoch: 0016 loss_train: 1.2600 acc_train: 0.8000 loss_val: 0.9048 acc_val: 1.0000 time: 0.9029s\n",
            "Epoch: 0017 loss_train: 1.0977 acc_train: 0.9500 loss_val: 0.8411 acc_val: 1.0000 time: 0.9667s\n",
            "Epoch: 0018 loss_train: 0.9295 acc_train: 0.9500 loss_val: 0.7802 acc_val: 1.0000 time: 0.8742s\n",
            "Epoch: 0019 loss_train: 0.9993 acc_train: 0.9500 loss_val: 0.7218 acc_val: 1.0000 time: 0.9625s\n",
            "Epoch: 0020 loss_train: 0.9634 acc_train: 0.9000 loss_val: 0.6665 acc_val: 1.0000 time: 1.1129s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 19.0405s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.6869 accuracy= 0.6053\n",
            "---> run_5_gold_standard_5w1h_graph_hin.nx f1_macro 0.48991990915067835 acc 0.6052631578947368\n",
            "Networkfile run_2_bbc_5w1h_graph_hin.nx\n",
            "(392, 392) (392, 392) 11 44\n",
            "Epoch: 0001 loss_train: 1.6499 acc_train: 0.2727 loss_val: 1.5194 acc_val: 0.2727 time: 0.4371s\n",
            "Epoch: 0002 loss_train: 1.4743 acc_train: 0.1818 loss_val: 1.4500 acc_val: 0.4545 time: 0.4666s\n",
            "Epoch: 0003 loss_train: 1.9818 acc_train: 0.0909 loss_val: 1.3799 acc_val: 0.5455 time: 0.4328s\n",
            "Epoch: 0004 loss_train: 1.7793 acc_train: 0.3636 loss_val: 1.3135 acc_val: 0.7273 time: 0.4313s\n",
            "Epoch: 0005 loss_train: 1.7578 acc_train: 0.1818 loss_val: 1.2416 acc_val: 0.9091 time: 0.4395s\n",
            "Epoch: 0006 loss_train: 1.2620 acc_train: 0.6364 loss_val: 1.1696 acc_val: 0.9091 time: 0.4451s\n",
            "Epoch: 0007 loss_train: 1.3901 acc_train: 0.3636 loss_val: 1.1005 acc_val: 0.9091 time: 0.4581s\n",
            "Epoch: 0008 loss_train: 1.0971 acc_train: 0.8182 loss_val: 1.0333 acc_val: 0.9091 time: 0.4325s\n",
            "Epoch: 0009 loss_train: 1.2207 acc_train: 0.5455 loss_val: 0.9692 acc_val: 0.9091 time: 0.4836s\n",
            "Epoch: 0010 loss_train: 1.1556 acc_train: 0.7273 loss_val: 0.9099 acc_val: 0.9091 time: 0.4706s\n",
            "Epoch: 0011 loss_train: 0.9179 acc_train: 0.8182 loss_val: 0.8516 acc_val: 0.9091 time: 0.5096s\n",
            "Epoch: 0012 loss_train: 1.0533 acc_train: 0.8182 loss_val: 0.7968 acc_val: 1.0000 time: 0.4482s\n",
            "Epoch: 0013 loss_train: 0.6609 acc_train: 0.9091 loss_val: 0.7436 acc_val: 1.0000 time: 0.4612s\n",
            "Epoch: 0014 loss_train: 0.9779 acc_train: 0.7273 loss_val: 0.6925 acc_val: 1.0000 time: 0.4561s\n",
            "Epoch: 0015 loss_train: 0.7234 acc_train: 1.0000 loss_val: 0.6450 acc_val: 1.0000 time: 0.4529s\n",
            "Epoch: 0016 loss_train: 1.0928 acc_train: 0.7273 loss_val: 0.6004 acc_val: 1.0000 time: 0.4356s\n",
            "Epoch: 0017 loss_train: 0.9328 acc_train: 0.6364 loss_val: 0.5587 acc_val: 1.0000 time: 0.4597s\n",
            "Epoch: 0018 loss_train: 0.8537 acc_train: 0.9091 loss_val: 0.5209 acc_val: 1.0000 time: 0.4680s\n",
            "Epoch: 0019 loss_train: 0.7052 acc_train: 0.8182 loss_val: 0.4856 acc_val: 1.0000 time: 0.4100s\n",
            "Epoch: 0020 loss_train: 0.6086 acc_train: 0.9091 loss_val: 0.4528 acc_val: 1.0000 time: 0.4143s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 9.1906s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.5541 accuracy= 0.3409\n",
            "---> run_2_bbc_5w1h_graph_hin.nx f1_macro 0.3133796436122017 acc 0.3409090909090909\n",
            "Networkfile run_7_40er_5w1h_graph_hin.nx\n",
            "(249, 249) (249, 249) 8 32\n",
            "Epoch: 0001 loss_train: 1.0488 acc_train: 0.5000 loss_val: 0.9264 acc_val: 0.7500 time: 0.1805s\n",
            "Epoch: 0002 loss_train: 1.4081 acc_train: 0.3750 loss_val: 0.8759 acc_val: 0.7500 time: 0.1830s\n",
            "Epoch: 0003 loss_train: 1.1207 acc_train: 0.5000 loss_val: 0.8080 acc_val: 0.7500 time: 0.1845s\n",
            "Epoch: 0004 loss_train: 0.8457 acc_train: 0.6250 loss_val: 0.7420 acc_val: 0.8750 time: 0.1788s\n",
            "Epoch: 0005 loss_train: 0.6631 acc_train: 0.7500 loss_val: 0.6722 acc_val: 0.8750 time: 0.1804s\n",
            "Epoch: 0006 loss_train: 0.7129 acc_train: 0.7500 loss_val: 0.6055 acc_val: 1.0000 time: 0.1810s\n",
            "Epoch: 0007 loss_train: 1.2587 acc_train: 0.2500 loss_val: 0.5441 acc_val: 1.0000 time: 0.1748s\n",
            "Epoch: 0008 loss_train: 0.7187 acc_train: 0.8750 loss_val: 0.4874 acc_val: 1.0000 time: 0.1827s\n",
            "Epoch: 0009 loss_train: 0.9598 acc_train: 0.5000 loss_val: 0.4358 acc_val: 1.0000 time: 0.1789s\n",
            "Epoch: 0010 loss_train: 0.6287 acc_train: 0.7500 loss_val: 0.3896 acc_val: 1.0000 time: 0.1884s\n",
            "Epoch: 0011 loss_train: 0.6037 acc_train: 0.7500 loss_val: 0.3487 acc_val: 1.0000 time: 0.1994s\n",
            "Epoch: 0012 loss_train: 0.5634 acc_train: 0.6250 loss_val: 0.3116 acc_val: 1.0000 time: 0.1777s\n",
            "Epoch: 0013 loss_train: 0.4663 acc_train: 0.8750 loss_val: 0.2785 acc_val: 1.0000 time: 0.1807s\n",
            "Epoch: 0014 loss_train: 0.3930 acc_train: 0.8750 loss_val: 0.2502 acc_val: 1.0000 time: 0.1724s\n",
            "Epoch: 0015 loss_train: 0.3175 acc_train: 1.0000 loss_val: 0.2241 acc_val: 1.0000 time: 0.1679s\n",
            "Epoch: 0016 loss_train: 0.5688 acc_train: 0.8750 loss_val: 0.2014 acc_val: 1.0000 time: 0.1717s\n",
            "Epoch: 0017 loss_train: 0.5389 acc_train: 0.8750 loss_val: 0.1814 acc_val: 1.0000 time: 0.1712s\n",
            "Epoch: 0018 loss_train: 0.4493 acc_train: 0.8750 loss_val: 0.1630 acc_val: 1.0000 time: 0.1906s\n",
            "Epoch: 0019 loss_train: 0.2311 acc_train: 1.0000 loss_val: 0.1470 acc_val: 1.0000 time: 0.1873s\n",
            "Epoch: 0020 loss_train: 0.4108 acc_train: 1.0000 loss_val: 0.1327 acc_val: 1.0000 time: 0.1714s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.7106s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 0.6340 accuracy= 0.8125\n",
            "---> run_7_40er_5w1h_graph_hin.nx f1_macro 0.5966183574879227 acc 0.8125\n",
            "Networkfile run_9_gold_standard_5w1h_graph_hin.nx\n",
            "(579, 579) (579, 579) 20 76\n",
            "Epoch: 0001 loss_train: 2.4974 acc_train: 0.1000 loss_val: 2.4651 acc_val: 0.2000 time: 0.8863s\n",
            "Epoch: 0002 loss_train: 2.4560 acc_train: 0.2500 loss_val: 2.3527 acc_val: 0.3500 time: 0.8298s\n",
            "Epoch: 0003 loss_train: 2.3695 acc_train: 0.2500 loss_val: 2.2397 acc_val: 0.6000 time: 0.8741s\n",
            "Epoch: 0004 loss_train: 2.2482 acc_train: 0.4000 loss_val: 2.1287 acc_val: 0.8000 time: 0.8661s\n",
            "Epoch: 0005 loss_train: 2.2350 acc_train: 0.4500 loss_val: 2.0171 acc_val: 0.9000 time: 0.8907s\n",
            "Epoch: 0006 loss_train: 2.1573 acc_train: 0.5000 loss_val: 1.9116 acc_val: 0.9500 time: 0.9116s\n",
            "Epoch: 0007 loss_train: 2.1368 acc_train: 0.5500 loss_val: 1.8075 acc_val: 1.0000 time: 0.9254s\n",
            "Epoch: 0008 loss_train: 1.9301 acc_train: 0.6000 loss_val: 1.7053 acc_val: 1.0000 time: 0.8732s\n",
            "Epoch: 0009 loss_train: 1.8010 acc_train: 0.8000 loss_val: 1.6024 acc_val: 1.0000 time: 0.8819s\n",
            "Epoch: 0010 loss_train: 1.8207 acc_train: 0.7500 loss_val: 1.5007 acc_val: 1.0000 time: 0.8975s\n",
            "Epoch: 0011 loss_train: 1.6192 acc_train: 0.9000 loss_val: 1.4022 acc_val: 1.0000 time: 0.8892s\n",
            "Epoch: 0012 loss_train: 1.7733 acc_train: 0.9000 loss_val: 1.3071 acc_val: 1.0000 time: 0.8710s\n",
            "Epoch: 0013 loss_train: 1.4356 acc_train: 0.9500 loss_val: 1.2136 acc_val: 1.0000 time: 0.8705s\n",
            "Epoch: 0014 loss_train: 1.4518 acc_train: 0.9000 loss_val: 1.1234 acc_val: 1.0000 time: 0.8779s\n",
            "Epoch: 0015 loss_train: 1.3435 acc_train: 0.8500 loss_val: 1.0376 acc_val: 1.0000 time: 0.8627s\n",
            "Epoch: 0016 loss_train: 1.0854 acc_train: 0.9500 loss_val: 0.9554 acc_val: 1.0000 time: 0.8708s\n",
            "Epoch: 0017 loss_train: 1.4080 acc_train: 0.8500 loss_val: 0.8767 acc_val: 1.0000 time: 0.8759s\n",
            "Epoch: 0018 loss_train: 1.1574 acc_train: 0.9000 loss_val: 0.8052 acc_val: 1.0000 time: 0.8788s\n",
            "Epoch: 0019 loss_train: 1.1732 acc_train: 0.9000 loss_val: 0.7375 acc_val: 1.0000 time: 0.8735s\n",
            "Epoch: 0020 loss_train: 1.1618 acc_train: 0.9000 loss_val: 0.6744 acc_val: 1.0000 time: 0.8359s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 17.7402s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.5795 accuracy= 0.6842\n",
            "---> run_9_gold_standard_5w1h_graph_hin.nx f1_macro 0.5465963252147462 acc 0.6842105263157895\n",
            "Networkfile run_8_google_news_5w1h_graph_hin.nx\n",
            "(227, 227) (227, 227) 7 25\n",
            "Epoch: 0001 loss_train: 1.8070 acc_train: 0.1429 loss_val: 1.8507 acc_val: 0.4286 time: 0.1446s\n",
            "Epoch: 0002 loss_train: 2.2394 acc_train: 0.0000 loss_val: 1.7880 acc_val: 0.5714 time: 0.1475s\n",
            "Epoch: 0003 loss_train: 2.0654 acc_train: 0.0000 loss_val: 1.7130 acc_val: 0.5714 time: 0.1413s\n",
            "Epoch: 0004 loss_train: 2.0867 acc_train: 0.2857 loss_val: 1.6423 acc_val: 0.5714 time: 0.1517s\n",
            "Epoch: 0005 loss_train: 1.7838 acc_train: 0.4286 loss_val: 1.5689 acc_val: 0.5714 time: 0.1428s\n",
            "Epoch: 0006 loss_train: 1.6713 acc_train: 0.2857 loss_val: 1.4904 acc_val: 0.7143 time: 0.1522s\n",
            "Epoch: 0007 loss_train: 1.7213 acc_train: 0.2857 loss_val: 1.4157 acc_val: 0.8571 time: 0.1405s\n",
            "Epoch: 0008 loss_train: 1.6928 acc_train: 0.4286 loss_val: 1.3446 acc_val: 0.8571 time: 0.1424s\n",
            "Epoch: 0009 loss_train: 1.1584 acc_train: 0.8571 loss_val: 1.2747 acc_val: 0.8571 time: 0.1397s\n",
            "Epoch: 0010 loss_train: 1.5812 acc_train: 0.1429 loss_val: 1.2037 acc_val: 0.8571 time: 0.1369s\n",
            "Epoch: 0011 loss_train: 1.2812 acc_train: 0.7143 loss_val: 1.1355 acc_val: 0.8571 time: 0.1419s\n",
            "Epoch: 0012 loss_train: 1.4593 acc_train: 0.5714 loss_val: 1.0692 acc_val: 0.8571 time: 0.1413s\n",
            "Epoch: 0013 loss_train: 1.2055 acc_train: 0.5714 loss_val: 1.0069 acc_val: 0.8571 time: 0.1217s\n",
            "Epoch: 0014 loss_train: 1.3141 acc_train: 0.2857 loss_val: 0.9458 acc_val: 1.0000 time: 0.1035s\n",
            "Epoch: 0015 loss_train: 1.0757 acc_train: 1.0000 loss_val: 0.8885 acc_val: 1.0000 time: 0.1517s\n",
            "Epoch: 0016 loss_train: 0.9797 acc_train: 1.0000 loss_val: 0.8358 acc_val: 1.0000 time: 0.1479s\n",
            "Epoch: 0017 loss_train: 1.1590 acc_train: 0.8571 loss_val: 0.7864 acc_val: 1.0000 time: 0.1529s\n",
            "Epoch: 0018 loss_train: 0.8758 acc_train: 1.0000 loss_val: 0.7401 acc_val: 1.0000 time: 0.1517s\n",
            "Epoch: 0019 loss_train: 0.8870 acc_train: 0.7143 loss_val: 0.6948 acc_val: 1.0000 time: 0.1561s\n",
            "Epoch: 0020 loss_train: 0.7643 acc_train: 0.8571 loss_val: 0.6527 acc_val: 1.0000 time: 0.1505s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 2.9519s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.5456 accuracy= 0.5600\n",
            "---> run_8_google_news_5w1h_graph_hin.nx f1_macro 0.4979437229437229 acc 0.56\n",
            "Networkfile run_10_bbc_5w1h_graph_hin.nx\n",
            "(392, 392) (392, 392) 11 44\n",
            "Epoch: 0001 loss_train: 1.6132 acc_train: 0.3636 loss_val: 1.5295 acc_val: 0.3636 time: 0.4063s\n",
            "Epoch: 0002 loss_train: 1.6941 acc_train: 0.2727 loss_val: 1.4752 acc_val: 0.5455 time: 0.4205s\n",
            "Epoch: 0003 loss_train: 1.6352 acc_train: 0.2727 loss_val: 1.4191 acc_val: 0.6364 time: 0.4089s\n",
            "Epoch: 0004 loss_train: 1.4554 acc_train: 0.4545 loss_val: 1.3597 acc_val: 0.6364 time: 0.3684s\n",
            "Epoch: 0005 loss_train: 1.8397 acc_train: 0.3636 loss_val: 1.2968 acc_val: 0.7273 time: 0.4261s\n",
            "Epoch: 0006 loss_train: 1.3161 acc_train: 0.4545 loss_val: 1.2275 acc_val: 0.8182 time: 0.4018s\n",
            "Epoch: 0007 loss_train: 1.5107 acc_train: 0.5455 loss_val: 1.1560 acc_val: 0.8182 time: 0.4041s\n",
            "Epoch: 0008 loss_train: 1.1901 acc_train: 0.8182 loss_val: 1.0877 acc_val: 0.8182 time: 0.3981s\n",
            "Epoch: 0009 loss_train: 1.1385 acc_train: 0.8182 loss_val: 1.0206 acc_val: 1.0000 time: 0.4054s\n",
            "Epoch: 0010 loss_train: 1.2309 acc_train: 0.7273 loss_val: 0.9534 acc_val: 1.0000 time: 0.3807s\n",
            "Epoch: 0011 loss_train: 1.0715 acc_train: 0.8182 loss_val: 0.8875 acc_val: 1.0000 time: 0.4124s\n",
            "Epoch: 0012 loss_train: 1.1985 acc_train: 0.6364 loss_val: 0.8269 acc_val: 1.0000 time: 0.4064s\n",
            "Epoch: 0013 loss_train: 0.9419 acc_train: 0.7273 loss_val: 0.7688 acc_val: 1.0000 time: 0.3871s\n",
            "Epoch: 0014 loss_train: 0.8653 acc_train: 0.8182 loss_val: 0.7150 acc_val: 1.0000 time: 0.4110s\n",
            "Epoch: 0015 loss_train: 0.9024 acc_train: 0.7273 loss_val: 0.6636 acc_val: 1.0000 time: 0.3844s\n",
            "Epoch: 0016 loss_train: 0.6709 acc_train: 0.9091 loss_val: 0.6149 acc_val: 1.0000 time: 0.3931s\n",
            "Epoch: 0017 loss_train: 0.8339 acc_train: 0.9091 loss_val: 0.5700 acc_val: 1.0000 time: 0.4021s\n",
            "Epoch: 0018 loss_train: 0.6733 acc_train: 0.9091 loss_val: 0.5297 acc_val: 1.0000 time: 0.4091s\n",
            "Epoch: 0019 loss_train: 0.3735 acc_train: 1.0000 loss_val: 0.4904 acc_val: 1.0000 time: 0.4033s\n",
            "Epoch: 0020 loss_train: 0.5438 acc_train: 1.0000 loss_val: 0.4532 acc_val: 1.0000 time: 0.4058s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 8.1675s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.4895 accuracy= 0.4545\n",
            "---> run_10_bbc_5w1h_graph_hin.nx f1_macro 0.4401414303853328 acc 0.45454545454545453\n",
            "Networkfile run_8_40er_5w1h_graph_hin.nx\n",
            "(249, 249) (249, 249) 8 32\n",
            "Epoch: 0001 loss_train: 1.0669 acc_train: 0.5000 loss_val: 0.9443 acc_val: 0.6250 time: 0.1714s\n",
            "Epoch: 0002 loss_train: 1.1675 acc_train: 0.5000 loss_val: 0.8783 acc_val: 0.6250 time: 0.1619s\n",
            "Epoch: 0003 loss_train: 0.9938 acc_train: 0.2500 loss_val: 0.8107 acc_val: 0.7500 time: 0.1804s\n",
            "Epoch: 0004 loss_train: 0.8579 acc_train: 0.6250 loss_val: 0.7484 acc_val: 0.7500 time: 0.1574s\n",
            "Epoch: 0005 loss_train: 0.7340 acc_train: 0.7500 loss_val: 0.6883 acc_val: 0.7500 time: 0.1664s\n",
            "Epoch: 0006 loss_train: 0.6370 acc_train: 0.7500 loss_val: 0.6286 acc_val: 0.7500 time: 0.1617s\n",
            "Epoch: 0007 loss_train: 0.6080 acc_train: 0.7500 loss_val: 0.5725 acc_val: 0.8750 time: 0.1686s\n",
            "Epoch: 0008 loss_train: 0.8608 acc_train: 0.6250 loss_val: 0.5210 acc_val: 0.8750 time: 0.1610s\n",
            "Epoch: 0009 loss_train: 0.5268 acc_train: 1.0000 loss_val: 0.4729 acc_val: 0.8750 time: 0.1702s\n",
            "Epoch: 0010 loss_train: 0.7926 acc_train: 0.6250 loss_val: 0.4298 acc_val: 0.8750 time: 0.1597s\n",
            "Epoch: 0011 loss_train: 0.6184 acc_train: 0.8750 loss_val: 0.3914 acc_val: 0.8750 time: 0.1761s\n",
            "Epoch: 0012 loss_train: 0.6292 acc_train: 0.7500 loss_val: 0.3573 acc_val: 0.8750 time: 0.1702s\n",
            "Epoch: 0013 loss_train: 0.5264 acc_train: 0.8750 loss_val: 0.3262 acc_val: 0.8750 time: 0.1433s\n",
            "Epoch: 0014 loss_train: 0.3525 acc_train: 1.0000 loss_val: 0.2987 acc_val: 0.8750 time: 0.1693s\n",
            "Epoch: 0015 loss_train: 0.5938 acc_train: 0.7500 loss_val: 0.2715 acc_val: 0.8750 time: 0.1618s\n",
            "Epoch: 0016 loss_train: 0.3393 acc_train: 1.0000 loss_val: 0.2466 acc_val: 1.0000 time: 0.1565s\n",
            "Epoch: 0017 loss_train: 0.3981 acc_train: 1.0000 loss_val: 0.2248 acc_val: 1.0000 time: 0.1589s\n",
            "Epoch: 0018 loss_train: 0.1975 acc_train: 1.0000 loss_val: 0.2050 acc_val: 1.0000 time: 0.1604s\n",
            "Epoch: 0019 loss_train: 0.2766 acc_train: 1.0000 loss_val: 0.1884 acc_val: 1.0000 time: 0.1650s\n",
            "Epoch: 0020 loss_train: 0.3459 acc_train: 0.8750 loss_val: 0.1726 acc_val: 1.0000 time: 0.1604s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.3775s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 0.5059 accuracy= 0.8438\n",
            "---> run_8_40er_5w1h_graph_hin.nx f1_macro 0.6216216216216216 acc 0.84375\n",
            "Networkfile run_6_bbc_5w1h_graph_hin.nx\n",
            "(392, 392) (392, 392) 11 44\n",
            "Epoch: 0001 loss_train: 1.7856 acc_train: 0.2727 loss_val: 1.6404 acc_val: 0.0909 time: 0.4099s\n",
            "Epoch: 0002 loss_train: 1.6807 acc_train: 0.2727 loss_val: 1.5748 acc_val: 0.2727 time: 0.3857s\n",
            "Epoch: 0003 loss_train: 1.8239 acc_train: 0.0000 loss_val: 1.5016 acc_val: 0.4545 time: 0.3703s\n",
            "Epoch: 0004 loss_train: 1.7348 acc_train: 0.2727 loss_val: 1.4277 acc_val: 0.7273 time: 0.3949s\n",
            "Epoch: 0005 loss_train: 1.6562 acc_train: 0.3636 loss_val: 1.3538 acc_val: 0.8182 time: 0.4034s\n",
            "Epoch: 0006 loss_train: 1.4770 acc_train: 0.3636 loss_val: 1.2797 acc_val: 0.9091 time: 0.4114s\n",
            "Epoch: 0007 loss_train: 1.4208 acc_train: 0.5455 loss_val: 1.2051 acc_val: 0.9091 time: 0.4020s\n",
            "Epoch: 0008 loss_train: 1.2215 acc_train: 0.6364 loss_val: 1.1322 acc_val: 1.0000 time: 0.3980s\n",
            "Epoch: 0009 loss_train: 1.4112 acc_train: 0.5455 loss_val: 1.0607 acc_val: 1.0000 time: 0.3906s\n",
            "Epoch: 0010 loss_train: 1.2710 acc_train: 0.5455 loss_val: 0.9922 acc_val: 1.0000 time: 0.4016s\n",
            "Epoch: 0011 loss_train: 1.0754 acc_train: 0.9091 loss_val: 0.9258 acc_val: 1.0000 time: 0.4101s\n",
            "Epoch: 0012 loss_train: 1.0789 acc_train: 0.8182 loss_val: 0.8611 acc_val: 1.0000 time: 0.3935s\n",
            "Epoch: 0013 loss_train: 0.9585 acc_train: 0.8182 loss_val: 0.8005 acc_val: 1.0000 time: 0.4043s\n",
            "Epoch: 0014 loss_train: 1.0222 acc_train: 0.9091 loss_val: 0.7430 acc_val: 1.0000 time: 0.3883s\n",
            "Epoch: 0015 loss_train: 0.8689 acc_train: 0.8182 loss_val: 0.6888 acc_val: 1.0000 time: 0.3944s\n",
            "Epoch: 0016 loss_train: 0.9094 acc_train: 0.9091 loss_val: 0.6373 acc_val: 1.0000 time: 0.3978s\n",
            "Epoch: 0017 loss_train: 0.8239 acc_train: 0.9091 loss_val: 0.5890 acc_val: 1.0000 time: 0.4256s\n",
            "Epoch: 0018 loss_train: 1.0493 acc_train: 0.8182 loss_val: 0.5442 acc_val: 1.0000 time: 0.4030s\n",
            "Epoch: 0019 loss_train: 0.5319 acc_train: 1.0000 loss_val: 0.5023 acc_val: 1.0000 time: 0.3988s\n",
            "Epoch: 0020 loss_train: 0.5835 acc_train: 0.9091 loss_val: 0.4638 acc_val: 1.0000 time: 0.3887s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 8.0993s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.4506 accuracy= 0.4091\n",
            "---> run_6_bbc_5w1h_graph_hin.nx f1_macro 0.3941414141414141 acc 0.4090909090909091\n",
            "Networkfile run_4_google_news_5w1h_graph_hin.nx\n",
            "(227, 227) (227, 227) 7 25\n",
            "Epoch: 0001 loss_train: 2.4481 acc_train: 0.1429 loss_val: 1.8745 acc_val: 0.4286 time: 0.1443s\n",
            "Epoch: 0002 loss_train: 2.0269 acc_train: 0.1429 loss_val: 1.8182 acc_val: 0.5714 time: 0.1393s\n",
            "Epoch: 0003 loss_train: 1.8206 acc_train: 0.2857 loss_val: 1.7559 acc_val: 0.5714 time: 0.1556s\n",
            "Epoch: 0004 loss_train: 1.9366 acc_train: 0.1429 loss_val: 1.6943 acc_val: 0.7143 time: 0.1416s\n",
            "Epoch: 0005 loss_train: 1.7767 acc_train: 0.2857 loss_val: 1.6307 acc_val: 0.7143 time: 0.1452s\n",
            "Epoch: 0006 loss_train: 1.7559 acc_train: 0.2857 loss_val: 1.5680 acc_val: 0.7143 time: 0.1497s\n",
            "Epoch: 0007 loss_train: 1.5306 acc_train: 0.5714 loss_val: 1.5036 acc_val: 0.7143 time: 0.1463s\n",
            "Epoch: 0008 loss_train: 1.9127 acc_train: 0.2857 loss_val: 1.4405 acc_val: 0.7143 time: 0.1441s\n",
            "Epoch: 0009 loss_train: 1.5397 acc_train: 0.4286 loss_val: 1.3781 acc_val: 0.8571 time: 0.1436s\n",
            "Epoch: 0010 loss_train: 1.4887 acc_train: 0.7143 loss_val: 1.3162 acc_val: 0.8571 time: 0.1389s\n",
            "Epoch: 0011 loss_train: 1.3714 acc_train: 0.7143 loss_val: 1.2536 acc_val: 0.8571 time: 0.1428s\n",
            "Epoch: 0012 loss_train: 1.3377 acc_train: 0.8571 loss_val: 1.1934 acc_val: 0.8571 time: 0.1404s\n",
            "Epoch: 0013 loss_train: 1.6812 acc_train: 0.4286 loss_val: 1.1334 acc_val: 0.8571 time: 0.1462s\n",
            "Epoch: 0014 loss_train: 1.2673 acc_train: 0.7143 loss_val: 1.0761 acc_val: 0.8571 time: 0.1407s\n",
            "Epoch: 0015 loss_train: 1.2002 acc_train: 0.7143 loss_val: 1.0225 acc_val: 0.8571 time: 0.1439s\n",
            "Epoch: 0016 loss_train: 0.9674 acc_train: 1.0000 loss_val: 0.9692 acc_val: 0.8571 time: 0.1456s\n",
            "Epoch: 0017 loss_train: 1.1769 acc_train: 0.7143 loss_val: 0.9175 acc_val: 1.0000 time: 0.1438s\n",
            "Epoch: 0018 loss_train: 1.3744 acc_train: 0.7143 loss_val: 0.8667 acc_val: 1.0000 time: 0.1372s\n",
            "Epoch: 0019 loss_train: 1.4155 acc_train: 0.4286 loss_val: 0.8198 acc_val: 1.0000 time: 0.1449s\n",
            "Epoch: 0020 loss_train: 0.9648 acc_train: 0.8571 loss_val: 0.7752 acc_val: 1.0000 time: 0.1428s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 2.9713s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.6467 accuracy= 0.3600\n",
            "---> run_4_google_news_5w1h_graph_hin.nx f1_macro 0.34492888064316635 acc 0.36\n",
            "Networkfile run_2_google_news_5w1h_graph_hin.nx\n",
            "(227, 227) (227, 227) 7 25\n",
            "Epoch: 0001 loss_train: 1.9912 acc_train: 0.1429 loss_val: 1.8361 acc_val: 0.4286 time: 0.1380s\n",
            "Epoch: 0002 loss_train: 2.1649 acc_train: 0.0000 loss_val: 1.7677 acc_val: 0.4286 time: 0.1449s\n",
            "Epoch: 0003 loss_train: 1.7061 acc_train: 0.1429 loss_val: 1.6888 acc_val: 0.5714 time: 0.1455s\n",
            "Epoch: 0004 loss_train: 1.8651 acc_train: 0.2857 loss_val: 1.6119 acc_val: 0.5714 time: 0.1529s\n",
            "Epoch: 0005 loss_train: 1.7482 acc_train: 0.1429 loss_val: 1.5319 acc_val: 0.5714 time: 0.1455s\n",
            "Epoch: 0006 loss_train: 1.7991 acc_train: 0.4286 loss_val: 1.4481 acc_val: 0.5714 time: 0.1532s\n",
            "Epoch: 0007 loss_train: 1.5881 acc_train: 0.5714 loss_val: 1.3666 acc_val: 0.5714 time: 0.1430s\n",
            "Epoch: 0008 loss_train: 1.2904 acc_train: 0.7143 loss_val: 1.2850 acc_val: 0.8571 time: 0.1453s\n",
            "Epoch: 0009 loss_train: 1.6035 acc_train: 0.5714 loss_val: 1.2062 acc_val: 0.8571 time: 0.1414s\n",
            "Epoch: 0010 loss_train: 1.5698 acc_train: 0.4286 loss_val: 1.1284 acc_val: 0.8571 time: 0.1471s\n",
            "Epoch: 0011 loss_train: 1.0979 acc_train: 0.8571 loss_val: 1.0512 acc_val: 0.8571 time: 0.1425s\n",
            "Epoch: 0012 loss_train: 1.3527 acc_train: 0.5714 loss_val: 0.9767 acc_val: 0.8571 time: 0.1487s\n",
            "Epoch: 0013 loss_train: 1.3306 acc_train: 0.5714 loss_val: 0.9052 acc_val: 0.8571 time: 0.1478s\n",
            "Epoch: 0014 loss_train: 0.5923 acc_train: 1.0000 loss_val: 0.8363 acc_val: 1.0000 time: 0.1475s\n",
            "Epoch: 0015 loss_train: 1.1164 acc_train: 0.7143 loss_val: 0.7735 acc_val: 1.0000 time: 0.1410s\n",
            "Epoch: 0016 loss_train: 0.9557 acc_train: 0.7143 loss_val: 0.7152 acc_val: 1.0000 time: 0.1499s\n",
            "Epoch: 0017 loss_train: 0.8310 acc_train: 0.7143 loss_val: 0.6605 acc_val: 1.0000 time: 0.1413s\n",
            "Epoch: 0018 loss_train: 0.8235 acc_train: 1.0000 loss_val: 0.6108 acc_val: 1.0000 time: 0.1441s\n",
            "Epoch: 0019 loss_train: 0.8289 acc_train: 0.8571 loss_val: 0.5644 acc_val: 1.0000 time: 0.1399s\n",
            "Epoch: 0020 loss_train: 0.9506 acc_train: 1.0000 loss_val: 0.5212 acc_val: 1.0000 time: 0.1425s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.0026s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.4458 accuracy= 0.5600\n",
            "---> run_2_google_news_5w1h_graph_hin.nx f1_macro 0.5071428571428572 acc 0.56\n",
            "Networkfile run_7_gold_standard_5w1h_graph_hin.nx\n",
            "(579, 579) (579, 579) 20 76\n",
            "Epoch: 0001 loss_train: 2.4575 acc_train: 0.1500 loss_val: 2.4980 acc_val: 0.2000 time: 0.8429s\n",
            "Epoch: 0002 loss_train: 2.5649 acc_train: 0.2000 loss_val: 2.3935 acc_val: 0.4000 time: 0.8159s\n",
            "Epoch: 0003 loss_train: 2.4011 acc_train: 0.1500 loss_val: 2.2882 acc_val: 0.6000 time: 0.8600s\n",
            "Epoch: 0004 loss_train: 2.4580 acc_train: 0.2000 loss_val: 2.1900 acc_val: 0.6500 time: 0.8429s\n",
            "Epoch: 0005 loss_train: 2.4533 acc_train: 0.3000 loss_val: 2.0963 acc_val: 0.7000 time: 0.8729s\n",
            "Epoch: 0006 loss_train: 2.4798 acc_train: 0.4500 loss_val: 2.0061 acc_val: 0.8000 time: 0.7657s\n",
            "Epoch: 0007 loss_train: 2.0570 acc_train: 0.6500 loss_val: 1.9129 acc_val: 0.8000 time: 0.8515s\n",
            "Epoch: 0008 loss_train: 1.9862 acc_train: 0.6000 loss_val: 1.8197 acc_val: 0.8500 time: 0.7825s\n",
            "Epoch: 0009 loss_train: 1.9939 acc_train: 0.5500 loss_val: 1.7268 acc_val: 0.9000 time: 0.8430s\n",
            "Epoch: 0010 loss_train: 1.7636 acc_train: 0.5500 loss_val: 1.6351 acc_val: 0.9000 time: 0.8300s\n",
            "Epoch: 0011 loss_train: 1.7268 acc_train: 0.8000 loss_val: 1.5448 acc_val: 0.9000 time: 0.8381s\n",
            "Epoch: 0012 loss_train: 1.7039 acc_train: 0.7500 loss_val: 1.4560 acc_val: 0.9000 time: 0.8408s\n",
            "Epoch: 0013 loss_train: 1.4773 acc_train: 0.8500 loss_val: 1.3674 acc_val: 1.0000 time: 0.8291s\n",
            "Epoch: 0014 loss_train: 1.5769 acc_train: 0.7500 loss_val: 1.2821 acc_val: 1.0000 time: 0.8314s\n",
            "Epoch: 0015 loss_train: 1.3629 acc_train: 0.9000 loss_val: 1.1988 acc_val: 1.0000 time: 0.8554s\n",
            "Epoch: 0016 loss_train: 1.4266 acc_train: 0.7500 loss_val: 1.1204 acc_val: 1.0000 time: 0.8409s\n",
            "Epoch: 0017 loss_train: 1.3145 acc_train: 0.8500 loss_val: 1.0454 acc_val: 1.0000 time: 0.8625s\n",
            "Epoch: 0018 loss_train: 1.1875 acc_train: 0.9500 loss_val: 0.9727 acc_val: 1.0000 time: 0.8669s\n",
            "Epoch: 0019 loss_train: 1.1871 acc_train: 0.9000 loss_val: 0.9031 acc_val: 1.0000 time: 0.8458s\n",
            "Epoch: 0020 loss_train: 1.3014 acc_train: 0.9500 loss_val: 0.8385 acc_val: 1.0000 time: 0.8419s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.9379s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.7247 accuracy= 0.5789\n",
            "---> run_7_gold_standard_5w1h_graph_hin.nx f1_macro 0.4598396839907876 acc 0.5789473684210527\n",
            "Networkfile run_4_gold_standard_5w1h_graph_hin.nx\n",
            "(579, 579) (579, 579) 20 76\n",
            "Epoch: 0001 loss_train: 2.6585 acc_train: 0.2000 loss_val: 2.4402 acc_val: 0.2500 time: 0.9143s\n",
            "Epoch: 0002 loss_train: 2.4274 acc_train: 0.1000 loss_val: 2.3275 acc_val: 0.4000 time: 0.8507s\n",
            "Epoch: 0003 loss_train: 2.3358 acc_train: 0.3500 loss_val: 2.2131 acc_val: 0.5000 time: 0.8160s\n",
            "Epoch: 0004 loss_train: 2.2278 acc_train: 0.5500 loss_val: 2.1032 acc_val: 0.6000 time: 0.8196s\n",
            "Epoch: 0005 loss_train: 2.1973 acc_train: 0.3500 loss_val: 1.9879 acc_val: 0.7500 time: 0.8559s\n",
            "Epoch: 0006 loss_train: 2.1267 acc_train: 0.3500 loss_val: 1.8739 acc_val: 0.8000 time: 0.8534s\n",
            "Epoch: 0007 loss_train: 2.0900 acc_train: 0.5500 loss_val: 1.7652 acc_val: 0.8000 time: 0.8261s\n",
            "Epoch: 0008 loss_train: 1.8586 acc_train: 0.6500 loss_val: 1.6595 acc_val: 0.8000 time: 0.8007s\n",
            "Epoch: 0009 loss_train: 2.0257 acc_train: 0.6000 loss_val: 1.5568 acc_val: 0.8000 time: 0.8251s\n",
            "Epoch: 0010 loss_train: 1.6291 acc_train: 0.8000 loss_val: 1.4562 acc_val: 0.8500 time: 0.8356s\n",
            "Epoch: 0011 loss_train: 1.4376 acc_train: 0.8000 loss_val: 1.3581 acc_val: 0.8500 time: 0.8313s\n",
            "Epoch: 0012 loss_train: 1.5750 acc_train: 0.8500 loss_val: 1.2615 acc_val: 0.9000 time: 0.8439s\n",
            "Epoch: 0013 loss_train: 1.4368 acc_train: 0.8000 loss_val: 1.1660 acc_val: 0.9500 time: 0.8300s\n",
            "Epoch: 0014 loss_train: 1.2438 acc_train: 0.8500 loss_val: 1.0732 acc_val: 1.0000 time: 0.8428s\n",
            "Epoch: 0015 loss_train: 0.9632 acc_train: 0.9000 loss_val: 0.9864 acc_val: 1.0000 time: 0.8305s\n",
            "Epoch: 0016 loss_train: 1.2196 acc_train: 0.8500 loss_val: 0.9069 acc_val: 1.0000 time: 0.8196s\n",
            "Epoch: 0017 loss_train: 1.0942 acc_train: 0.8500 loss_val: 0.8306 acc_val: 1.0000 time: 0.7921s\n",
            "Epoch: 0018 loss_train: 1.0239 acc_train: 1.0000 loss_val: 0.7606 acc_val: 1.0000 time: 0.7849s\n",
            "Epoch: 0019 loss_train: 1.2217 acc_train: 0.8000 loss_val: 0.6949 acc_val: 1.0000 time: 0.8332s\n",
            "Epoch: 0020 loss_train: 1.0706 acc_train: 0.9500 loss_val: 0.6320 acc_val: 1.0000 time: 0.8300s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.8099s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.7332 accuracy= 0.5263\n",
            "---> run_4_gold_standard_5w1h_graph_hin.nx f1_macro 0.43145465645465647 acc 0.5263157894736842\n",
            "Networkfile run_5_40er_5w1h_graph_hin.nx\n",
            "(249, 249) (249, 249) 8 32\n",
            "Epoch: 0001 loss_train: 1.1621 acc_train: 0.2500 loss_val: 0.9561 acc_val: 0.7500 time: 0.1721s\n",
            "Epoch: 0002 loss_train: 1.2741 acc_train: 0.3750 loss_val: 0.8984 acc_val: 0.7500 time: 0.1662s\n",
            "Epoch: 0003 loss_train: 1.0560 acc_train: 0.5000 loss_val: 0.8328 acc_val: 0.7500 time: 0.1716s\n",
            "Epoch: 0004 loss_train: 1.0728 acc_train: 0.6250 loss_val: 0.7699 acc_val: 0.7500 time: 0.1775s\n",
            "Epoch: 0005 loss_train: 0.9544 acc_train: 0.6250 loss_val: 0.7128 acc_val: 0.7500 time: 0.1591s\n",
            "Epoch: 0006 loss_train: 0.8536 acc_train: 0.3750 loss_val: 0.6561 acc_val: 0.8750 time: 0.1688s\n",
            "Epoch: 0007 loss_train: 0.9017 acc_train: 0.6250 loss_val: 0.6027 acc_val: 0.8750 time: 0.1686s\n",
            "Epoch: 0008 loss_train: 0.7519 acc_train: 0.6250 loss_val: 0.5527 acc_val: 0.8750 time: 0.1747s\n",
            "Epoch: 0009 loss_train: 0.8626 acc_train: 0.7500 loss_val: 0.5061 acc_val: 0.8750 time: 0.1689s\n",
            "Epoch: 0010 loss_train: 0.8327 acc_train: 0.6250 loss_val: 0.4623 acc_val: 0.8750 time: 0.1761s\n",
            "Epoch: 0011 loss_train: 0.6636 acc_train: 0.8750 loss_val: 0.4208 acc_val: 0.8750 time: 0.1742s\n",
            "Epoch: 0012 loss_train: 0.7129 acc_train: 0.7500 loss_val: 0.3817 acc_val: 1.0000 time: 0.1859s\n",
            "Epoch: 0013 loss_train: 0.6317 acc_train: 0.7500 loss_val: 0.3455 acc_val: 1.0000 time: 0.1703s\n",
            "Epoch: 0014 loss_train: 0.3091 acc_train: 1.0000 loss_val: 0.3131 acc_val: 1.0000 time: 0.1740s\n",
            "Epoch: 0015 loss_train: 0.5038 acc_train: 0.8750 loss_val: 0.2824 acc_val: 1.0000 time: 0.1519s\n",
            "Epoch: 0016 loss_train: 0.3088 acc_train: 1.0000 loss_val: 0.2547 acc_val: 1.0000 time: 0.1233s\n",
            "Epoch: 0017 loss_train: 0.3504 acc_train: 1.0000 loss_val: 0.2301 acc_val: 1.0000 time: 0.1650s\n",
            "Epoch: 0018 loss_train: 0.3742 acc_train: 1.0000 loss_val: 0.2082 acc_val: 1.0000 time: 0.1675s\n",
            "Epoch: 0019 loss_train: 0.2003 acc_train: 1.0000 loss_val: 0.1893 acc_val: 1.0000 time: 0.1651s\n",
            "Epoch: 0020 loss_train: 0.3276 acc_train: 1.0000 loss_val: 0.1715 acc_val: 1.0000 time: 0.1682s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.4492s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 0.7613 accuracy= 0.6875\n",
            "---> run_5_40er_5w1h_graph_hin.nx f1_macro 0.5936507936507938 acc 0.6875\n",
            "Networkfile run_3_gold_standard_5w1h_graph_hin.nx\n",
            "(579, 579) (579, 579) 20 76\n",
            "Epoch: 0001 loss_train: 2.3931 acc_train: 0.2500 loss_val: 2.4439 acc_val: 0.3000 time: 0.8384s\n",
            "Epoch: 0002 loss_train: 2.4534 acc_train: 0.1000 loss_val: 2.3098 acc_val: 0.5000 time: 0.8684s\n",
            "Epoch: 0003 loss_train: 2.3936 acc_train: 0.3500 loss_val: 2.1781 acc_val: 0.6500 time: 0.8600s\n",
            "Epoch: 0004 loss_train: 2.1206 acc_train: 0.5000 loss_val: 2.0478 acc_val: 0.7500 time: 0.8991s\n",
            "Epoch: 0005 loss_train: 2.0756 acc_train: 0.4500 loss_val: 1.9155 acc_val: 0.8500 time: 0.8713s\n",
            "Epoch: 0006 loss_train: 1.9619 acc_train: 0.6500 loss_val: 1.7871 acc_val: 0.8500 time: 0.8544s\n",
            "Epoch: 0007 loss_train: 1.9097 acc_train: 0.8000 loss_val: 1.6691 acc_val: 0.9000 time: 0.8638s\n",
            "Epoch: 0008 loss_train: 1.8884 acc_train: 0.6500 loss_val: 1.5552 acc_val: 1.0000 time: 0.8113s\n",
            "Epoch: 0009 loss_train: 1.7088 acc_train: 0.7500 loss_val: 1.4444 acc_val: 0.9500 time: 0.8580s\n",
            "Epoch: 0010 loss_train: 1.4747 acc_train: 0.7500 loss_val: 1.3372 acc_val: 0.9500 time: 0.8743s\n",
            "Epoch: 0011 loss_train: 1.3902 acc_train: 0.8500 loss_val: 1.2366 acc_val: 0.9500 time: 0.8043s\n",
            "Epoch: 0012 loss_train: 1.4162 acc_train: 0.7500 loss_val: 1.1405 acc_val: 0.9500 time: 0.8366s\n",
            "Epoch: 0013 loss_train: 1.2933 acc_train: 0.9500 loss_val: 1.0488 acc_val: 0.9500 time: 0.8405s\n",
            "Epoch: 0014 loss_train: 1.0361 acc_train: 0.9000 loss_val: 0.9619 acc_val: 0.9500 time: 0.8427s\n",
            "Epoch: 0015 loss_train: 1.2271 acc_train: 0.8500 loss_val: 0.8814 acc_val: 0.9500 time: 0.8011s\n",
            "Epoch: 0016 loss_train: 1.0608 acc_train: 0.9000 loss_val: 0.8060 acc_val: 0.9500 time: 0.8585s\n",
            "Epoch: 0017 loss_train: 1.1520 acc_train: 0.9500 loss_val: 0.7354 acc_val: 0.9500 time: 0.8443s\n",
            "Epoch: 0018 loss_train: 1.0780 acc_train: 0.9500 loss_val: 0.6699 acc_val: 0.9500 time: 0.8786s\n",
            "Epoch: 0019 loss_train: 0.8342 acc_train: 0.9000 loss_val: 0.6088 acc_val: 0.9500 time: 0.8312s\n",
            "Epoch: 0020 loss_train: 1.0775 acc_train: 0.8000 loss_val: 0.5548 acc_val: 1.0000 time: 0.7957s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 17.0926s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.6396 accuracy= 0.5789\n",
            "---> run_3_gold_standard_5w1h_graph_hin.nx f1_macro 0.4205339105339105 acc 0.5789473684210527\n",
            "Networkfile run_4_40er_5w1h_graph_hin.nx\n",
            "(249, 249) (249, 249) 8 32\n",
            "Epoch: 0001 loss_train: 1.3140 acc_train: 0.1250 loss_val: 1.0639 acc_val: 0.3750 time: 0.1831s\n",
            "Epoch: 0002 loss_train: 1.5398 acc_train: 0.3750 loss_val: 1.0122 acc_val: 0.5000 time: 0.1644s\n",
            "Epoch: 0003 loss_train: 1.8467 acc_train: 0.2500 loss_val: 0.9581 acc_val: 0.5000 time: 0.1657s\n",
            "Epoch: 0004 loss_train: 1.8227 acc_train: 0.3750 loss_val: 0.9121 acc_val: 0.6250 time: 0.1596s\n",
            "Epoch: 0005 loss_train: 1.0458 acc_train: 0.2500 loss_val: 0.8581 acc_val: 0.7500 time: 0.1379s\n",
            "Epoch: 0006 loss_train: 0.9454 acc_train: 0.6250 loss_val: 0.7999 acc_val: 0.8750 time: 0.1655s\n",
            "Epoch: 0007 loss_train: 1.3749 acc_train: 0.2500 loss_val: 0.7447 acc_val: 0.8750 time: 0.1564s\n",
            "Epoch: 0008 loss_train: 1.1990 acc_train: 0.5000 loss_val: 0.6918 acc_val: 0.8750 time: 0.1438s\n",
            "Epoch: 0009 loss_train: 0.9330 acc_train: 0.6250 loss_val: 0.6406 acc_val: 1.0000 time: 0.1650s\n",
            "Epoch: 0010 loss_train: 0.9955 acc_train: 0.5000 loss_val: 0.5951 acc_val: 1.0000 time: 0.1714s\n",
            "Epoch: 0011 loss_train: 0.6594 acc_train: 0.8750 loss_val: 0.5510 acc_val: 1.0000 time: 0.1682s\n",
            "Epoch: 0012 loss_train: 0.5935 acc_train: 0.8750 loss_val: 0.5093 acc_val: 1.0000 time: 0.1645s\n",
            "Epoch: 0013 loss_train: 0.4621 acc_train: 1.0000 loss_val: 0.4712 acc_val: 1.0000 time: 0.1603s\n",
            "Epoch: 0014 loss_train: 0.5730 acc_train: 0.7500 loss_val: 0.4349 acc_val: 1.0000 time: 0.1589s\n",
            "Epoch: 0015 loss_train: 0.4022 acc_train: 1.0000 loss_val: 0.4003 acc_val: 1.0000 time: 0.1659s\n",
            "Epoch: 0016 loss_train: 0.5826 acc_train: 1.0000 loss_val: 0.3686 acc_val: 1.0000 time: 0.1673s\n",
            "Epoch: 0017 loss_train: 0.6831 acc_train: 0.8750 loss_val: 0.3400 acc_val: 1.0000 time: 0.1545s\n",
            "Epoch: 0018 loss_train: 0.6283 acc_train: 0.7500 loss_val: 0.3130 acc_val: 1.0000 time: 0.1650s\n",
            "Epoch: 0019 loss_train: 0.2986 acc_train: 1.0000 loss_val: 0.2881 acc_val: 1.0000 time: 0.1629s\n",
            "Epoch: 0020 loss_train: 0.4248 acc_train: 1.0000 loss_val: 0.2638 acc_val: 1.0000 time: 0.1596s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.3288s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 0.7623 accuracy= 0.6875\n",
            "---> run_4_40er_5w1h_graph_hin.nx f1_macro 0.48926237161531283 acc 0.6875\n",
            "Networkfile run_5_google_news_5w1h_graph_hin.nx\n",
            "(227, 227) (227, 227) 7 25\n",
            "Epoch: 0001 loss_train: 2.0300 acc_train: 0.2857 loss_val: 1.9349 acc_val: 0.2857 time: 0.1426s\n",
            "Epoch: 0002 loss_train: 2.0077 acc_train: 0.2857 loss_val: 1.8852 acc_val: 0.2857 time: 0.1370s\n",
            "Epoch: 0003 loss_train: 1.9695 acc_train: 0.1429 loss_val: 1.8259 acc_val: 0.2857 time: 0.1415s\n",
            "Epoch: 0004 loss_train: 2.4449 acc_train: 0.0000 loss_val: 1.7630 acc_val: 0.4286 time: 0.1404s\n",
            "Epoch: 0005 loss_train: 2.0254 acc_train: 0.2857 loss_val: 1.6959 acc_val: 0.7143 time: 0.1406s\n",
            "Epoch: 0006 loss_train: 1.9776 acc_train: 0.1429 loss_val: 1.6297 acc_val: 0.7143 time: 0.1369s\n",
            "Epoch: 0007 loss_train: 1.5552 acc_train: 0.4286 loss_val: 1.5629 acc_val: 0.7143 time: 0.1465s\n",
            "Epoch: 0008 loss_train: 1.6006 acc_train: 0.4286 loss_val: 1.4927 acc_val: 0.7143 time: 0.1168s\n",
            "Epoch: 0009 loss_train: 1.3963 acc_train: 0.5714 loss_val: 1.4213 acc_val: 0.7143 time: 0.1154s\n",
            "Epoch: 0010 loss_train: 1.6221 acc_train: 0.4286 loss_val: 1.3502 acc_val: 0.7143 time: 0.1447s\n",
            "Epoch: 0011 loss_train: 1.3772 acc_train: 0.5714 loss_val: 1.2819 acc_val: 0.7143 time: 0.1433s\n",
            "Epoch: 0012 loss_train: 1.3017 acc_train: 0.7143 loss_val: 1.2142 acc_val: 0.8571 time: 0.1455s\n",
            "Epoch: 0013 loss_train: 1.2942 acc_train: 0.7143 loss_val: 1.1488 acc_val: 0.8571 time: 0.1403s\n",
            "Epoch: 0014 loss_train: 1.2909 acc_train: 0.5714 loss_val: 1.0839 acc_val: 0.8571 time: 0.1380s\n",
            "Epoch: 0015 loss_train: 1.1935 acc_train: 0.7143 loss_val: 1.0189 acc_val: 0.8571 time: 0.1383s\n",
            "Epoch: 0016 loss_train: 1.2226 acc_train: 1.0000 loss_val: 0.9559 acc_val: 0.8571 time: 0.1411s\n",
            "Epoch: 0017 loss_train: 1.2725 acc_train: 0.8571 loss_val: 0.8960 acc_val: 1.0000 time: 0.1396s\n",
            "Epoch: 0018 loss_train: 1.5569 acc_train: 0.5714 loss_val: 0.8398 acc_val: 1.0000 time: 0.1341s\n",
            "Epoch: 0019 loss_train: 1.2644 acc_train: 0.5714 loss_val: 0.7869 acc_val: 1.0000 time: 0.1002s\n",
            "Epoch: 0020 loss_train: 0.7810 acc_train: 0.8571 loss_val: 0.7383 acc_val: 1.0000 time: 0.1561s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 2.8257s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.4507 accuracy= 0.4800\n",
            "---> run_5_google_news_5w1h_graph_hin.nx f1_macro 0.3487940630797774 acc 0.48\n",
            "Networkfile run_10_40er_5w1h_graph_hin.nx\n",
            "(249, 249) (249, 249) 8 32\n",
            "Epoch: 0001 loss_train: 1.2242 acc_train: 0.2500 loss_val: 0.9804 acc_val: 0.5000 time: 0.1650s\n",
            "Epoch: 0002 loss_train: 1.4413 acc_train: 0.3750 loss_val: 0.9402 acc_val: 0.5000 time: 0.1662s\n",
            "Epoch: 0003 loss_train: 0.9567 acc_train: 0.3750 loss_val: 0.8777 acc_val: 0.6250 time: 0.1686s\n",
            "Epoch: 0004 loss_train: 1.0057 acc_train: 0.3750 loss_val: 0.8158 acc_val: 0.7500 time: 0.1646s\n",
            "Epoch: 0005 loss_train: 0.8921 acc_train: 0.5000 loss_val: 0.7541 acc_val: 0.7500 time: 0.1702s\n",
            "Epoch: 0006 loss_train: 0.9590 acc_train: 0.6250 loss_val: 0.6976 acc_val: 0.7500 time: 0.1628s\n",
            "Epoch: 0007 loss_train: 0.9329 acc_train: 0.7500 loss_val: 0.6428 acc_val: 0.8750 time: 0.1748s\n",
            "Epoch: 0008 loss_train: 0.7800 acc_train: 0.6250 loss_val: 0.5919 acc_val: 0.8750 time: 0.1406s\n",
            "Epoch: 0009 loss_train: 0.8090 acc_train: 0.5000 loss_val: 0.5446 acc_val: 0.8750 time: 0.1674s\n",
            "Epoch: 0010 loss_train: 0.7296 acc_train: 0.7500 loss_val: 0.5020 acc_val: 1.0000 time: 0.1262s\n",
            "Epoch: 0011 loss_train: 0.6904 acc_train: 0.6250 loss_val: 0.4612 acc_val: 1.0000 time: 0.1590s\n",
            "Epoch: 0012 loss_train: 0.6389 acc_train: 0.7500 loss_val: 0.4243 acc_val: 1.0000 time: 0.1715s\n",
            "Epoch: 0013 loss_train: 0.4268 acc_train: 1.0000 loss_val: 0.3908 acc_val: 1.0000 time: 0.1666s\n",
            "Epoch: 0014 loss_train: 0.6759 acc_train: 0.7500 loss_val: 0.3600 acc_val: 1.0000 time: 0.1227s\n",
            "Epoch: 0015 loss_train: 0.4721 acc_train: 1.0000 loss_val: 0.3301 acc_val: 1.0000 time: 0.1573s\n",
            "Epoch: 0016 loss_train: 0.6290 acc_train: 1.0000 loss_val: 0.3029 acc_val: 1.0000 time: 0.1635s\n",
            "Epoch: 0017 loss_train: 0.4321 acc_train: 0.8750 loss_val: 0.2766 acc_val: 1.0000 time: 0.1685s\n",
            "Epoch: 0018 loss_train: 0.6246 acc_train: 0.7500 loss_val: 0.2529 acc_val: 1.0000 time: 0.1754s\n",
            "Epoch: 0019 loss_train: 0.2769 acc_train: 1.0000 loss_val: 0.2314 acc_val: 1.0000 time: 0.1617s\n",
            "Epoch: 0020 loss_train: 0.4317 acc_train: 0.8750 loss_val: 0.2106 acc_val: 1.0000 time: 0.1599s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.2984s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 0.7463 accuracy= 0.6875\n",
            "---> run_10_40er_5w1h_graph_hin.nx f1_macro 0.49731800766283535 acc 0.6875\n",
            "Networkfile run_9_40er_5w1h_graph_hin.nx\n",
            "(249, 249) (249, 249) 8 32\n",
            "Epoch: 0001 loss_train: 1.2945 acc_train: 0.2500 loss_val: 1.0122 acc_val: 0.5000 time: 0.1699s\n",
            "Epoch: 0002 loss_train: 0.9928 acc_train: 0.6250 loss_val: 0.9461 acc_val: 0.5000 time: 0.1604s\n",
            "Epoch: 0003 loss_train: 1.2756 acc_train: 0.2500 loss_val: 0.8719 acc_val: 0.5000 time: 0.1749s\n",
            "Epoch: 0004 loss_train: 1.2113 acc_train: 0.5000 loss_val: 0.8061 acc_val: 0.7500 time: 0.1657s\n",
            "Epoch: 0005 loss_train: 1.0876 acc_train: 0.5000 loss_val: 0.7403 acc_val: 0.8750 time: 0.1674s\n",
            "Epoch: 0006 loss_train: 1.0158 acc_train: 0.3750 loss_val: 0.6775 acc_val: 0.8750 time: 0.1619s\n",
            "Epoch: 0007 loss_train: 0.9654 acc_train: 0.6250 loss_val: 0.6209 acc_val: 0.8750 time: 0.1704s\n",
            "Epoch: 0008 loss_train: 0.8564 acc_train: 0.6250 loss_val: 0.5680 acc_val: 0.8750 time: 0.1666s\n",
            "Epoch: 0009 loss_train: 0.6218 acc_train: 0.8750 loss_val: 0.5167 acc_val: 0.8750 time: 0.1688s\n",
            "Epoch: 0010 loss_train: 0.7532 acc_train: 0.7500 loss_val: 0.4708 acc_val: 0.8750 time: 0.1626s\n",
            "Epoch: 0011 loss_train: 0.5929 acc_train: 0.8750 loss_val: 0.4279 acc_val: 0.8750 time: 0.1712s\n",
            "Epoch: 0012 loss_train: 0.6837 acc_train: 0.7500 loss_val: 0.3887 acc_val: 1.0000 time: 0.1686s\n",
            "Epoch: 0013 loss_train: 0.6611 acc_train: 0.7500 loss_val: 0.3533 acc_val: 1.0000 time: 0.1794s\n",
            "Epoch: 0014 loss_train: 0.5343 acc_train: 0.8750 loss_val: 0.3213 acc_val: 1.0000 time: 0.1730s\n",
            "Epoch: 0015 loss_train: 0.5355 acc_train: 0.7500 loss_val: 0.2910 acc_val: 1.0000 time: 0.1322s\n",
            "Epoch: 0016 loss_train: 0.4705 acc_train: 1.0000 loss_val: 0.2636 acc_val: 1.0000 time: 0.1356s\n",
            "Epoch: 0017 loss_train: 0.4530 acc_train: 1.0000 loss_val: 0.2393 acc_val: 1.0000 time: 0.1811s\n",
            "Epoch: 0018 loss_train: 0.5388 acc_train: 0.8750 loss_val: 0.2173 acc_val: 1.0000 time: 0.1602s\n",
            "Epoch: 0019 loss_train: 0.2624 acc_train: 1.0000 loss_val: 0.1977 acc_val: 1.0000 time: 0.1594s\n",
            "Epoch: 0020 loss_train: 0.4518 acc_train: 0.8750 loss_val: 0.1797 acc_val: 1.0000 time: 0.1647s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.4028s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 0.5850 accuracy= 0.8125\n",
            "---> run_9_40er_5w1h_graph_hin.nx f1_macro 0.5912698412698413 acc 0.8125\n",
            "Networkfile run_10_google_news_5w1h_graph_hin.nx\n",
            "(227, 227) (227, 227) 7 25\n",
            "Epoch: 0001 loss_train: 1.7361 acc_train: 0.1429 loss_val: 1.9153 acc_val: 0.4286 time: 0.1403s\n",
            "Epoch: 0002 loss_train: 2.0196 acc_train: 0.1429 loss_val: 1.8388 acc_val: 0.4286 time: 0.1435s\n",
            "Epoch: 0003 loss_train: 1.7561 acc_train: 0.1429 loss_val: 1.7606 acc_val: 0.4286 time: 0.1433s\n",
            "Epoch: 0004 loss_train: 2.0477 acc_train: 0.1429 loss_val: 1.6811 acc_val: 0.4286 time: 0.1355s\n",
            "Epoch: 0005 loss_train: 1.8247 acc_train: 0.2857 loss_val: 1.6007 acc_val: 0.4286 time: 0.1542s\n",
            "Epoch: 0006 loss_train: 1.9213 acc_train: 0.2857 loss_val: 1.5180 acc_val: 0.8571 time: 0.1516s\n",
            "Epoch: 0007 loss_train: 1.5709 acc_train: 0.4286 loss_val: 1.4397 acc_val: 0.8571 time: 0.1419s\n",
            "Epoch: 0008 loss_train: 1.6003 acc_train: 0.4286 loss_val: 1.3595 acc_val: 0.8571 time: 0.1474s\n",
            "Epoch: 0009 loss_train: 1.3858 acc_train: 0.5714 loss_val: 1.2783 acc_val: 0.8571 time: 0.1434s\n",
            "Epoch: 0010 loss_train: 1.6698 acc_train: 0.4286 loss_val: 1.1978 acc_val: 1.0000 time: 0.1380s\n",
            "Epoch: 0011 loss_train: 1.4152 acc_train: 0.5714 loss_val: 1.1210 acc_val: 1.0000 time: 0.1383s\n",
            "Epoch: 0012 loss_train: 1.2639 acc_train: 0.8571 loss_val: 1.0453 acc_val: 1.0000 time: 0.1380s\n",
            "Epoch: 0013 loss_train: 1.4907 acc_train: 0.8571 loss_val: 0.9728 acc_val: 1.0000 time: 0.1434s\n",
            "Epoch: 0014 loss_train: 1.3263 acc_train: 0.5714 loss_val: 0.9063 acc_val: 1.0000 time: 0.1361s\n",
            "Epoch: 0015 loss_train: 1.0836 acc_train: 0.8571 loss_val: 0.8432 acc_val: 1.0000 time: 0.1423s\n",
            "Epoch: 0016 loss_train: 1.0196 acc_train: 0.8571 loss_val: 0.7835 acc_val: 1.0000 time: 0.1359s\n",
            "Epoch: 0017 loss_train: 1.4023 acc_train: 0.7143 loss_val: 0.7274 acc_val: 1.0000 time: 0.1235s\n",
            "Epoch: 0018 loss_train: 1.1341 acc_train: 0.7143 loss_val: 0.6750 acc_val: 1.0000 time: 0.1396s\n",
            "Epoch: 0019 loss_train: 0.9508 acc_train: 0.8571 loss_val: 0.6263 acc_val: 1.0000 time: 0.1317s\n",
            "Epoch: 0020 loss_train: 0.7455 acc_train: 0.8571 loss_val: 0.5803 acc_val: 1.0000 time: 0.1428s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 2.8989s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.4148 accuracy= 0.6000\n",
            "---> run_10_google_news_5w1h_graph_hin.nx f1_macro 0.5273654916512059 acc 0.6\n",
            "Networkfile run_6_google_news_5w1h_graph_hin.nx\n",
            "(227, 227) (227, 227) 7 25\n",
            "Epoch: 0001 loss_train: 2.1514 acc_train: 0.0000 loss_val: 1.9503 acc_val: 0.2857 time: 0.1412s\n",
            "Epoch: 0002 loss_train: 2.2181 acc_train: 0.1429 loss_val: 1.8755 acc_val: 0.2857 time: 0.1445s\n",
            "Epoch: 0003 loss_train: 2.0575 acc_train: 0.0000 loss_val: 1.7900 acc_val: 0.2857 time: 0.1277s\n",
            "Epoch: 0004 loss_train: 2.1329 acc_train: 0.0000 loss_val: 1.7061 acc_val: 0.4286 time: 0.1120s\n",
            "Epoch: 0005 loss_train: 2.0212 acc_train: 0.1429 loss_val: 1.6249 acc_val: 0.5714 time: 0.1415s\n",
            "Epoch: 0006 loss_train: 1.8487 acc_train: 0.1429 loss_val: 1.5400 acc_val: 0.7143 time: 0.1411s\n",
            "Epoch: 0007 loss_train: 1.7264 acc_train: 0.2857 loss_val: 1.4570 acc_val: 1.0000 time: 0.1369s\n",
            "Epoch: 0008 loss_train: 1.6199 acc_train: 0.4286 loss_val: 1.3745 acc_val: 1.0000 time: 0.1392s\n",
            "Epoch: 0009 loss_train: 1.5007 acc_train: 0.4286 loss_val: 1.2953 acc_val: 1.0000 time: 0.1497s\n",
            "Epoch: 0010 loss_train: 1.6322 acc_train: 0.5714 loss_val: 1.2188 acc_val: 1.0000 time: 0.1288s\n",
            "Epoch: 0011 loss_train: 1.4004 acc_train: 0.7143 loss_val: 1.1455 acc_val: 1.0000 time: 0.1202s\n",
            "Epoch: 0012 loss_train: 1.3689 acc_train: 0.7143 loss_val: 1.0737 acc_val: 1.0000 time: 0.1400s\n",
            "Epoch: 0013 loss_train: 1.1918 acc_train: 1.0000 loss_val: 1.0061 acc_val: 1.0000 time: 0.1372s\n",
            "Epoch: 0014 loss_train: 1.2476 acc_train: 0.5714 loss_val: 0.9396 acc_val: 1.0000 time: 0.1171s\n",
            "Epoch: 0015 loss_train: 1.1703 acc_train: 0.7143 loss_val: 0.8774 acc_val: 1.0000 time: 0.1456s\n",
            "Epoch: 0016 loss_train: 1.1919 acc_train: 0.8571 loss_val: 0.8194 acc_val: 1.0000 time: 0.1402s\n",
            "Epoch: 0017 loss_train: 1.3287 acc_train: 0.7143 loss_val: 0.7671 acc_val: 1.0000 time: 0.1360s\n",
            "Epoch: 0018 loss_train: 0.8452 acc_train: 0.8571 loss_val: 0.7181 acc_val: 1.0000 time: 0.1147s\n",
            "Epoch: 0019 loss_train: 1.0807 acc_train: 0.7143 loss_val: 0.6697 acc_val: 1.0000 time: 0.1335s\n",
            "Epoch: 0020 loss_train: 0.9139 acc_train: 0.7143 loss_val: 0.6230 acc_val: 1.0000 time: 0.1460s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 2.8053s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.4857 accuracy= 0.4800\n",
            "---> run_6_google_news_5w1h_graph_hin.nx f1_macro 0.43076923076923074 acc 0.48\n",
            "Networkfile run_7_bbc_5w1h_graph_hin.nx\n",
            "(392, 392) (392, 392) 11 44\n",
            "Epoch: 0001 loss_train: 1.8327 acc_train: 0.1818 loss_val: 1.7033 acc_val: 0.1818 time: 0.3885s\n",
            "Epoch: 0002 loss_train: 1.5356 acc_train: 0.3636 loss_val: 1.6323 acc_val: 0.3636 time: 0.3778s\n",
            "Epoch: 0003 loss_train: 1.8450 acc_train: 0.0909 loss_val: 1.5656 acc_val: 0.3636 time: 0.3978s\n",
            "Epoch: 0004 loss_train: 1.8576 acc_train: 0.1818 loss_val: 1.4989 acc_val: 0.4545 time: 0.3976s\n",
            "Epoch: 0005 loss_train: 1.8488 acc_train: 0.4545 loss_val: 1.4347 acc_val: 0.5455 time: 0.4062s\n",
            "Epoch: 0006 loss_train: 1.5954 acc_train: 0.3636 loss_val: 1.3665 acc_val: 0.5455 time: 0.4001s\n",
            "Epoch: 0007 loss_train: 1.3956 acc_train: 0.4545 loss_val: 1.2986 acc_val: 0.7273 time: 0.4001s\n",
            "Epoch: 0008 loss_train: 1.3975 acc_train: 0.4545 loss_val: 1.2307 acc_val: 0.7273 time: 0.3833s\n",
            "Epoch: 0009 loss_train: 1.3903 acc_train: 0.4545 loss_val: 1.1638 acc_val: 0.9091 time: 0.3941s\n",
            "Epoch: 0010 loss_train: 1.2789 acc_train: 0.6364 loss_val: 1.0996 acc_val: 0.9091 time: 0.3171s\n",
            "Epoch: 0011 loss_train: 1.3174 acc_train: 0.4545 loss_val: 1.0373 acc_val: 0.9091 time: 0.4031s\n",
            "Epoch: 0012 loss_train: 1.0310 acc_train: 0.7273 loss_val: 0.9762 acc_val: 0.9091 time: 0.3960s\n",
            "Epoch: 0013 loss_train: 1.3121 acc_train: 0.7273 loss_val: 0.9190 acc_val: 0.9091 time: 0.4024s\n",
            "Epoch: 0014 loss_train: 1.0544 acc_train: 0.7273 loss_val: 0.8606 acc_val: 0.9091 time: 0.3606s\n",
            "Epoch: 0015 loss_train: 1.1547 acc_train: 0.6364 loss_val: 0.8045 acc_val: 0.9091 time: 0.3992s\n",
            "Epoch: 0016 loss_train: 1.0407 acc_train: 0.7273 loss_val: 0.7504 acc_val: 1.0000 time: 0.3993s\n",
            "Epoch: 0017 loss_train: 0.7886 acc_train: 0.9091 loss_val: 0.6991 acc_val: 1.0000 time: 0.4040s\n",
            "Epoch: 0018 loss_train: 0.8524 acc_train: 0.9091 loss_val: 0.6516 acc_val: 1.0000 time: 0.3949s\n",
            "Epoch: 0019 loss_train: 0.8855 acc_train: 0.7273 loss_val: 0.6072 acc_val: 1.0000 time: 0.4103s\n",
            "Epoch: 0020 loss_train: 1.0075 acc_train: 0.7273 loss_val: 0.5654 acc_val: 1.0000 time: 0.4083s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 7.9715s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.4359 accuracy= 0.4091\n",
            "---> run_7_bbc_5w1h_graph_hin.nx f1_macro 0.38595238095238094 acc 0.4090909090909091\n",
            "Networkfile run_1_gold_standard_5w1h_graph_hin.nx\n",
            "(579, 579) (579, 579) 20 76\n",
            "Epoch: 0001 loss_train: 2.5044 acc_train: 0.2000 loss_val: 2.4777 acc_val: 0.2500 time: 0.8350s\n",
            "Epoch: 0002 loss_train: 2.3656 acc_train: 0.1000 loss_val: 2.3700 acc_val: 0.3000 time: 0.7449s\n",
            "Epoch: 0003 loss_train: 2.5600 acc_train: 0.1500 loss_val: 2.2660 acc_val: 0.6000 time: 0.8215s\n",
            "Epoch: 0004 loss_train: 2.2586 acc_train: 0.3000 loss_val: 2.1668 acc_val: 0.7000 time: 0.8198s\n",
            "Epoch: 0005 loss_train: 2.4453 acc_train: 0.3500 loss_val: 2.0700 acc_val: 0.7000 time: 0.8382s\n",
            "Epoch: 0006 loss_train: 2.1115 acc_train: 0.5000 loss_val: 1.9716 acc_val: 0.7500 time: 0.8337s\n",
            "Epoch: 0007 loss_train: 2.2911 acc_train: 0.3500 loss_val: 1.8728 acc_val: 0.7500 time: 0.7959s\n",
            "Epoch: 0008 loss_train: 1.9790 acc_train: 0.6000 loss_val: 1.7733 acc_val: 0.8000 time: 0.8381s\n",
            "Epoch: 0009 loss_train: 1.8275 acc_train: 0.7500 loss_val: 1.6730 acc_val: 0.9000 time: 0.8274s\n",
            "Epoch: 0010 loss_train: 1.7023 acc_train: 0.6000 loss_val: 1.5757 acc_val: 0.9000 time: 0.8290s\n",
            "Epoch: 0011 loss_train: 1.4804 acc_train: 0.8000 loss_val: 1.4786 acc_val: 0.9000 time: 0.8339s\n",
            "Epoch: 0012 loss_train: 1.6745 acc_train: 0.8000 loss_val: 1.3843 acc_val: 0.9500 time: 0.8389s\n",
            "Epoch: 0013 loss_train: 1.4236 acc_train: 0.8500 loss_val: 1.2898 acc_val: 1.0000 time: 0.8292s\n",
            "Epoch: 0014 loss_train: 1.5517 acc_train: 0.8000 loss_val: 1.1993 acc_val: 1.0000 time: 0.8396s\n",
            "Epoch: 0015 loss_train: 1.1901 acc_train: 0.8500 loss_val: 1.1119 acc_val: 1.0000 time: 0.8457s\n",
            "Epoch: 0016 loss_train: 1.3653 acc_train: 0.7500 loss_val: 1.0288 acc_val: 1.0000 time: 0.8308s\n",
            "Epoch: 0017 loss_train: 1.4413 acc_train: 0.7500 loss_val: 0.9469 acc_val: 1.0000 time: 0.8037s\n",
            "Epoch: 0018 loss_train: 1.1315 acc_train: 0.9000 loss_val: 0.8686 acc_val: 1.0000 time: 0.7900s\n",
            "Epoch: 0019 loss_train: 1.1069 acc_train: 0.9000 loss_val: 0.7949 acc_val: 1.0000 time: 0.8544s\n",
            "Epoch: 0020 loss_train: 1.1885 acc_train: 0.9000 loss_val: 0.7254 acc_val: 1.0000 time: 0.8357s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.6593s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.5593 accuracy= 0.6842\n",
            "---> run_1_gold_standard_5w1h_graph_hin.nx f1_macro 0.5796224866464804 acc 0.6842105263157895\n",
            "Networkfile run_7_google_news_5w1h_graph_hin.nx\n",
            "(227, 227) (227, 227) 7 25\n",
            "Epoch: 0001 loss_train: 1.7527 acc_train: 0.2857 loss_val: 1.9135 acc_val: 0.2857 time: 0.1363s\n",
            "Epoch: 0002 loss_train: 2.2157 acc_train: 0.0000 loss_val: 1.8427 acc_val: 0.2857 time: 0.1413s\n",
            "Epoch: 0003 loss_train: 1.9511 acc_train: 0.1429 loss_val: 1.7684 acc_val: 0.5714 time: 0.1342s\n",
            "Epoch: 0004 loss_train: 2.1041 acc_train: 0.0000 loss_val: 1.6923 acc_val: 0.7143 time: 0.1403s\n",
            "Epoch: 0005 loss_train: 1.7228 acc_train: 0.2857 loss_val: 1.6111 acc_val: 0.7143 time: 0.1377s\n",
            "Epoch: 0006 loss_train: 1.9050 acc_train: 0.1429 loss_val: 1.5285 acc_val: 0.7143 time: 0.1400s\n",
            "Epoch: 0007 loss_train: 1.4224 acc_train: 0.7143 loss_val: 1.4460 acc_val: 0.7143 time: 0.1361s\n",
            "Epoch: 0008 loss_train: 1.5652 acc_train: 0.5714 loss_val: 1.3635 acc_val: 0.8571 time: 0.1467s\n",
            "Epoch: 0009 loss_train: 1.5293 acc_train: 0.4286 loss_val: 1.2845 acc_val: 0.8571 time: 0.1246s\n",
            "Epoch: 0010 loss_train: 1.4124 acc_train: 0.7143 loss_val: 1.2084 acc_val: 0.8571 time: 0.1577s\n",
            "Epoch: 0011 loss_train: 1.2185 acc_train: 0.8571 loss_val: 1.1333 acc_val: 0.8571 time: 0.1396s\n",
            "Epoch: 0012 loss_train: 1.2560 acc_train: 1.0000 loss_val: 1.0634 acc_val: 0.8571 time: 0.1593s\n",
            "Epoch: 0013 loss_train: 1.1839 acc_train: 0.5714 loss_val: 0.9979 acc_val: 0.8571 time: 0.1369s\n",
            "Epoch: 0014 loss_train: 0.8873 acc_train: 0.7143 loss_val: 0.9352 acc_val: 0.8571 time: 0.1388s\n",
            "Epoch: 0015 loss_train: 1.1271 acc_train: 1.0000 loss_val: 0.8755 acc_val: 0.8571 time: 0.1257s\n",
            "Epoch: 0016 loss_train: 0.9510 acc_train: 0.8571 loss_val: 0.8174 acc_val: 1.0000 time: 0.1021s\n",
            "Epoch: 0017 loss_train: 1.0854 acc_train: 0.5714 loss_val: 0.7626 acc_val: 1.0000 time: 0.1400s\n",
            "Epoch: 0018 loss_train: 0.9667 acc_train: 0.7143 loss_val: 0.7110 acc_val: 1.0000 time: 0.1383s\n",
            "Epoch: 0019 loss_train: 1.0162 acc_train: 0.7143 loss_val: 0.6617 acc_val: 1.0000 time: 0.1366s\n",
            "Epoch: 0020 loss_train: 0.9810 acc_train: 0.8571 loss_val: 0.6154 acc_val: 1.0000 time: 0.1377s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 2.8378s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.4741 accuracy= 0.4000\n",
            "---> run_7_google_news_5w1h_graph_hin.nx f1_macro 0.4333333333333333 acc 0.4\n",
            "Networkfile run_10_gold_standard_5w1h_graph_hin.nx\n",
            "(579, 579) (579, 579) 20 76\n",
            "Epoch: 0001 loss_train: 2.7315 acc_train: 0.1000 loss_val: 2.4747 acc_val: 0.1500 time: 0.8554s\n",
            "Epoch: 0002 loss_train: 2.5107 acc_train: 0.0000 loss_val: 2.3788 acc_val: 0.2500 time: 0.8337s\n",
            "Epoch: 0003 loss_train: 2.4228 acc_train: 0.2000 loss_val: 2.2775 acc_val: 0.5000 time: 0.8382s\n",
            "Epoch: 0004 loss_train: 2.2127 acc_train: 0.3500 loss_val: 2.1715 acc_val: 0.7000 time: 0.8333s\n",
            "Epoch: 0005 loss_train: 2.1619 acc_train: 0.3000 loss_val: 2.0637 acc_val: 0.7500 time: 0.8312s\n",
            "Epoch: 0006 loss_train: 2.1599 acc_train: 0.5500 loss_val: 1.9584 acc_val: 0.8000 time: 0.8635s\n",
            "Epoch: 0007 loss_train: 2.0930 acc_train: 0.6500 loss_val: 1.8538 acc_val: 0.8000 time: 0.8418s\n",
            "Epoch: 0008 loss_train: 1.9988 acc_train: 0.4000 loss_val: 1.7499 acc_val: 0.9000 time: 0.7449s\n",
            "Epoch: 0009 loss_train: 1.9360 acc_train: 0.6000 loss_val: 1.6464 acc_val: 0.9000 time: 0.8367s\n",
            "Epoch: 0010 loss_train: 1.6229 acc_train: 0.6500 loss_val: 1.5443 acc_val: 0.9000 time: 0.8212s\n",
            "Epoch: 0011 loss_train: 1.5149 acc_train: 0.8000 loss_val: 1.4448 acc_val: 0.9500 time: 0.8279s\n",
            "Epoch: 0012 loss_train: 1.8330 acc_train: 0.8000 loss_val: 1.3481 acc_val: 0.9500 time: 0.8112s\n",
            "Epoch: 0013 loss_train: 1.4923 acc_train: 0.9000 loss_val: 1.2528 acc_val: 0.9500 time: 0.7976s\n",
            "Epoch: 0014 loss_train: 1.5152 acc_train: 0.7500 loss_val: 1.1591 acc_val: 0.9500 time: 0.8489s\n",
            "Epoch: 0015 loss_train: 1.3046 acc_train: 0.7000 loss_val: 1.0671 acc_val: 0.9500 time: 0.8404s\n",
            "Epoch: 0016 loss_train: 1.2124 acc_train: 0.9000 loss_val: 0.9813 acc_val: 1.0000 time: 0.8406s\n",
            "Epoch: 0017 loss_train: 1.3602 acc_train: 0.8000 loss_val: 0.8962 acc_val: 1.0000 time: 0.8150s\n",
            "Epoch: 0018 loss_train: 1.2456 acc_train: 0.9000 loss_val: 0.8188 acc_val: 1.0000 time: 0.8290s\n",
            "Epoch: 0019 loss_train: 1.1752 acc_train: 0.7500 loss_val: 0.7454 acc_val: 1.0000 time: 0.8269s\n",
            "Epoch: 0020 loss_train: 1.1403 acc_train: 0.9000 loss_val: 0.6770 acc_val: 1.0000 time: 0.8313s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.7151s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.5633 accuracy= 0.6053\n",
            "---> run_10_gold_standard_5w1h_graph_hin.nx f1_macro 0.44276078327802465 acc 0.6052631578947368\n",
            "Networkfile run_6_gold_standard_5w1h_graph_hin.nx\n",
            "(579, 579) (579, 579) 20 76\n",
            "Epoch: 0001 loss_train: 2.5843 acc_train: 0.1500 loss_val: 2.4348 acc_val: 0.2500 time: 0.8261s\n",
            "Epoch: 0002 loss_train: 2.3100 acc_train: 0.2000 loss_val: 2.3143 acc_val: 0.5000 time: 0.8191s\n",
            "Epoch: 0003 loss_train: 2.4703 acc_train: 0.2000 loss_val: 2.1948 acc_val: 0.7000 time: 0.8187s\n",
            "Epoch: 0004 loss_train: 2.3045 acc_train: 0.3000 loss_val: 2.0781 acc_val: 0.7500 time: 0.8559s\n",
            "Epoch: 0005 loss_train: 2.1842 acc_train: 0.5000 loss_val: 1.9629 acc_val: 0.8500 time: 0.7939s\n",
            "Epoch: 0006 loss_train: 1.9629 acc_train: 0.6000 loss_val: 1.8483 acc_val: 0.9000 time: 0.8481s\n",
            "Epoch: 0007 loss_train: 2.1135 acc_train: 0.6000 loss_val: 1.7362 acc_val: 0.9000 time: 0.7939s\n",
            "Epoch: 0008 loss_train: 1.8586 acc_train: 0.7000 loss_val: 1.6265 acc_val: 0.9000 time: 0.7964s\n",
            "Epoch: 0009 loss_train: 1.7100 acc_train: 0.7500 loss_val: 1.5173 acc_val: 0.9000 time: 0.8392s\n",
            "Epoch: 0010 loss_train: 1.5893 acc_train: 0.6500 loss_val: 1.4115 acc_val: 0.9500 time: 0.8473s\n",
            "Epoch: 0011 loss_train: 1.5723 acc_train: 0.8000 loss_val: 1.3104 acc_val: 0.9500 time: 0.8205s\n",
            "Epoch: 0012 loss_train: 1.5612 acc_train: 0.8500 loss_val: 1.2126 acc_val: 1.0000 time: 0.8332s\n",
            "Epoch: 0013 loss_train: 1.2464 acc_train: 0.9000 loss_val: 1.1177 acc_val: 1.0000 time: 0.8359s\n",
            "Epoch: 0014 loss_train: 1.4084 acc_train: 0.9000 loss_val: 1.0291 acc_val: 1.0000 time: 0.8232s\n",
            "Epoch: 0015 loss_train: 1.1070 acc_train: 0.8000 loss_val: 0.9445 acc_val: 1.0000 time: 0.8285s\n",
            "Epoch: 0016 loss_train: 1.1271 acc_train: 0.8500 loss_val: 0.8661 acc_val: 1.0000 time: 0.8483s\n",
            "Epoch: 0017 loss_train: 1.2269 acc_train: 0.8000 loss_val: 0.7901 acc_val: 1.0000 time: 0.8296s\n",
            "Epoch: 0018 loss_train: 1.0682 acc_train: 0.9500 loss_val: 0.7217 acc_val: 1.0000 time: 0.8367s\n",
            "Epoch: 0019 loss_train: 1.1645 acc_train: 0.8500 loss_val: 0.6619 acc_val: 1.0000 time: 0.8256s\n",
            "Epoch: 0020 loss_train: 1.0987 acc_train: 0.9000 loss_val: 0.6049 acc_val: 1.0000 time: 0.8383s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 16.7167s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.6189 accuracy= 0.6184\n",
            "---> run_6_gold_standard_5w1h_graph_hin.nx f1_macro 0.46648351648351655 acc 0.618421052631579\n",
            "Networkfile run_3_bbc_5w1h_graph_hin.nx\n",
            "(392, 392) (392, 392) 11 44\n",
            "Epoch: 0001 loss_train: 1.8004 acc_train: 0.1818 loss_val: 1.5672 acc_val: 0.1818 time: 0.4219s\n",
            "Epoch: 0002 loss_train: 1.6889 acc_train: 0.0909 loss_val: 1.5196 acc_val: 0.2727 time: 0.4117s\n",
            "Epoch: 0003 loss_train: 1.8827 acc_train: 0.1818 loss_val: 1.4609 acc_val: 0.4545 time: 0.3919s\n",
            "Epoch: 0004 loss_train: 1.6030 acc_train: 0.5455 loss_val: 1.4009 acc_val: 0.7273 time: 0.4113s\n",
            "Epoch: 0005 loss_train: 1.6176 acc_train: 0.4545 loss_val: 1.3390 acc_val: 0.8182 time: 0.4118s\n",
            "Epoch: 0006 loss_train: 1.5400 acc_train: 0.3636 loss_val: 1.2731 acc_val: 0.8182 time: 0.3935s\n",
            "Epoch: 0007 loss_train: 1.4396 acc_train: 0.5455 loss_val: 1.2089 acc_val: 0.9091 time: 0.4260s\n",
            "Epoch: 0008 loss_train: 1.2216 acc_train: 0.6364 loss_val: 1.1456 acc_val: 0.9091 time: 0.4000s\n",
            "Epoch: 0009 loss_train: 1.3429 acc_train: 0.4545 loss_val: 1.0830 acc_val: 0.9091 time: 0.4121s\n",
            "Epoch: 0010 loss_train: 1.3202 acc_train: 0.5455 loss_val: 1.0229 acc_val: 0.9091 time: 0.3946s\n",
            "Epoch: 0011 loss_train: 1.0665 acc_train: 0.6364 loss_val: 0.9645 acc_val: 0.9091 time: 0.3758s\n",
            "Epoch: 0012 loss_train: 1.0039 acc_train: 0.8182 loss_val: 0.9075 acc_val: 1.0000 time: 0.3977s\n",
            "Epoch: 0013 loss_train: 1.0241 acc_train: 0.6364 loss_val: 0.8539 acc_val: 1.0000 time: 0.3821s\n",
            "Epoch: 0014 loss_train: 1.0023 acc_train: 0.8182 loss_val: 0.8023 acc_val: 1.0000 time: 0.4029s\n",
            "Epoch: 0015 loss_train: 0.8866 acc_train: 0.8182 loss_val: 0.7530 acc_val: 1.0000 time: 0.3998s\n",
            "Epoch: 0016 loss_train: 1.0457 acc_train: 0.9091 loss_val: 0.7052 acc_val: 1.0000 time: 0.4176s\n",
            "Epoch: 0017 loss_train: 0.9585 acc_train: 0.8182 loss_val: 0.6608 acc_val: 1.0000 time: 0.3922s\n",
            "Epoch: 0018 loss_train: 0.8729 acc_train: 0.8182 loss_val: 0.6196 acc_val: 1.0000 time: 0.3857s\n",
            "Epoch: 0019 loss_train: 0.7672 acc_train: 0.8182 loss_val: 0.5808 acc_val: 1.0000 time: 0.3872s\n",
            "Epoch: 0020 loss_train: 0.6097 acc_train: 0.8182 loss_val: 0.5439 acc_val: 1.0000 time: 0.3788s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 8.1139s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.4177 accuracy= 0.4773\n",
            "---> run_3_bbc_5w1h_graph_hin.nx f1_macro 0.46698412698412695 acc 0.4772727272727273\n",
            "Networkfile run_1_bbc_5w1h_graph_hin.nx\n",
            "(392, 392) (392, 392) 11 44\n",
            "Epoch: 0001 loss_train: 1.5750 acc_train: 0.2727 loss_val: 1.4461 acc_val: 0.5455 time: 0.3988s\n",
            "Epoch: 0002 loss_train: 1.6744 acc_train: 0.3636 loss_val: 1.3791 acc_val: 0.6364 time: 0.4067s\n",
            "Epoch: 0003 loss_train: 1.4768 acc_train: 0.4545 loss_val: 1.3067 acc_val: 0.8182 time: 0.4105s\n",
            "Epoch: 0004 loss_train: 1.4680 acc_train: 0.4545 loss_val: 1.2386 acc_val: 0.8182 time: 0.4111s\n",
            "Epoch: 0005 loss_train: 1.5950 acc_train: 0.3636 loss_val: 1.1699 acc_val: 0.8182 time: 0.3768s\n",
            "Epoch: 0006 loss_train: 1.1860 acc_train: 0.6364 loss_val: 1.0977 acc_val: 0.8182 time: 0.4015s\n",
            "Epoch: 0007 loss_train: 1.1999 acc_train: 0.6364 loss_val: 1.0256 acc_val: 1.0000 time: 0.3918s\n",
            "Epoch: 0008 loss_train: 1.0721 acc_train: 0.6364 loss_val: 0.9566 acc_val: 1.0000 time: 0.4394s\n",
            "Epoch: 0009 loss_train: 1.2310 acc_train: 0.7273 loss_val: 0.8913 acc_val: 1.0000 time: 0.4087s\n",
            "Epoch: 0010 loss_train: 1.0215 acc_train: 0.7273 loss_val: 0.8287 acc_val: 1.0000 time: 0.4106s\n",
            "Epoch: 0011 loss_train: 0.9948 acc_train: 0.9091 loss_val: 0.7695 acc_val: 1.0000 time: 0.4163s\n",
            "Epoch: 0012 loss_train: 1.0248 acc_train: 0.7273 loss_val: 0.7133 acc_val: 1.0000 time: 0.4025s\n",
            "Epoch: 0013 loss_train: 0.9154 acc_train: 0.9091 loss_val: 0.6604 acc_val: 1.0000 time: 0.3925s\n",
            "Epoch: 0014 loss_train: 0.8953 acc_train: 1.0000 loss_val: 0.6106 acc_val: 1.0000 time: 0.4022s\n",
            "Epoch: 0015 loss_train: 0.6904 acc_train: 0.9091 loss_val: 0.5637 acc_val: 1.0000 time: 0.4035s\n",
            "Epoch: 0016 loss_train: 0.7125 acc_train: 1.0000 loss_val: 0.5197 acc_val: 1.0000 time: 0.3998s\n",
            "Epoch: 0017 loss_train: 0.5425 acc_train: 1.0000 loss_val: 0.4773 acc_val: 1.0000 time: 0.3866s\n",
            "Epoch: 0018 loss_train: 0.7793 acc_train: 0.9091 loss_val: 0.4381 acc_val: 1.0000 time: 0.3992s\n",
            "Epoch: 0019 loss_train: 0.6080 acc_train: 0.9091 loss_val: 0.4008 acc_val: 1.0000 time: 0.3823s\n",
            "Epoch: 0020 loss_train: 0.4803 acc_train: 1.0000 loss_val: 0.3659 acc_val: 1.0000 time: 0.4050s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 8.1696s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.4825 accuracy= 0.4545\n",
            "---> run_1_bbc_5w1h_graph_hin.nx f1_macro 0.468759310864574 acc 0.45454545454545453\n",
            "Networkfile run_2_gold_standard_5w1h_graph_hin.nx\n",
            "(579, 579) (579, 579) 20 76\n",
            "Epoch: 0001 loss_train: 2.4436 acc_train: 0.3000 loss_val: 2.4566 acc_val: 0.2500 time: 0.8456s\n",
            "Epoch: 0002 loss_train: 2.4989 acc_train: 0.1500 loss_val: 2.3409 acc_val: 0.3500 time: 0.8604s\n",
            "Epoch: 0003 loss_train: 2.4932 acc_train: 0.2000 loss_val: 2.2301 acc_val: 0.5000 time: 0.8307s\n",
            "Epoch: 0004 loss_train: 2.2087 acc_train: 0.3500 loss_val: 2.1171 acc_val: 0.7000 time: 0.8111s\n",
            "Epoch: 0005 loss_train: 2.2174 acc_train: 0.3500 loss_val: 2.0050 acc_val: 0.8500 time: 0.8514s\n",
            "Epoch: 0006 loss_train: 2.0312 acc_train: 0.5000 loss_val: 1.8955 acc_val: 0.9000 time: 0.8379s\n",
            "Epoch: 0007 loss_train: 2.1548 acc_train: 0.5500 loss_val: 1.7885 acc_val: 0.9000 time: 0.8324s\n",
            "Epoch: 0008 loss_train: 1.9065 acc_train: 0.7500 loss_val: 1.6825 acc_val: 0.9000 time: 0.8125s\n",
            "Epoch: 0009 loss_train: 1.7843 acc_train: 0.6500 loss_val: 1.5762 acc_val: 0.9500 time: 0.8257s\n",
            "Epoch: 0010 loss_train: 1.7650 acc_train: 0.8500 loss_val: 1.4723 acc_val: 1.0000 time: 0.8135s\n",
            "Epoch: 0011 loss_train: 1.5574 acc_train: 0.7000 loss_val: 1.3720 acc_val: 1.0000 time: 0.7862s\n",
            "Epoch: 0012 loss_train: 1.4251 acc_train: 0.9000 loss_val: 1.2757 acc_val: 1.0000 time: 0.8437s\n",
            "Epoch: 0013 loss_train: 1.3867 acc_train: 0.8000 loss_val: 1.1822 acc_val: 1.0000 time: 0.8831s\n",
            "Epoch: 0014 loss_train: 1.4281 acc_train: 0.8000 loss_val: 1.0969 acc_val: 1.0000 time: 0.8838s\n",
            "Epoch: 0015 loss_train: 1.1226 acc_train: 0.8500 loss_val: 1.0146 acc_val: 1.0000 time: 0.8603s\n",
            "Epoch: 0016 loss_train: 1.2355 acc_train: 0.8500 loss_val: 0.9404 acc_val: 1.0000 time: 0.8817s\n",
            "Epoch: 0017 loss_train: 1.4147 acc_train: 0.8500 loss_val: 0.8712 acc_val: 1.0000 time: 0.8784s\n",
            "Epoch: 0018 loss_train: 0.9178 acc_train: 0.9500 loss_val: 0.8047 acc_val: 1.0000 time: 0.8594s\n",
            "Epoch: 0019 loss_train: 0.9249 acc_train: 0.9500 loss_val: 0.7402 acc_val: 1.0000 time: 0.9069s\n",
            "Epoch: 0020 loss_train: 1.0411 acc_train: 0.9500 loss_val: 0.6754 acc_val: 1.0000 time: 0.8592s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 17.1276s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.5872 accuracy= 0.5921\n",
            "---> run_2_gold_standard_5w1h_graph_hin.nx f1_macro 0.4827360426929393 acc 0.5921052631578947\n",
            "Networkfile run_3_google_news_5w1h_graph_hin.nx\n",
            "(227, 227) (227, 227) 7 25\n",
            "Epoch: 0001 loss_train: 2.0580 acc_train: 0.1429 loss_val: 1.8737 acc_val: 0.4286 time: 0.1459s\n",
            "Epoch: 0002 loss_train: 1.9137 acc_train: 0.2857 loss_val: 1.8025 acc_val: 0.5714 time: 0.1505s\n",
            "Epoch: 0003 loss_train: 1.9175 acc_train: 0.2857 loss_val: 1.7262 acc_val: 0.5714 time: 0.1323s\n",
            "Epoch: 0004 loss_train: 2.2730 acc_train: 0.1429 loss_val: 1.6534 acc_val: 0.5714 time: 0.1415s\n",
            "Epoch: 0005 loss_train: 2.0777 acc_train: 0.1429 loss_val: 1.5768 acc_val: 0.7143 time: 0.1415s\n",
            "Epoch: 0006 loss_train: 1.8403 acc_train: 0.1429 loss_val: 1.4985 acc_val: 0.7143 time: 0.1411s\n",
            "Epoch: 0007 loss_train: 1.7125 acc_train: 0.1429 loss_val: 1.4232 acc_val: 0.8571 time: 0.1393s\n",
            "Epoch: 0008 loss_train: 1.5558 acc_train: 0.4286 loss_val: 1.3452 acc_val: 0.8571 time: 0.1428s\n",
            "Epoch: 0009 loss_train: 1.5288 acc_train: 0.5714 loss_val: 1.2687 acc_val: 0.8571 time: 0.1365s\n",
            "Epoch: 0010 loss_train: 1.4875 acc_train: 0.2857 loss_val: 1.1940 acc_val: 0.8571 time: 0.1427s\n",
            "Epoch: 0011 loss_train: 1.2244 acc_train: 0.7143 loss_val: 1.1218 acc_val: 1.0000 time: 0.1472s\n",
            "Epoch: 0012 loss_train: 1.4034 acc_train: 0.8571 loss_val: 1.0513 acc_val: 1.0000 time: 0.1332s\n",
            "Epoch: 0013 loss_train: 1.4236 acc_train: 0.8571 loss_val: 0.9858 acc_val: 1.0000 time: 0.1429s\n",
            "Epoch: 0014 loss_train: 1.3114 acc_train: 0.5714 loss_val: 0.9216 acc_val: 1.0000 time: 0.1463s\n",
            "Epoch: 0015 loss_train: 1.1612 acc_train: 0.8571 loss_val: 0.8582 acc_val: 1.0000 time: 0.1390s\n",
            "Epoch: 0016 loss_train: 1.0372 acc_train: 1.0000 loss_val: 0.7980 acc_val: 1.0000 time: 0.1403s\n",
            "Epoch: 0017 loss_train: 1.1620 acc_train: 0.8571 loss_val: 0.7420 acc_val: 1.0000 time: 0.1439s\n",
            "Epoch: 0018 loss_train: 1.1936 acc_train: 0.5714 loss_val: 0.6913 acc_val: 1.0000 time: 0.1407s\n",
            "Epoch: 0019 loss_train: 0.9280 acc_train: 0.8571 loss_val: 0.6437 acc_val: 1.0000 time: 0.1439s\n",
            "Epoch: 0020 loss_train: 0.8910 acc_train: 1.0000 loss_val: 0.5987 acc_val: 1.0000 time: 0.1445s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 2.9348s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.5300 accuracy= 0.4400\n",
            "---> run_3_google_news_5w1h_graph_hin.nx f1_macro 0.38153717627401834 acc 0.44\n",
            "Networkfile run_9_bbc_5w1h_graph_hin.nx\n",
            "(392, 392) (392, 392) 11 44\n",
            "Epoch: 0001 loss_train: 1.7112 acc_train: 0.1818 loss_val: 1.6277 acc_val: 0.1818 time: 0.4008s\n",
            "Epoch: 0002 loss_train: 1.7230 acc_train: 0.2727 loss_val: 1.5708 acc_val: 0.3636 time: 0.3776s\n",
            "Epoch: 0003 loss_train: 1.8226 acc_train: 0.1818 loss_val: 1.5103 acc_val: 0.4545 time: 0.4081s\n",
            "Epoch: 0004 loss_train: 1.6425 acc_train: 0.2727 loss_val: 1.4424 acc_val: 0.5455 time: 0.4028s\n",
            "Epoch: 0005 loss_train: 1.5834 acc_train: 0.2727 loss_val: 1.3693 acc_val: 0.6364 time: 0.3999s\n",
            "Epoch: 0006 loss_train: 1.5115 acc_train: 0.5455 loss_val: 1.2952 acc_val: 0.8182 time: 0.4019s\n",
            "Epoch: 0007 loss_train: 1.3434 acc_train: 0.5455 loss_val: 1.2199 acc_val: 0.8182 time: 0.3966s\n",
            "Epoch: 0008 loss_train: 1.2543 acc_train: 0.5455 loss_val: 1.1467 acc_val: 0.8182 time: 0.4081s\n",
            "Epoch: 0009 loss_train: 1.3190 acc_train: 0.4545 loss_val: 1.0737 acc_val: 0.9091 time: 0.3971s\n",
            "Epoch: 0010 loss_train: 1.2965 acc_train: 0.6364 loss_val: 1.0040 acc_val: 0.9091 time: 0.3955s\n",
            "Epoch: 0011 loss_train: 1.2112 acc_train: 0.6364 loss_val: 0.9363 acc_val: 0.9091 time: 0.3940s\n",
            "Epoch: 0012 loss_train: 0.9367 acc_train: 0.9091 loss_val: 0.8692 acc_val: 1.0000 time: 0.4028s\n",
            "Epoch: 0013 loss_train: 1.1953 acc_train: 0.7273 loss_val: 0.8043 acc_val: 1.0000 time: 0.3946s\n",
            "Epoch: 0014 loss_train: 1.0053 acc_train: 0.7273 loss_val: 0.7405 acc_val: 1.0000 time: 0.4058s\n",
            "Epoch: 0015 loss_train: 0.8218 acc_train: 0.8182 loss_val: 0.6811 acc_val: 1.0000 time: 0.4203s\n",
            "Epoch: 0016 loss_train: 0.8574 acc_train: 0.8182 loss_val: 0.6253 acc_val: 1.0000 time: 0.3910s\n",
            "Epoch: 0017 loss_train: 0.7208 acc_train: 0.9091 loss_val: 0.5740 acc_val: 1.0000 time: 0.3958s\n",
            "Epoch: 0018 loss_train: 0.8550 acc_train: 1.0000 loss_val: 0.5263 acc_val: 1.0000 time: 0.4097s\n",
            "Epoch: 0019 loss_train: 0.7846 acc_train: 0.8182 loss_val: 0.4822 acc_val: 1.0000 time: 0.4027s\n",
            "Epoch: 0020 loss_train: 0.7188 acc_train: 0.9091 loss_val: 0.4418 acc_val: 1.0000 time: 0.4146s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 8.1460s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.4397 accuracy= 0.5000\n",
            "---> run_9_bbc_5w1h_graph_hin.nx f1_macro 0.4998076923076923 acc 0.5\n",
            "Networkfile run_2_40er_5w1h_graph_hin.nx\n",
            "(249, 249) (249, 249) 8 32\n",
            "Epoch: 0001 loss_train: 1.0899 acc_train: 0.5000 loss_val: 0.9638 acc_val: 0.5000 time: 0.1711s\n",
            "Epoch: 0002 loss_train: 1.3186 acc_train: 0.2500 loss_val: 0.9104 acc_val: 0.7500 time: 0.1579s\n",
            "Epoch: 0003 loss_train: 1.0553 acc_train: 0.6250 loss_val: 0.8435 acc_val: 0.7500 time: 0.1639s\n",
            "Epoch: 0004 loss_train: 1.1711 acc_train: 0.5000 loss_val: 0.7773 acc_val: 0.7500 time: 0.1626s\n",
            "Epoch: 0005 loss_train: 0.9423 acc_train: 0.3750 loss_val: 0.7112 acc_val: 1.0000 time: 0.1758s\n",
            "Epoch: 0006 loss_train: 0.9454 acc_train: 0.3750 loss_val: 0.6466 acc_val: 1.0000 time: 0.1625s\n",
            "Epoch: 0007 loss_train: 1.0006 acc_train: 0.2500 loss_val: 0.5832 acc_val: 1.0000 time: 0.1662s\n",
            "Epoch: 0008 loss_train: 1.1363 acc_train: 0.7500 loss_val: 0.5251 acc_val: 1.0000 time: 0.1638s\n",
            "Epoch: 0009 loss_train: 0.8209 acc_train: 0.6250 loss_val: 0.4713 acc_val: 1.0000 time: 0.1652s\n",
            "Epoch: 0010 loss_train: 0.8921 acc_train: 0.3750 loss_val: 0.4252 acc_val: 1.0000 time: 0.1601s\n",
            "Epoch: 0011 loss_train: 0.7181 acc_train: 0.8750 loss_val: 0.3841 acc_val: 1.0000 time: 0.1717s\n",
            "Epoch: 0012 loss_train: 0.6066 acc_train: 0.6250 loss_val: 0.3469 acc_val: 1.0000 time: 0.1673s\n",
            "Epoch: 0013 loss_train: 0.7104 acc_train: 0.8750 loss_val: 0.3140 acc_val: 1.0000 time: 0.1635s\n",
            "Epoch: 0014 loss_train: 0.4177 acc_train: 0.8750 loss_val: 0.2846 acc_val: 1.0000 time: 0.1695s\n",
            "Epoch: 0015 loss_train: 0.3758 acc_train: 1.0000 loss_val: 0.2578 acc_val: 1.0000 time: 0.1637s\n",
            "Epoch: 0016 loss_train: 0.4311 acc_train: 0.8750 loss_val: 0.2336 acc_val: 1.0000 time: 0.1634s\n",
            "Epoch: 0017 loss_train: 0.4246 acc_train: 1.0000 loss_val: 0.2116 acc_val: 1.0000 time: 0.1684s\n",
            "Epoch: 0018 loss_train: 0.4633 acc_train: 0.8750 loss_val: 0.1921 acc_val: 1.0000 time: 0.1647s\n",
            "Epoch: 0019 loss_train: 0.3571 acc_train: 1.0000 loss_val: 0.1742 acc_val: 1.0000 time: 0.1650s\n",
            "Epoch: 0020 loss_train: 0.3512 acc_train: 1.0000 loss_val: 0.1583 acc_val: 1.0000 time: 0.1651s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.4115s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 0.7384 accuracy= 0.6875\n",
            "---> run_2_40er_5w1h_graph_hin.nx f1_macro 0.49561403508771934 acc 0.6875\n",
            "Networkfile run_8_bbc_5w1h_graph_hin.nx\n",
            "(392, 392) (392, 392) 11 44\n",
            "Epoch: 0001 loss_train: 1.7449 acc_train: 0.2727 loss_val: 1.6290 acc_val: 0.1818 time: 0.4105s\n",
            "Epoch: 0002 loss_train: 1.8012 acc_train: 0.3636 loss_val: 1.5716 acc_val: 0.1818 time: 0.3692s\n",
            "Epoch: 0003 loss_train: 1.6817 acc_train: 0.0000 loss_val: 1.5062 acc_val: 0.3636 time: 0.4078s\n",
            "Epoch: 0004 loss_train: 1.5943 acc_train: 0.1818 loss_val: 1.4372 acc_val: 0.5455 time: 0.3975s\n",
            "Epoch: 0005 loss_train: 2.0733 acc_train: 0.1818 loss_val: 1.3655 acc_val: 0.6364 time: 0.3769s\n",
            "Epoch: 0006 loss_train: 1.4599 acc_train: 0.2727 loss_val: 1.2895 acc_val: 0.8182 time: 0.3945s\n",
            "Epoch: 0007 loss_train: 1.3748 acc_train: 0.4545 loss_val: 1.2164 acc_val: 0.8182 time: 0.4045s\n",
            "Epoch: 0008 loss_train: 1.2427 acc_train: 0.6364 loss_val: 1.1446 acc_val: 0.8182 time: 0.3839s\n",
            "Epoch: 0009 loss_train: 1.3879 acc_train: 0.4545 loss_val: 1.0753 acc_val: 0.9091 time: 0.3868s\n",
            "Epoch: 0010 loss_train: 1.2861 acc_train: 0.4545 loss_val: 1.0095 acc_val: 1.0000 time: 0.3343s\n",
            "Epoch: 0011 loss_train: 1.1305 acc_train: 0.6364 loss_val: 0.9466 acc_val: 1.0000 time: 0.3934s\n",
            "Epoch: 0012 loss_train: 1.1398 acc_train: 0.8182 loss_val: 0.8858 acc_val: 1.0000 time: 0.3988s\n",
            "Epoch: 0013 loss_train: 0.9285 acc_train: 1.0000 loss_val: 0.8289 acc_val: 1.0000 time: 0.4152s\n",
            "Epoch: 0014 loss_train: 1.2110 acc_train: 0.5455 loss_val: 0.7752 acc_val: 1.0000 time: 0.4133s\n",
            "Epoch: 0015 loss_train: 1.0377 acc_train: 0.8182 loss_val: 0.7231 acc_val: 1.0000 time: 0.4055s\n",
            "Epoch: 0016 loss_train: 1.0402 acc_train: 0.8182 loss_val: 0.6731 acc_val: 1.0000 time: 0.4153s\n",
            "Epoch: 0017 loss_train: 0.7570 acc_train: 0.8182 loss_val: 0.6262 acc_val: 1.0000 time: 0.3869s\n",
            "Epoch: 0018 loss_train: 1.0080 acc_train: 0.8182 loss_val: 0.5820 acc_val: 1.0000 time: 0.3439s\n",
            "Epoch: 0019 loss_train: 0.6626 acc_train: 0.9091 loss_val: 0.5408 acc_val: 1.0000 time: 0.4005s\n",
            "Epoch: 0020 loss_train: 0.6450 acc_train: 1.0000 loss_val: 0.5017 acc_val: 1.0000 time: 0.3963s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 7.9510s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 1.4866 accuracy= 0.4091\n",
            "---> run_8_bbc_5w1h_graph_hin.nx f1_macro 0.38206766917293233 acc 0.4090909090909091\n",
            "Networkfile run_1_40er_5w1h_graph_hin.nx\n",
            "(249, 249) (249, 249) 8 32\n",
            "Epoch: 0001 loss_train: 1.1675 acc_train: 0.5000 loss_val: 0.9526 acc_val: 0.6250 time: 0.1661s\n",
            "Epoch: 0002 loss_train: 1.6591 acc_train: 0.2500 loss_val: 0.9039 acc_val: 0.6250 time: 0.1675s\n",
            "Epoch: 0003 loss_train: 0.9374 acc_train: 0.6250 loss_val: 0.8460 acc_val: 0.7500 time: 0.1586s\n",
            "Epoch: 0004 loss_train: 0.9307 acc_train: 0.6250 loss_val: 0.7861 acc_val: 0.7500 time: 0.1490s\n",
            "Epoch: 0005 loss_train: 0.8000 acc_train: 0.7500 loss_val: 0.7279 acc_val: 0.7500 time: 0.1664s\n",
            "Epoch: 0006 loss_train: 0.9562 acc_train: 0.5000 loss_val: 0.6741 acc_val: 0.8750 time: 0.1599s\n",
            "Epoch: 0007 loss_train: 1.1755 acc_train: 0.3750 loss_val: 0.6222 acc_val: 0.8750 time: 0.1784s\n",
            "Epoch: 0008 loss_train: 0.7652 acc_train: 0.8750 loss_val: 0.5732 acc_val: 0.8750 time: 0.1746s\n",
            "Epoch: 0009 loss_train: 0.8209 acc_train: 0.7500 loss_val: 0.5273 acc_val: 1.0000 time: 0.1716s\n",
            "Epoch: 0010 loss_train: 0.7828 acc_train: 0.6250 loss_val: 0.4848 acc_val: 1.0000 time: 0.1710s\n",
            "Epoch: 0011 loss_train: 0.6192 acc_train: 0.7500 loss_val: 0.4455 acc_val: 1.0000 time: 0.1190s\n",
            "Epoch: 0012 loss_train: 0.6661 acc_train: 0.8750 loss_val: 0.4083 acc_val: 1.0000 time: 0.1629s\n",
            "Epoch: 0013 loss_train: 0.7329 acc_train: 0.7500 loss_val: 0.3740 acc_val: 1.0000 time: 0.1777s\n",
            "Epoch: 0014 loss_train: 0.6542 acc_train: 0.7500 loss_val: 0.3440 acc_val: 1.0000 time: 0.1632s\n",
            "Epoch: 0015 loss_train: 0.3498 acc_train: 1.0000 loss_val: 0.3156 acc_val: 1.0000 time: 0.1710s\n",
            "Epoch: 0016 loss_train: 0.5004 acc_train: 0.8750 loss_val: 0.2898 acc_val: 1.0000 time: 0.1687s\n",
            "Epoch: 0017 loss_train: 0.6389 acc_train: 0.8750 loss_val: 0.2672 acc_val: 1.0000 time: 0.1750s\n",
            "Epoch: 0018 loss_train: 0.4921 acc_train: 0.8750 loss_val: 0.2459 acc_val: 1.0000 time: 0.1622s\n",
            "Epoch: 0019 loss_train: 0.3761 acc_train: 1.0000 loss_val: 0.2264 acc_val: 1.0000 time: 0.1686s\n",
            "Epoch: 0020 loss_train: 0.4254 acc_train: 1.0000 loss_val: 0.2084 acc_val: 1.0000 time: 0.1676s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.3935s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 0.6762 accuracy= 0.8438\n",
            "---> run_1_40er_5w1h_graph_hin.nx f1_macro 0.6216216216216216 acc 0.84375\n",
            "Networkfile run_3_40er_5w1h_graph_hin.nx\n",
            "(249, 249) (249, 249) 8 32\n",
            "Epoch: 0001 loss_train: 1.1483 acc_train: 0.3750 loss_val: 1.0000 acc_val: 0.5000 time: 0.1749s\n",
            "Epoch: 0002 loss_train: 0.9418 acc_train: 0.6250 loss_val: 0.9233 acc_val: 0.6250 time: 0.1558s\n",
            "Epoch: 0003 loss_train: 1.0966 acc_train: 0.2500 loss_val: 0.8440 acc_val: 0.7500 time: 0.1850s\n",
            "Epoch: 0004 loss_train: 1.3892 acc_train: 0.6250 loss_val: 0.7838 acc_val: 0.8750 time: 0.1617s\n",
            "Epoch: 0005 loss_train: 0.7587 acc_train: 0.7500 loss_val: 0.7254 acc_val: 0.8750 time: 0.1828s\n",
            "Epoch: 0006 loss_train: 0.8037 acc_train: 0.6250 loss_val: 0.6706 acc_val: 0.8750 time: 0.1521s\n",
            "Epoch: 0007 loss_train: 0.7431 acc_train: 0.7500 loss_val: 0.6222 acc_val: 0.8750 time: 0.1661s\n",
            "Epoch: 0008 loss_train: 0.7527 acc_train: 0.6250 loss_val: 0.5799 acc_val: 0.8750 time: 0.1720s\n",
            "Epoch: 0009 loss_train: 0.6105 acc_train: 0.8750 loss_val: 0.5407 acc_val: 0.8750 time: 0.1692s\n",
            "Epoch: 0010 loss_train: 0.7090 acc_train: 0.8750 loss_val: 0.5042 acc_val: 0.8750 time: 0.1664s\n",
            "Epoch: 0011 loss_train: 0.5478 acc_train: 0.8750 loss_val: 0.4707 acc_val: 0.8750 time: 0.1686s\n",
            "Epoch: 0012 loss_train: 0.7933 acc_train: 0.7500 loss_val: 0.4400 acc_val: 0.8750 time: 0.1387s\n",
            "Epoch: 0013 loss_train: 0.4195 acc_train: 1.0000 loss_val: 0.4108 acc_val: 0.8750 time: 0.1555s\n",
            "Epoch: 0014 loss_train: 0.4647 acc_train: 0.8750 loss_val: 0.3837 acc_val: 0.8750 time: 0.1637s\n",
            "Epoch: 0015 loss_train: 0.3315 acc_train: 1.0000 loss_val: 0.3588 acc_val: 0.8750 time: 0.1800s\n",
            "Epoch: 0016 loss_train: 0.5331 acc_train: 0.7500 loss_val: 0.3334 acc_val: 0.8750 time: 0.1714s\n",
            "Epoch: 0017 loss_train: 0.4778 acc_train: 1.0000 loss_val: 0.3094 acc_val: 0.8750 time: 0.1760s\n",
            "Epoch: 0018 loss_train: 0.3958 acc_train: 1.0000 loss_val: 0.2868 acc_val: 0.8750 time: 0.1567s\n",
            "Epoch: 0019 loss_train: 0.4516 acc_train: 0.7500 loss_val: 0.2642 acc_val: 0.8750 time: 0.1575s\n",
            "Epoch: 0020 loss_train: 0.3562 acc_train: 1.0000 loss_val: 0.2418 acc_val: 0.8750 time: 0.1688s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.4217s\n",
            "Loading 19th epoch\n",
            "Test set results: loss= 0.6074 accuracy= 0.7500\n",
            "---> run_3_40er_5w1h_graph_hin.nx f1_macro 0.5473684210526316 acc 0.75\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_DmibZSSI-96",
        "outputId": "2fcc2892-3ff8-44e7-802a-33955e07bcc1"
      },
      "source": [
        "df_results = pd.DataFrame(experimental_results)\r\n",
        "df_results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>run_1_google_news_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.442857</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>[1, 0, 4, 2, 2, 5, 0, 2, 5, 6, 5, 4, 2, 1, 3, ...</td>\n",
              "      <td>[4, 2, 4, 1, 2, 5, 3, 1, 1, 2, 1, 4, 2, 4, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>run_6_40er_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.715666</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.843750</td>\n",
              "      <td>[0, 1, 1, 0, 0, 2, 1, 0, 1, 0, 1, 0, 1, 1, 2, ...</td>\n",
              "      <td>[0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>run_4_bbc_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.397116</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.409091</td>\n",
              "      <td>[1, 2, 3, 1, 2, 0, 3, 3, 0, 4, 3, 0, 3, 2, 2, ...</td>\n",
              "      <td>[1, 2, 0, 0, 0, 0, 3, 0, 0, 2, 3, 0, 0, 4, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>run_8_gold_standard_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.482059</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>[0, 1, 3, 4, 5, 0, 1, 6, 3, 0, 5, 4, 6, 1, 8, ...</td>\n",
              "      <td>[0, 1, 1, 8, 9, 0, 1, 6, 3, 0, 5, 8, 6, 1, 8, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>run_5_bbc_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.525696</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.522727</td>\n",
              "      <td>[1, 2, 3, 1, 2, 0, 3, 0, 4, 3, 0, 2, 2, 4, 0, ...</td>\n",
              "      <td>[1, 0, 3, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>run_9_google_news_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.505028</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>[0, 2, 0, 4, 2, 2, 5, 0, 5, 5, 6, 5, 4, 1, 2, ...</td>\n",
              "      <td>[4, 4, 2, 4, 2, 4, 5, 4, 5, 5, 2, 5, 4, 4, 4, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>run_5_gold_standard_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.489920</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>[0, 1, 2, 3, 4, 0, 1, 6, 0, 1, 5, 7, 0, 4, 6, ...</td>\n",
              "      <td>[0, 1, 0, 0, 8, 0, 1, 6, 0, 1, 5, 7, 0, 0, 6, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>run_2_bbc_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.313380</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.340909</td>\n",
              "      <td>[1, 2, 2, 3, 1, 2, 3, 0, 4, 3, 0, 3, 2, 2, 4, ...</td>\n",
              "      <td>[0, 0, 2, 2, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>run_7_40er_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.596618</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>[0, 0, 1, 0, 0, 1, 2, 0, 1, 0, 1, 1, 0, 1, 1, ...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>run_9_gold_standard_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.546596</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>[0, 0, 2, 3, 4, 5, 1, 0, 1, 6, 3, 0, 1, 5, 7, ...</td>\n",
              "      <td>[0, 0, 0, 0, 8, 1, 1, 0, 1, 6, 3, 0, 1, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>run_8_google_news_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.497944</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>[0, 1, 2, 0, 2, 0, 2, 5, 5, 6, 5, 4, 2, 1, 3, ...</td>\n",
              "      <td>[1, 1, 2, 2, 2, 3, 1, 6, 5, 6, 5, 4, 2, 2, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>run_10_bbc_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.440141</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>[0, 1, 2, 2, 3, 1, 0, 3, 3, 0, 4, 3, 2, 2, 4, ...</td>\n",
              "      <td>[0, 1, 0, 2, 0, 0, 0, 3, 0, 0, 2, 0, 2, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>run_8_40er_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.621622</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.843750</td>\n",
              "      <td>[0, 0, 1, 1, 0, 2, 1, 0, 1, 1, 0, 1, 1, 1, 2, ...</td>\n",
              "      <td>[0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>run_6_bbc_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.394141</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.409091</td>\n",
              "      <td>[0, 1, 2, 3, 1, 2, 3, 0, 4, 3, 0, 3, 2, 2, 4, ...</td>\n",
              "      <td>[0, 0, 0, 3, 0, 0, 4, 0, 2, 0, 0, 3, 2, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>run_4_google_news_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.344929</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.360000</td>\n",
              "      <td>[0, 1, 2, 3, 4, 2, 2, 5, 0, 2, 5, 5, 6, 5, 4, ...</td>\n",
              "      <td>[1, 4, 1, 3, 4, 3, 4, 4, 3, 2, 5, 1, 3, 3, 4, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>run_2_google_news_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.507143</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>[1, 2, 0, 2, 2, 5, 0, 2, 5, 5, 5, 4, 2, 1, 3, ...</td>\n",
              "      <td>[6, 1, 2, 2, 2, 5, 3, 1, 6, 5, 5, 4, 2, 1, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>run_7_gold_standard_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.459840</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>[0, 0, 1, 2, 3, 4, 5, 0, 1, 6, 0, 1, 7, 4, 6, ...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 8, 9, 0, 1, 6, 0, 1, 0, 8, 6, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>run_4_gold_standard_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.431455</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.526316</td>\n",
              "      <td>[0, 1, 2, 4, 5, 1, 3, 0, 1, 5, 7, 0, 4, 1, 0, ...</td>\n",
              "      <td>[0, 1, 0, 8, 0, 1, 3, 0, 1, 7, 1, 0, 0, 1, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>run_5_40er_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.593651</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>[0, 0, 1, 0, 0, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, ...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>run_3_gold_standard_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.420534</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.578947</td>\n",
              "      <td>[0, 1, 3, 4, 5, 1, 0, 1, 0, 1, 5, 7, 0, 4, 6, ...</td>\n",
              "      <td>[0, 1, 0, 8, 9, 1, 0, 1, 0, 1, 0, 0, 0, 0, 6, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>run_4_40er_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.489262</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>[1, 1, 0, 0, 1, 2, 1, 0, 1, 1, 0, 1, 1, 1, 0, ...</td>\n",
              "      <td>[1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>run_5_google_news_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.348794</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>[0, 2, 0, 3, 4, 2, 2, 0, 2, 5, 5, 6, 5, 1, 1, ...</td>\n",
              "      <td>[1, 2, 3, 3, 4, 3, 2, 3, 0, 3, 5, 2, 5, 2, 6, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>run_10_40er_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.497318</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>[0, 0, 1, 1, 0, 1, 2, 0, 1, 0, 1, 0, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>run_9_40er_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.591270</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>[0, 1, 0, 0, 1, 2, 1, 0, 1, 0, 1, 1, 0, 1, 1, ...</td>\n",
              "      <td>[0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>run_10_google_news_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.527365</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>[2, 0, 3, 4, 2, 2, 5, 0, 2, 5, 5, 6, 5, 4, 1, ...</td>\n",
              "      <td>[2, 2, 3, 4, 3, 2, 5, 3, 2, 3, 5, 6, 3, 4, 2, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>run_6_google_news_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.430769</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>[0, 2, 0, 3, 2, 2, 5, 0, 5, 5, 6, 5, 4, 1, 2, ...</td>\n",
              "      <td>[5, 3, 2, 3, 3, 5, 5, 3, 3, 5, 6, 3, 4, 0, 2, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>run_7_bbc_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.385952</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.409091</td>\n",
              "      <td>[2, 2, 1, 2, 0, 3, 3, 0, 4, 3, 3, 2, 4, 4, 0, ...</td>\n",
              "      <td>[0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 3, 3, 4, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>run_1_gold_standard_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.579622</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>[0, 1, 2, 3, 4, 5, 1, 0, 1, 6, 3, 5, 7, 0, 4, ...</td>\n",
              "      <td>[0, 1, 0, 7, 8, 1, 1, 0, 1, 6, 3, 5, 7, 0, 8, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>run_7_google_news_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.433333</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>[0, 1, 2, 0, 4, 2, 5, 2, 5, 5, 5, 2, 1, 3, 2, ...</td>\n",
              "      <td>[1, 4, 1, 5, 4, 5, 5, 1, 6, 5, 5, 5, 4, 3, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>run_10_gold_standard_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.442761</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.605263</td>\n",
              "      <td>[0, 1, 2, 3, 4, 5, 1, 0, 1, 6, 3, 0, 1, 7, 0, ...</td>\n",
              "      <td>[0, 1, 0, 7, 8, 1, 1, 0, 1, 6, 3, 0, 1, 7, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>run_6_gold_standard_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.466484</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>[0, 2, 3, 4, 5, 1, 0, 1, 6, 3, 0, 1, 7, 4, 1, ...</td>\n",
              "      <td>[0, 0, 3, 8, 0, 1, 0, 1, 6, 0, 0, 1, 6, 8, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>run_3_bbc_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.466984</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.477273</td>\n",
              "      <td>[1, 2, 2, 3, 1, 2, 0, 3, 0, 3, 0, 3, 2, 4, 4, ...</td>\n",
              "      <td>[0, 0, 2, 3, 0, 0, 0, 3, 0, 3, 0, 3, 2, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>run_1_bbc_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.468759</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>[0, 1, 2, 2, 3, 1, 0, 3, 3, 0, 4, 3, 0, 3, 2, ...</td>\n",
              "      <td>[0, 1, 0, 2, 0, 0, 0, 3, 0, 0, 2, 3, 1, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>run_2_gold_standard_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.482736</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.592105</td>\n",
              "      <td>[0, 0, 2, 3, 4, 5, 1, 0, 1, 3, 1, 5, 7, 0, 4, ...</td>\n",
              "      <td>[0, 0, 0, 1, 8, 1, 1, 0, 1, 3, 1, 0, 0, 0, 8, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>run_3_google_news_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.381537</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>[0, 2, 0, 3, 2, 5, 0, 2, 5, 6, 5, 4, 1, 2, 1, ...</td>\n",
              "      <td>[1, 2, 2, 3, 2, 2, 3, 1, 6, 6, 2, 4, 2, 2, 6, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>run_9_bbc_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.499808</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>[1, 2, 2, 3, 1, 2, 0, 3, 0, 4, 3, 0, 3, 2, 2, ...</td>\n",
              "      <td>[0, 0, 2, 3, 0, 0, 0, 3, 0, 2, 0, 0, 3, 2, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>run_2_40er_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.495614</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>[0, 1, 1, 0, 0, 2, 1, 1, 0, 1, 0, 1, 1, 1, 2, ...</td>\n",
              "      <td>[0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>run_8_bbc_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.382068</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.409091</td>\n",
              "      <td>[0, 1, 2, 2, 3, 1, 2, 0, 3, 0, 4, 3, 0, 2, 4, ...</td>\n",
              "      <td>[0, 0, 0, 2, 3, 0, 0, 0, 2, 0, 2, 0, 0, 0, 4, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>run_1_40er_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.621622</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.843750</td>\n",
              "      <td>[0, 0, 1, 0, 0, 2, 1, 1, 0, 1, 1, 1, 1, 1, 2, ...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>run_3_40er_5w1h_graph_hin.nx</td>\n",
              "      <td>f1_macro</td>\n",
              "      <td>0.547368</td>\n",
              "      <td>acc</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>[0, 1, 1, 0, 2, 1, 0, 1, 0, 1, 1, 1, 1, 2, 1, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         0  ...                                                  6\n",
              "0      run_1_google_news_5w1h_graph_hin.nx  ...  [4, 2, 4, 1, 2, 5, 3, 1, 1, 2, 1, 4, 2, 4, 3, ...\n",
              "1             run_6_40er_5w1h_graph_hin.nx  ...  [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, ...\n",
              "2              run_4_bbc_5w1h_graph_hin.nx  ...  [1, 2, 0, 0, 0, 0, 3, 0, 0, 2, 3, 0, 0, 4, 0, ...\n",
              "3    run_8_gold_standard_5w1h_graph_hin.nx  ...  [0, 1, 1, 8, 9, 0, 1, 6, 3, 0, 5, 8, 6, 1, 8, ...\n",
              "4              run_5_bbc_5w1h_graph_hin.nx  ...  [1, 0, 3, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 0, 0, ...\n",
              "5      run_9_google_news_5w1h_graph_hin.nx  ...  [4, 4, 2, 4, 2, 4, 5, 4, 5, 5, 2, 5, 4, 4, 4, ...\n",
              "6    run_5_gold_standard_5w1h_graph_hin.nx  ...  [0, 1, 0, 0, 8, 0, 1, 6, 0, 1, 5, 7, 0, 0, 6, ...\n",
              "7              run_2_bbc_5w1h_graph_hin.nx  ...  [0, 0, 2, 2, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, ...\n",
              "8             run_7_40er_5w1h_graph_hin.nx  ...  [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, ...\n",
              "9    run_9_gold_standard_5w1h_graph_hin.nx  ...  [0, 0, 0, 0, 8, 1, 1, 0, 1, 6, 3, 0, 1, 0, 0, ...\n",
              "10     run_8_google_news_5w1h_graph_hin.nx  ...  [1, 1, 2, 2, 2, 3, 1, 6, 5, 6, 5, 4, 2, 2, 3, ...\n",
              "11            run_10_bbc_5w1h_graph_hin.nx  ...  [0, 1, 0, 2, 0, 0, 0, 3, 0, 0, 2, 0, 2, 0, 0, ...\n",
              "12            run_8_40er_5w1h_graph_hin.nx  ...  [0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, ...\n",
              "13             run_6_bbc_5w1h_graph_hin.nx  ...  [0, 0, 0, 3, 0, 0, 4, 0, 2, 0, 0, 3, 2, 0, 0, ...\n",
              "14     run_4_google_news_5w1h_graph_hin.nx  ...  [1, 4, 1, 3, 4, 3, 4, 4, 3, 2, 5, 1, 3, 3, 4, ...\n",
              "15     run_2_google_news_5w1h_graph_hin.nx  ...  [6, 1, 2, 2, 2, 5, 3, 1, 6, 5, 5, 4, 2, 1, 3, ...\n",
              "16   run_7_gold_standard_5w1h_graph_hin.nx  ...  [0, 0, 1, 0, 0, 8, 9, 0, 1, 6, 0, 1, 0, 8, 6, ...\n",
              "17   run_4_gold_standard_5w1h_graph_hin.nx  ...  [0, 1, 0, 8, 0, 1, 3, 0, 1, 7, 1, 0, 0, 1, 0, ...\n",
              "18            run_5_40er_5w1h_graph_hin.nx  ...  [0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, ...\n",
              "19   run_3_gold_standard_5w1h_graph_hin.nx  ...  [0, 1, 0, 8, 9, 1, 0, 1, 0, 1, 0, 0, 0, 0, 6, ...\n",
              "20            run_4_40er_5w1h_graph_hin.nx  ...  [1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, ...\n",
              "21     run_5_google_news_5w1h_graph_hin.nx  ...  [1, 2, 3, 3, 4, 3, 2, 3, 0, 3, 5, 2, 5, 2, 6, ...\n",
              "22           run_10_40er_5w1h_graph_hin.nx  ...  [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, ...\n",
              "23            run_9_40er_5w1h_graph_hin.nx  ...  [0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, ...\n",
              "24    run_10_google_news_5w1h_graph_hin.nx  ...  [2, 2, 3, 4, 3, 2, 5, 3, 2, 3, 5, 6, 3, 4, 2, ...\n",
              "25     run_6_google_news_5w1h_graph_hin.nx  ...  [5, 3, 2, 3, 3, 5, 5, 3, 3, 5, 6, 3, 4, 0, 2, ...\n",
              "26             run_7_bbc_5w1h_graph_hin.nx  ...  [0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 3, 3, 4, 0, 0, ...\n",
              "27   run_1_gold_standard_5w1h_graph_hin.nx  ...  [0, 1, 0, 7, 8, 1, 1, 0, 1, 6, 3, 5, 7, 0, 8, ...\n",
              "28     run_7_google_news_5w1h_graph_hin.nx  ...  [1, 4, 1, 5, 4, 5, 5, 1, 6, 5, 5, 5, 4, 3, 1, ...\n",
              "29  run_10_gold_standard_5w1h_graph_hin.nx  ...  [0, 1, 0, 7, 8, 1, 1, 0, 1, 6, 3, 0, 1, 7, 0, ...\n",
              "30   run_6_gold_standard_5w1h_graph_hin.nx  ...  [0, 0, 3, 8, 0, 1, 0, 1, 6, 0, 0, 1, 6, 8, 1, ...\n",
              "31             run_3_bbc_5w1h_graph_hin.nx  ...  [0, 0, 2, 3, 0, 0, 0, 3, 0, 3, 0, 3, 2, 0, 0, ...\n",
              "32             run_1_bbc_5w1h_graph_hin.nx  ...  [0, 1, 0, 2, 0, 0, 0, 3, 0, 0, 2, 3, 1, 0, 0, ...\n",
              "33   run_2_gold_standard_5w1h_graph_hin.nx  ...  [0, 0, 0, 1, 8, 1, 1, 0, 1, 3, 1, 0, 0, 0, 8, ...\n",
              "34     run_3_google_news_5w1h_graph_hin.nx  ...  [1, 2, 2, 3, 2, 2, 3, 1, 6, 6, 2, 4, 2, 2, 6, ...\n",
              "35             run_9_bbc_5w1h_graph_hin.nx  ...  [0, 0, 2, 3, 0, 0, 0, 3, 0, 2, 0, 0, 3, 2, 0, ...\n",
              "36            run_2_40er_5w1h_graph_hin.nx  ...  [0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, ...\n",
              "37             run_8_bbc_5w1h_graph_hin.nx  ...  [0, 0, 0, 2, 3, 0, 0, 0, 2, 0, 2, 0, 0, 0, 4, ...\n",
              "38            run_1_40er_5w1h_graph_hin.nx  ...  [0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, ...\n",
              "39            run_3_40er_5w1h_graph_hin.nx  ...  [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, ...\n",
              "\n",
              "[40 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHKLJPd6Mdgx"
      },
      "source": [
        "df_results.to_excel('gat_noreg_results_r1.xls')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qwJ7oQMnawM"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}