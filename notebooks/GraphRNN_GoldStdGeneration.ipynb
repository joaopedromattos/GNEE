{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GraphRNN-GoldStdGeneration.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bI6OWMtvJVJ"
      },
      "source": [
        "# GraphRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdjFTZRRvSQL",
        "outputId": "3b1594ed-2b49-4210-f987-6bd14a58728d"
      },
      "source": [
        "!git clone --single-branch --branch colab https://github.com/joaopedromattos/GraphRNN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GraphRNN'...\n",
            "remote: Enumerating objects: 99, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 99 (delta 12), reused 19 (delta 8), pack-reused 74\u001b[K\n",
            "Unpacking objects: 100% (99/99), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtZJ-j-avcqB",
        "outputId": "18987363-40c5-4bbc-c8df-fd5dc487d485"
      },
      "source": [
        "!pip install gdown\n",
        "!gdown --id 1RF_bIo5ndxPhu9SJw-T8HBcuHyaGQGL0 && tar -xzvf datasets.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RF_bIo5ndxPhu9SJw-T8HBcuHyaGQGL0\n",
            "To: /content/datasets.tar.gz\n",
            "22.7MB [00:00, 62.4MB/s]\n",
            "datasets_runs/\n",
            "datasets_runs/run_1_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_6_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_4_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_8_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_5_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_9_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_5_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_2_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_9_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_7_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_9_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_8_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_10_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_8_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_2_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_8_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_6_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_4_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_2_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_7_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_4_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_5_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_3_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_4_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_5_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_10_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_10_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_9_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_10_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_6_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_1_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_3_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_5_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_4_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_7_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_1_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_7_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_10_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_6_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_3_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_1_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_2_gold_standard_5w1h_graph_hin.nx\n",
            "datasets_runs/run_3_google_news_5w1h_graph_hin.nx\n",
            "datasets_runs/run_9_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_2_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_8_bbc_5w1h_graph_hin.nx\n",
            "datasets_runs/run_6_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_1_40er_5w1h_graph_hin.nx\n",
            "datasets_runs/run_7_news_cluster_5w1h_graph_hin.nx\n",
            "datasets_runs/run_3_40er_5w1h_graph_hin.nx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJkKQn3dvSkl"
      },
      "source": [
        "!mv GraphRNN/* ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u46kWkWzPgS"
      },
      "source": [
        "!mkdir ./dataset/EVENT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMUUapg1vXKn"
      },
      "source": [
        "## Preparing our graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFAK7Hn2vkdK",
        "outputId": "c19d19ea-a143-4ecb-f332-5bf1b3a217ed"
      },
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "G = nx.read_gpickle('./datasets_runs/run_1_gold_standard_5w1h_graph_hin.nx') # selecting the graph\n",
        "len(G.nodes), len(G.edges)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(579, 803)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vszfSPvf1-t3"
      },
      "source": [
        "### Indicator file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sur4ZcIj2Cl0"
      },
      "source": [
        "# GraphRNN receives a file with a number in every ith line, \n",
        "# that represents the graph to which the ith node belongs to.\n",
        "# E.g.: line 85824 with a value 222 means that the node 85824 belongs to \n",
        "# the graph number 222.\n",
        "node_mapper = {i : v for i, v in enumerate(G.nodes)}\n",
        "node_mapper.keys()\n",
        "np.savetxt(\"EVENT_graph_indicator.txt\", np.ones(shape=len(G.nodes)), fmt='%d')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3WrL0xTv0xw"
      },
      "source": [
        "### Adj Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzEmGsiVv2X8"
      },
      "source": [
        "# GraphRNN receives an edgelist to mount an adjacency matrix\n",
        "# inside data.py file on Graph_load_batch method.\n",
        "G_relabel = nx.relabel_nodes(G, {v : k for k, v in node_mapper.items()})\n",
        "nx.write_edgelist(G_relabel, \"EVENT_A.txt\", data=False, delimiter=', ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrGjgsE-xz4r"
      },
      "source": [
        "### Node Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox5edKeKxx_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a0c75d-bfef-49e0-a148-8f23cf2273c2"
      },
      "source": [
        "# Here we get a list of all labels of all nodes.\n",
        "# In case of non-labeled nodes, we manually label them with \"no_label\"\n",
        "labels = [G.nodes[v]['label'] if 'label' in G.nodes[v] else 'no_label' for i, v in enumerate(G.nodes)]\n",
        "\n",
        "\n",
        "# We'll give a unique natural number to each label of our graph.\n",
        "label_mapper = dict()\n",
        "count = 0\n",
        "for i, v in enumerate(labels):\n",
        "    if (not (v in label_mapper) ):\n",
        "        label_mapper[v] = count\n",
        "        count += 1\n",
        "print(label_mapper)\n",
        "\n",
        "# Mapping our labels to natural numbers and writing them to a file.\n",
        "node_labels_list = list(map(lambda x: label_mapper[x], labels))\n",
        "print(node_labels_list)\n",
        "np.savetxt(\"EVENT_node_labels.txt\", node_labels_list, fmt='%d')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'toberone-gate': 0, 'no_label': 1, 'clinton blames comey': 2, 'china well': 3, 'cubs win championship': 4, 'seattle shooting': 5, 'gaga protests': 6, 'f1': 7, 'clinton clear': 8, 'consulate attack': 9, 'trump and obama': 10, 'tram': 11, 'tram victims': 12, nan: 13}\n",
            "[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 9, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 10, 1, 1, 1, 1, 1, 10, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 11, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 11, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 9, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 10, 1, 1, 1, 1, 1, 1, 1, 12, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 2, 1, 1, 12, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 11, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 7, 1, 1, 1, 9, 1, 1, 1, 1, 9, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 10, 1, 1, 1, 1, 10, 1, 1, 1, 1, 1, 1, 11, 1, 1, 1, 1, 1, 1, 11, 1, 1, 1, 1, 1, 2, 1, 1, 1, 7, 1, 1, 11, 1, 1, 1, 3, 1, 1, 1, 1, 10, 1, 1, 1, 1, 1, 1, 9, 1, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 8, 1, 1, 2, 1, 1, 1, 11, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 10, 1, 1, 11, 1, 1, 1, 1, 1, 1, 9, 1, 1, 1, 8, 1, 1, 12, 1, 1, 7, 1, 1, 1, 1, 1, 2, 1, 1, 1, 7, 1, 1, 1, 10, 1, 1, 1, 1, 8, 1, 1, 11, 1, 1, 1, 1, 1, 11, 1, 1, 1, 1, 12, 1, 1, 1, 1, 1, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 10, 1, 1, 1, 8, 1, 1, 1, 1, 0, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC0WNn7W52bX"
      },
      "source": [
        "!mv EVENT_* dataset/EVENT/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOSOc6xzyzKo"
      },
      "source": [
        "## Running GraphRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ1aMwBUy1ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a63d5b54-b396-430f-f062-7e8e498df054"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboard-logger\n",
            "  Downloading https://files.pythonhosted.org/packages/87/7a/ec0fd26dba69191f82eb8f38f5b401c124f45a207490a7ade6ea9717ecdb/tensorboard_logger-0.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.4.1)\n",
            "Collecting networkx==1.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2c/e473e54afc9fae58dfa97066ef6709a7e35a1dd1c28c5a3842989322be00/networkx-1.11-py2.py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyemd in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.5.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard-logger->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard-logger->-r requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboard-logger->-r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboard-logger->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from tensorboard-logger->-r requirements.txt (line 1)) (3.12.4)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.36.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.32.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (3.7.4.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.3.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (2.4.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (2.10.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from networkx==1.11->-r requirements.txt (line 3)) (4.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->tensorboard-logger->-r requirements.txt (line 1)) (56.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (1.28.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (3.10.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->-r requirements.txt (line 2)) (0.4.8)\n",
            "\u001b[31mERROR: scikit-image 0.16.2 has requirement networkx>=2.0, but you'll have networkx 1.11 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard-logger, networkx\n",
            "  Found existing installation: networkx 2.5.1\n",
            "    Uninstalling networkx-2.5.1:\n",
            "      Successfully uninstalled networkx-2.5.1\n",
            "Successfully installed networkx-1.11 tensorboard-logger-0.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "networkx"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhkD5HJcy3jK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "006fcb8b-e6df-401c-b64f-1c9e8c1aeedd"
      },
      "source": [
        "!python main.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-08 15:06:51.369565: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "1.8.1+cu101\n",
            "Using CUDA: True\n",
            "CUDA available: True\n",
            "_CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15109MB, multi_processor_count=40)\n",
            "CUDA 1\n",
            "File name prefix GraphRNN_RNN_EVENT_4_128_\n",
            "Loading graph dataset: EVENT\n",
            "Loaded 1\n",
            "[<networkx.classes.graph.Graph object at 0x7fade5894950>] [] [] [<networkx.classes.graph.Graph object at 0x7fade5894950>]\n",
            "graph_validate_len 0.0\n",
            "graph_test_len 578.0\n",
            "total graph num: 1, training set: 1\n",
            "max number node: 578\n",
            "max/min number edge: 794; 794\n",
            "max previous node: 230\n",
            "train and test graphs saved at:  ./graphs/GraphRNN_RNN_EVENT_4_128_test_0.dat\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/content/model.py:299: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  nn.init.xavier_uniform(param,gain=nn.init.calculate_gain('sigmoid'))\n",
            "/content/model.py:297: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  nn.init.constant(param, 0.25)\n",
            "/content/model.py:302: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  m.weight.data = init.xavier_uniform(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "Epoch: 1/10, train loss: 0.115169, graph type: EVENT, num_layer: 4, hidden: 128\n",
            "Epoch: 2/10, train loss: 0.020691, graph type: EVENT, num_layer: 4, hidden: 128\n",
            "Epoch: 3/10, train loss: 0.017589, graph type: EVENT, num_layer: 4, hidden: 128\n",
            "Epoch: 4/10, train loss: 0.016219, graph type: EVENT, num_layer: 4, hidden: 128\n",
            "Epoch: 5/10, train loss: 0.015187, graph type: EVENT, num_layer: 4, hidden: 128\n",
            "Epoch: 6/10, train loss: 0.015666, graph type: EVENT, num_layer: 4, hidden: 128\n",
            "Epoch: 7/10, train loss: 0.017421, graph type: EVENT, num_layer: 4, hidden: 128\n",
            "Epoch: 8/10, train loss: 0.015043, graph type: EVENT, num_layer: 4, hidden: 128\n",
            "Epoch: 9/10, train loss: 0.015092, graph type: EVENT, num_layer: 4, hidden: 128\n",
            "Epoch: 10/10, train loss: 0.016112, graph type: EVENT, num_layer: 4, hidden: 128\n",
            "Max num node: 4000\n",
            "100% 4000/4000 [2:51:44<00:00,  2.58s/it]\n",
            "Completed - ./graphs/GraphRNN_RNN_EVENT_4_128_pred_10_1_4000_nodes.dat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHt3aAKKl_qJ"
      },
      "source": [
        "## Converting our outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-ahXJpr-hY3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a198c5b-d2bb-4ef1-a985-22dec7bfc402"
      },
      "source": [
        "import pickle\n",
        "\n",
        "G_pred_list = pickle.load( open( \"graphs/GraphRNN_RNN_EVENT_4_128_pred_10_1_4000_nodes.dat\", \"rb\" ) )\n",
        "G_pred_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<networkx.classes.graph.Graph at 0x7f604987a090>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lxqUs_B-psX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "503775a2-cce6-4fdf-fb60-f48f93f03bcd"
      },
      "source": [
        "graph_list = []\n",
        "for i in G_pred_list:\n",
        "\n",
        "    cur_graph_edges = [(j, k, i.edge[j][k]['weight']) for j in i.edge.keys() for k in i.edge[j]]\n",
        "    test_graph = nx.DiGraph()\n",
        "    test_graph.add_nodes_from(i.node)\n",
        "    test_graph.add_weighted_edges_from(cur_graph_edges)\n",
        "    graph_list.append(test_graph)\n",
        "\n",
        "    print(\"Nodes, edges:\", len(i.node.keys()), len(cur_graph_edges))\n",
        "\n",
        "pickle.dump(graph_list, open('graph_list.dat', 'wb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nodes, edges: 4001 150140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyB8K-g628Jt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}