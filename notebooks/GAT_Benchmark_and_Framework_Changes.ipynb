{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAT Benchmark and Framework Changes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYPB9v42yxCk"
      },
      "source": [
        "# Graph Attention Networks Benchmark - Cora\n",
        "\n",
        "This notebook aims to verify quality of the implementation of [GAT - Graph Attention Networks](https://arxiv.org/abs/1710.10903) paper.\n",
        "\n",
        "## Objectives\n",
        "- Test the PyTorch Implementation of [GAT](https://github.com/Diego999/pyGAT);\n",
        "- Test the [Original Implementation of GAT](https://github.com/PetarV-/GAT), from one of the authors, Petar Veličković.\n",
        "- Improve the usability of these implementations in order to provide a modular and simple code, favouring the adoption and replication of it in other experiments.\n",
        "\n",
        "_Note: Both repositories have been forked to my personal Github profile, in order to mantain a version control over the changes done to each code_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMoLZYc42fOt"
      },
      "source": [
        "## [PyTorch Implementation](https://github.com/Diego999/pyGAT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Daky1s-Kypr0",
        "outputId": "6fae440b-753e-46fc-ecd2-a95e5e5db0c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/joaopedromattos/pyGAT\n",
        "!pip install --quiet spektral"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pyGAT'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Total 163 (delta 0), reused 0 (delta 0), pack-reused 163\u001b[K\n",
            "Receiving objects: 100% (163/163), 216.21 KiB | 3.54 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n",
            "\u001b[K     |████████████████████████████████| 102kB 3.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUpxUjXxDHeR"
      },
      "source": [
        "Running our model with the default dataset (Cora)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NomTatw13XD8",
        "outputId": "41012b05-4af0-42fe-cd2b-81726795c27a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python3 pyGAT/train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-28 20:29:14.427138: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "[LOAD DATA]: cora\n",
            "tensor([0, 6, 3,  ..., 2, 5, 0])\n",
            "Epoch: 0001 loss_train: 2.3115 acc_train: 0.1700 loss_val: 1.8774 acc_val: 0.3192 time: 18.9700s\n",
            "Epoch: 0002 loss_train: 2.0392 acc_train: 0.2420 loss_val: 1.7392 acc_val: 0.4041 time: 15.4444s\n",
            "Epoch: 0003 loss_train: 1.8679 acc_train: 0.3282 loss_val: 1.6107 acc_val: 0.4613 time: 15.3702s\n",
            "Epoch: 0004 loss_train: 1.7721 acc_train: 0.3688 loss_val: 1.4859 acc_val: 0.5240 time: 15.3919s\n",
            "Traceback (most recent call last):\n",
            "  File \"pyGAT/train.py\", line 202, in <module>\n",
            "    gat.train_pipeline()\n",
            "  File \"pyGAT/train.py\", line 134, in train_pipeline\n",
            "    loss_values.append(train(epoch))\n",
            "  File \"pyGAT/train.py\", line 104, in train\n",
            "    output = model(features, adj)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/pyGAT/models.py\", line 21, in forward\n",
            "    x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
            "  File \"/content/pyGAT/models.py\", line 21, in <listcomp>\n",
            "    x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/pyGAT/layers.py\", line 31, in forward\n",
            "    a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvzdaElT_74V"
      },
      "source": [
        "### Testing field\n",
        "\n",
        "This subsection was used to understand how the function ```original_load_data```, from file ```pyGAT/utils.py``` works. So unless you're also interested in understanding each detail from that function, you can skip this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SD9tmpSDOoA"
      },
      "source": [
        "Here we have the function we want to understand. This function is the original DataLoader from GAT, thus we'll try to reuse it to run other benchmarks. \n",
        "\n",
        "Note: ```normalize_features```, ```normalize_adj``` and ```encode_onehot``` are just auxiliary functions and its name might be just enough to understand what each one does :)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ldf2sGFLDwm"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import spektral as spk\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import os\n",
        "\n",
        "def normalize_adj(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[\n",
        "        i, :] for i, c in enumerate(classes)}\n",
        "    labels_onehot = np.array(\n",
        "        list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "def cora_networkx(path=None):\n",
        "    if (path == None):\n",
        "        raise ValueError(\"Dataset path shouldn't be of type 'None'.\")\n",
        "    else:\n",
        "        # Reading our graph, according to documentation\n",
        "        edgelist = pd.read_csv(os.path.join(\n",
        "            path, \"cora.cites\"), sep='\\t', header=None, names=[\"target\", \"source\"])\n",
        "        edgelist[\"label\"] = \"cites\"\n",
        "\n",
        "        # Transforming it into a\n",
        "        Gnx = nx.from_pandas_edgelist(edgelist, edge_attr=\"label\")\n",
        "        \n",
        "        adj = nx.to_scipy_sparse_matrix(Gnx)\n",
        "\n",
        "        # Sparse feature matrix\n",
        "        feature_names = [\"w_{}\".format(ii) for ii in range(1433)]\n",
        "        column_names = feature_names + [\"subject\"]\n",
        "        node_data = pd.read_csv(os.path.join(\n",
        "            path, \"cora.content\"), sep='\\t', header=None, names=column_names)\n",
        "        node_data.to_numpy()[:, :-1]\n",
        "        features = sp.csr_matrix(node_data.to_numpy()[\n",
        "                                 :, :-1], dtype=np.float32)\n",
        "\n",
        "        # Train / val / test spliting...\n",
        "        num_nodes = features.shape[0]\n",
        "        idxs = np.arange(0, num_nodes)\n",
        "        idx_train, idx_val, idx_test = np.split(\n",
        "            idxs, [int(.6*num_nodes), int(.8*num_nodes)])\n",
        "\n",
        "        labels = encode_onehot(node_data.to_numpy()[:, -1])\n",
        "\n",
        "        return adj, features, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "adj, features, labels, idx_train, idx_val, idx_test = cora_networkx(\"./pyGAT/data/cora/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL3WticMLPtf",
        "outputId": "cb202a57-3224-447d-b31a-a02d8497166e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "print(adj)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 1)\t1\n",
            "  (0, 99)\t1\n",
            "  (0, 324)\t1\n",
            "  (0, 330)\t1\n",
            "  (0, 1736)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 2)\t1\n",
            "  (1, 3)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 5)\t1\n",
            "  (1, 6)\t1\n",
            "  (1, 7)\t1\n",
            "  (1, 8)\t1\n",
            "  (1, 9)\t1\n",
            "  (1, 10)\t1\n",
            "  (1, 11)\t1\n",
            "  (1, 12)\t1\n",
            "  (1, 13)\t1\n",
            "  (1, 14)\t1\n",
            "  (1, 15)\t1\n",
            "  (1, 16)\t1\n",
            "  (1, 17)\t1\n",
            "  (1, 18)\t1\n",
            "  (1, 19)\t1\n",
            "  (1, 20)\t1\n",
            "  :\t:\n",
            "  (2693, 1101)\t1\n",
            "  (2694, 2695)\t1\n",
            "  (2695, 2694)\t1\n",
            "  (2696, 2697)\t1\n",
            "  (2697, 2696)\t1\n",
            "  (2698, 2365)\t1\n",
            "  (2699, 2700)\t1\n",
            "  (2700, 2699)\t1\n",
            "  (2701, 2702)\t1\n",
            "  (2702, 2701)\t1\n",
            "  (2703, 2396)\t1\n",
            "  (2704, 1493)\t1\n",
            "  (2704, 1502)\t1\n",
            "  (2704, 2705)\t1\n",
            "  (2704, 2706)\t1\n",
            "  (2705, 1502)\t1\n",
            "  (2705, 2704)\t1\n",
            "  (2705, 2706)\t1\n",
            "  (2705, 2707)\t1\n",
            "  (2706, 1493)\t1\n",
            "  (2706, 1502)\t1\n",
            "  (2706, 2704)\t1\n",
            "  (2706, 2705)\t1\n",
            "  (2707, 729)\t1\n",
            "  (2707, 2705)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9cN7ktNCj59",
        "outputId": "c39c42b8-26cb-4f39-9145-695fddcca340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def normalize_features(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "def normalize_adj(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "path=\"./pyGAT/data/cora/\"\n",
        "dataset=\"cora\"\n",
        "\"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "# Reading from NumPy file...\n",
        "idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
        "\n",
        "# \n",
        "features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "# build graph\n",
        "idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "idx_map = {j: i for i, j in enumerate(idx)}\n",
        "edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32)\n",
        "edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
        "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
        "\n",
        "# build symmetric adjacency matrix\n",
        "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "print(adj)\n",
        "features = normalize_features(features)\n",
        "\n",
        "\n",
        "adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "idx_train = range(140)\n",
        "idx_val = range(200, 500)\n",
        "idx_test = range(500, 1500)\n",
        "\n",
        "adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "features = torch.FloatTensor(np.array(features.todense()))\n",
        "labels = torch.LongTensor(np.where(labels)[1])\n",
        "\n",
        "idx_train = torch.LongTensor(idx_train)\n",
        "idx_val = torch.LongTensor(idx_val)\n",
        "idx_test = torch.LongTensor(idx_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cora dataset...\n",
            "  (0, 8)\t1.0\n",
            "  (0, 14)\t1.0\n",
            "  (0, 258)\t1.0\n",
            "  (0, 435)\t1.0\n",
            "  (0, 544)\t1.0\n",
            "  (1, 344)\t1.0\n",
            "  (2, 410)\t1.0\n",
            "  (2, 471)\t1.0\n",
            "  (2, 552)\t1.0\n",
            "  (2, 565)\t1.0\n",
            "  (3, 197)\t1.0\n",
            "  (3, 463)\t1.0\n",
            "  (3, 601)\t1.0\n",
            "  (4, 170)\t1.0\n",
            "  (5, 490)\t1.0\n",
            "  (5, 2164)\t1.0\n",
            "  (6, 251)\t1.0\n",
            "  (6, 490)\t1.0\n",
            "  (7, 258)\t1.0\n",
            "  (8, 0)\t1.0\n",
            "  (8, 14)\t1.0\n",
            "  (8, 258)\t1.0\n",
            "  (8, 435)\t1.0\n",
            "  (8, 751)\t1.0\n",
            "  (9, 308)\t1.0\n",
            "  :\t:\n",
            "  (2698, 2697)\t1.0\n",
            "  (2698, 2700)\t1.0\n",
            "  (2699, 2153)\t1.0\n",
            "  (2700, 2697)\t1.0\n",
            "  (2700, 2698)\t1.0\n",
            "  (2701, 2247)\t1.0\n",
            "  (2701, 2263)\t1.0\n",
            "  (2702, 881)\t1.0\n",
            "  (2702, 2624)\t1.0\n",
            "  (2703, 1221)\t1.0\n",
            "  (2703, 1409)\t1.0\n",
            "  (2703, 2200)\t1.0\n",
            "  (2704, 209)\t1.0\n",
            "  (2704, 2407)\t1.0\n",
            "  (2705, 1784)\t1.0\n",
            "  (2705, 1839)\t1.0\n",
            "  (2705, 1840)\t1.0\n",
            "  (2705, 2216)\t1.0\n",
            "  (2706, 1046)\t1.0\n",
            "  (2706, 1138)\t1.0\n",
            "  (2706, 1640)\t1.0\n",
            "  (2706, 1752)\t1.0\n",
            "  (2707, 774)\t1.0\n",
            "  (2707, 1389)\t1.0\n",
            "  (2707, 2344)\t1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8_nr0KMBOsL"
      },
      "source": [
        "Exhibiting each value returned by the function ```load_data()```:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8ilurbX7MZo",
        "outputId": "bfb1285a-f548-4a8f-bfc8-c9d2a7d59842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2708x1433 sparse matrix of type '<class 'numpy.float32'>'\n",
              "\twith 49216 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuSiIJE_36VZ",
        "outputId": "249426c5-b646-42ec-f065-9038c005a801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "edges_unordered.flatten()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([     35,    1033,      35, ...,  853118,  954315, 1155073],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy5zMrw04D7G",
        "outputId": "f63d39ce-0181-416d-f9e2-0dc30508badf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "adj"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1667, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.2000,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        ...,\n",
              "        [0.0000, 0.0000, 0.0000,  ..., 0.2000, 0.0000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2000, 0.0000],\n",
              "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2500]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KRMwH1n4GnC",
        "outputId": "de1d3f47-3673-44e8-ee4f-3a3f79d3f4e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3, 5, 6,  ..., 1, 0, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVW8EzBn4IHO",
        "outputId": "373dff9b-4f32-4e2b-a958-464ec6c60d41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "idx_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([140])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5bjsuYs4KBG",
        "outputId": "bde6838c-00c6-42bf-c977-66ea95e7c07a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "idx_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiFrN7624LXL",
        "outputId": "b3177f78-78b6-477a-e4e0-229422312cb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "idx_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-UeJaKg_FVz"
      },
      "source": [
        "## [Original implementation of GAT](https://github.com/PetarV-/GAT)\n",
        "\n",
        "This section aims to verify if the PyTorch implementation of GAT is equivalent in accuracy and performance to the original implementation, developed in Tensowflow. One can see, by the results, that the PyTorch implementation is equivalent to the Tensorflow version.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0n0yR_N-_y3",
        "outputId": "05b651e7-e170-4631-a7ec-757312dbe2b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/joaopedromattos/GAT"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'GAT' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqnSwWR3kjj5"
      },
      "source": [
        "This code was originally written in Tensorflow 1.6, so we'll downgrade."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ_BFcn4ki4t",
        "outputId": "1fcee4d3-6413-445a-eca3-adc117a61fd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.6.0\n",
        "!pip install tensorflow==1.6.0\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/8c/35ba6f94dd9729517b899c3ba764e604ffe22daeba04f7c771dd452ba55b/tensorflow_gpu-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (209.2MB)\n",
            "\u001b[K     |████████████████████████████████| 209.2MB 72kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.7.0,>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/67/a8c91665987d359211dcdca5c8b2a7c1e0876eb0702a4383c1e4ff76228d/tensorboard-1.6.0-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 36.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.6.0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.6.0) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.6.0) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.6.0) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.6.0) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.6.0) (0.35.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.6.0) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.6.0) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.6.0) (1.32.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow-gpu==1.6.0) (1.0.1)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow-gpu==1.6.0) (3.2.2)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 41.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-gpu==1.6.0) (50.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.7.0,>=1.6.0->tensorflow-gpu==1.6.0) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.7.0,>=1.6.0->tensorflow-gpu==1.6.0) (3.1.0)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=7916045ca943e6be96c49a8c540761942766aaa2015e3124355cf2d7869ac3a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.6.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.5\n",
            "    Uninstalling bleach-3.1.5:\n",
            "      Successfully uninstalled bleach-3.1.5\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.6.0 tensorflow-gpu-1.6.0\n",
            "Collecting tensorflow==1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/0f/fbd8bb92459c75db93040f80702ebe4ba83a52cdb6ad930654c31dc0b711/tensorflow-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (45.8MB)\n",
            "\u001b[K     |████████████████████████████████| 45.9MB 62kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (0.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.7.0,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (1.6.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (1.18.5)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (1.32.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6.0) (0.8.1)\n",
            "Requirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow==1.6.0) (0.9999999)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow==1.6.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow==1.6.0) (3.2.2)\n",
            "Requirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow==1.6.0) (1.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.6.0) (50.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.7.0,>=1.6.0->tensorflow==1.6.0) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.7.0,>=1.6.0->tensorflow==1.6.0) (3.1.0)\n",
            "\u001b[31mERROR: spektral 0.6.1 has requirement tensorflow>=2.1.0, but you'll have tensorflow 1.6.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed tensorflow-1.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUJg0by9_UAy",
        "outputId": "9ae3d3a3-8ea0-4caf-d1e7-8ebbcb1105c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "run GAT/execute_cora.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: cora\n",
            "----- Opt. hyperparams -----\n",
            "lr: 0.005\n",
            "l2_coef: 0.0005\n",
            "----- Archi. hyperparams -----\n",
            "nb. layers: 1\n",
            "nb. units per layer: [8]\n",
            "nb. attention heads: [8, 1]\n",
            "residual: False\n",
            "nonlinearity: <function elu at 0x7fbb2d4cf840>\n",
            "model: <class 'models.gat.GAT'>\n",
            "(2708, 2708)\n",
            "(2708, 1433)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`NHWC` for data_format is deprecated, use `NWC` instead\n",
            "WARNING:tensorflow:From /content/GAT/models/base_gattn.py:41: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
            "\n",
            "Training: loss = 1.94072, acc = 0.16429 | Val: loss = 1.94814, acc = 0.15200\n",
            "Training: loss = 1.95123, acc = 0.17857 | Val: loss = 1.94673, acc = 0.19000\n",
            "Training: loss = 1.94346, acc = 0.18571 | Val: loss = 1.94322, acc = 0.23600\n",
            "Training: loss = 1.93882, acc = 0.21429 | Val: loss = 1.93923, acc = 0.31800\n",
            "Training: loss = 1.93936, acc = 0.12857 | Val: loss = 1.93453, acc = 0.34800\n",
            "Training: loss = 1.93183, acc = 0.22857 | Val: loss = 1.93066, acc = 0.50200\n",
            "Training: loss = 1.93080, acc = 0.17857 | Val: loss = 1.92793, acc = 0.60600\n",
            "Training: loss = 1.92470, acc = 0.22143 | Val: loss = 1.92537, acc = 0.59000\n",
            "Training: loss = 1.92791, acc = 0.23571 | Val: loss = 1.92318, acc = 0.53600\n",
            "Training: loss = 1.90893, acc = 0.28571 | Val: loss = 1.92077, acc = 0.53800\n",
            "Training: loss = 1.91819, acc = 0.27143 | Val: loss = 1.91826, acc = 0.56000\n",
            "Training: loss = 1.91608, acc = 0.31429 | Val: loss = 1.91610, acc = 0.55600\n",
            "Training: loss = 1.91692, acc = 0.21429 | Val: loss = 1.91368, acc = 0.59800\n",
            "Training: loss = 1.90944, acc = 0.24286 | Val: loss = 1.91129, acc = 0.61600\n",
            "Training: loss = 1.91049, acc = 0.33571 | Val: loss = 1.90939, acc = 0.63200\n",
            "Training: loss = 1.90249, acc = 0.28571 | Val: loss = 1.90753, acc = 0.66200\n",
            "Training: loss = 1.89189, acc = 0.31429 | Val: loss = 1.90587, acc = 0.69200\n",
            "Training: loss = 1.89203, acc = 0.33571 | Val: loss = 1.90418, acc = 0.69400\n",
            "Training: loss = 1.90223, acc = 0.34286 | Val: loss = 1.90300, acc = 0.70400\n",
            "Training: loss = 1.89815, acc = 0.30000 | Val: loss = 1.90060, acc = 0.71600\n",
            "Training: loss = 1.87750, acc = 0.40000 | Val: loss = 1.89792, acc = 0.72800\n",
            "Training: loss = 1.86973, acc = 0.41429 | Val: loss = 1.89517, acc = 0.73000\n",
            "Training: loss = 1.88865, acc = 0.32857 | Val: loss = 1.89361, acc = 0.76200\n",
            "Training: loss = 1.87676, acc = 0.35714 | Val: loss = 1.89173, acc = 0.79200\n",
            "Training: loss = 1.87964, acc = 0.37857 | Val: loss = 1.89000, acc = 0.77600\n",
            "Training: loss = 1.86138, acc = 0.37857 | Val: loss = 1.88823, acc = 0.76200\n",
            "Training: loss = 1.86370, acc = 0.35000 | Val: loss = 1.88579, acc = 0.75800\n",
            "Training: loss = 1.87566, acc = 0.36429 | Val: loss = 1.88381, acc = 0.74200\n",
            "Training: loss = 1.85716, acc = 0.34286 | Val: loss = 1.88067, acc = 0.75000\n",
            "Training: loss = 1.86137, acc = 0.42857 | Val: loss = 1.87729, acc = 0.76200\n",
            "Training: loss = 1.84763, acc = 0.42143 | Val: loss = 1.87378, acc = 0.75800\n",
            "Training: loss = 1.86004, acc = 0.35714 | Val: loss = 1.87019, acc = 0.76000\n",
            "Training: loss = 1.87765, acc = 0.32143 | Val: loss = 1.86726, acc = 0.76600\n",
            "Training: loss = 1.86157, acc = 0.41429 | Val: loss = 1.86456, acc = 0.77000\n",
            "Training: loss = 1.83500, acc = 0.43571 | Val: loss = 1.86188, acc = 0.77000\n",
            "Training: loss = 1.84004, acc = 0.42143 | Val: loss = 1.85956, acc = 0.77000\n",
            "Training: loss = 1.83140, acc = 0.38571 | Val: loss = 1.85696, acc = 0.76800\n",
            "Training: loss = 1.80607, acc = 0.51429 | Val: loss = 1.85370, acc = 0.77400\n",
            "Training: loss = 1.81967, acc = 0.45000 | Val: loss = 1.85037, acc = 0.77200\n",
            "Training: loss = 1.79830, acc = 0.42857 | Val: loss = 1.84841, acc = 0.75600\n",
            "Training: loss = 1.85769, acc = 0.39286 | Val: loss = 1.84604, acc = 0.75600\n",
            "Training: loss = 1.83079, acc = 0.39286 | Val: loss = 1.84332, acc = 0.76800\n",
            "Training: loss = 1.79429, acc = 0.45000 | Val: loss = 1.84104, acc = 0.76200\n",
            "Training: loss = 1.80828, acc = 0.40714 | Val: loss = 1.83832, acc = 0.74600\n",
            "Training: loss = 1.76647, acc = 0.46429 | Val: loss = 1.83493, acc = 0.74200\n",
            "Training: loss = 1.75622, acc = 0.50000 | Val: loss = 1.83123, acc = 0.74400\n",
            "Training: loss = 1.78104, acc = 0.49286 | Val: loss = 1.82690, acc = 0.75000\n",
            "Training: loss = 1.77628, acc = 0.45000 | Val: loss = 1.82199, acc = 0.76000\n",
            "Training: loss = 1.75749, acc = 0.52143 | Val: loss = 1.81843, acc = 0.74400\n",
            "Training: loss = 1.76451, acc = 0.43571 | Val: loss = 1.81430, acc = 0.74200\n",
            "Training: loss = 1.76924, acc = 0.48571 | Val: loss = 1.81022, acc = 0.75400\n",
            "Training: loss = 1.74645, acc = 0.44286 | Val: loss = 1.80661, acc = 0.75200\n",
            "Training: loss = 1.74263, acc = 0.45000 | Val: loss = 1.80263, acc = 0.75000\n",
            "Training: loss = 1.73158, acc = 0.49286 | Val: loss = 1.79754, acc = 0.76000\n",
            "Training: loss = 1.74194, acc = 0.47857 | Val: loss = 1.79230, acc = 0.77000\n",
            "Training: loss = 1.70208, acc = 0.50714 | Val: loss = 1.78733, acc = 0.78200\n",
            "Training: loss = 1.74153, acc = 0.46429 | Val: loss = 1.78190, acc = 0.79200\n",
            "Training: loss = 1.71165, acc = 0.50000 | Val: loss = 1.77659, acc = 0.79000\n",
            "Training: loss = 1.66429, acc = 0.52143 | Val: loss = 1.77156, acc = 0.79000\n",
            "Training: loss = 1.63717, acc = 0.52857 | Val: loss = 1.76742, acc = 0.78600\n",
            "Training: loss = 1.69979, acc = 0.49286 | Val: loss = 1.76439, acc = 0.78800\n",
            "Training: loss = 1.62373, acc = 0.56429 | Val: loss = 1.76052, acc = 0.78200\n",
            "Training: loss = 1.67819, acc = 0.50714 | Val: loss = 1.75605, acc = 0.76600\n",
            "Training: loss = 1.67700, acc = 0.53571 | Val: loss = 1.75177, acc = 0.75800\n",
            "Training: loss = 1.68625, acc = 0.48571 | Val: loss = 1.74543, acc = 0.76800\n",
            "Training: loss = 1.67003, acc = 0.47857 | Val: loss = 1.73858, acc = 0.77400\n",
            "Training: loss = 1.69299, acc = 0.45714 | Val: loss = 1.73187, acc = 0.78600\n",
            "Training: loss = 1.63103, acc = 0.52143 | Val: loss = 1.72524, acc = 0.79400\n",
            "Training: loss = 1.63830, acc = 0.45714 | Val: loss = 1.71804, acc = 0.80000\n",
            "Training: loss = 1.66464, acc = 0.47857 | Val: loss = 1.71036, acc = 0.80400\n",
            "Training: loss = 1.59135, acc = 0.51429 | Val: loss = 1.70298, acc = 0.80200\n",
            "Training: loss = 1.64800, acc = 0.50000 | Val: loss = 1.69639, acc = 0.80400\n",
            "Training: loss = 1.65136, acc = 0.50714 | Val: loss = 1.69028, acc = 0.80200\n",
            "Training: loss = 1.64694, acc = 0.50714 | Val: loss = 1.68400, acc = 0.80400\n",
            "Training: loss = 1.60376, acc = 0.47143 | Val: loss = 1.67871, acc = 0.79800\n",
            "Training: loss = 1.65375, acc = 0.48571 | Val: loss = 1.67417, acc = 0.79800\n",
            "Training: loss = 1.56947, acc = 0.50714 | Val: loss = 1.67013, acc = 0.79400\n",
            "Training: loss = 1.57099, acc = 0.56429 | Val: loss = 1.66609, acc = 0.79400\n",
            "Training: loss = 1.59956, acc = 0.48571 | Val: loss = 1.66306, acc = 0.79200\n",
            "Training: loss = 1.60469, acc = 0.55000 | Val: loss = 1.66094, acc = 0.78000\n",
            "Training: loss = 1.58063, acc = 0.49286 | Val: loss = 1.65792, acc = 0.75200\n",
            "Training: loss = 1.57004, acc = 0.47143 | Val: loss = 1.65476, acc = 0.72800\n",
            "Training: loss = 1.58374, acc = 0.42857 | Val: loss = 1.65063, acc = 0.72400\n",
            "Training: loss = 1.62618, acc = 0.45714 | Val: loss = 1.64605, acc = 0.72200\n",
            "Training: loss = 1.57743, acc = 0.50714 | Val: loss = 1.64029, acc = 0.72800\n",
            "Training: loss = 1.55006, acc = 0.49286 | Val: loss = 1.63445, acc = 0.73800\n",
            "Training: loss = 1.56439, acc = 0.46429 | Val: loss = 1.62922, acc = 0.75200\n",
            "Training: loss = 1.49105, acc = 0.52143 | Val: loss = 1.62346, acc = 0.76400\n",
            "Training: loss = 1.62176, acc = 0.44286 | Val: loss = 1.61785, acc = 0.77200\n",
            "Training: loss = 1.58908, acc = 0.46429 | Val: loss = 1.61137, acc = 0.78800\n",
            "Training: loss = 1.55220, acc = 0.50000 | Val: loss = 1.60538, acc = 0.79800\n",
            "Training: loss = 1.57831, acc = 0.42143 | Val: loss = 1.59955, acc = 0.80400\n",
            "Training: loss = 1.56790, acc = 0.52857 | Val: loss = 1.59360, acc = 0.80200\n",
            "Training: loss = 1.48064, acc = 0.57857 | Val: loss = 1.58679, acc = 0.79800\n",
            "Training: loss = 1.48272, acc = 0.54286 | Val: loss = 1.57963, acc = 0.79800\n",
            "Training: loss = 1.45726, acc = 0.55714 | Val: loss = 1.57190, acc = 0.80000\n",
            "Training: loss = 1.54936, acc = 0.50000 | Val: loss = 1.56520, acc = 0.81200\n",
            "Training: loss = 1.52390, acc = 0.51429 | Val: loss = 1.55931, acc = 0.81200\n",
            "Training: loss = 1.50916, acc = 0.52857 | Val: loss = 1.55323, acc = 0.81600\n",
            "Training: loss = 1.52972, acc = 0.47857 | Val: loss = 1.54665, acc = 0.81200\n",
            "Training: loss = 1.55390, acc = 0.46429 | Val: loss = 1.54262, acc = 0.81600\n",
            "Training: loss = 1.49678, acc = 0.55714 | Val: loss = 1.53833, acc = 0.80800\n",
            "Training: loss = 1.50856, acc = 0.50714 | Val: loss = 1.53373, acc = 0.80200\n",
            "Training: loss = 1.53670, acc = 0.47857 | Val: loss = 1.52960, acc = 0.80200\n",
            "Training: loss = 1.43341, acc = 0.56429 | Val: loss = 1.52636, acc = 0.79000\n",
            "Training: loss = 1.56229, acc = 0.47857 | Val: loss = 1.52399, acc = 0.79000\n",
            "Training: loss = 1.47238, acc = 0.51429 | Val: loss = 1.52069, acc = 0.79400\n",
            "Training: loss = 1.43060, acc = 0.52143 | Val: loss = 1.51861, acc = 0.79000\n",
            "Training: loss = 1.43055, acc = 0.56429 | Val: loss = 1.51658, acc = 0.79000\n",
            "Training: loss = 1.50215, acc = 0.52143 | Val: loss = 1.51396, acc = 0.79400\n",
            "Training: loss = 1.44008, acc = 0.52143 | Val: loss = 1.51199, acc = 0.79000\n",
            "Training: loss = 1.43973, acc = 0.55000 | Val: loss = 1.50819, acc = 0.79400\n",
            "Training: loss = 1.48844, acc = 0.50714 | Val: loss = 1.50616, acc = 0.79600\n",
            "Training: loss = 1.49889, acc = 0.45000 | Val: loss = 1.50472, acc = 0.79800\n",
            "Training: loss = 1.48246, acc = 0.49286 | Val: loss = 1.50203, acc = 0.79800\n",
            "Training: loss = 1.47829, acc = 0.54286 | Val: loss = 1.49844, acc = 0.80000\n",
            "Training: loss = 1.45686, acc = 0.52143 | Val: loss = 1.49470, acc = 0.79600\n",
            "Training: loss = 1.46390, acc = 0.52857 | Val: loss = 1.49012, acc = 0.79600\n",
            "Training: loss = 1.45419, acc = 0.56429 | Val: loss = 1.48341, acc = 0.79400\n",
            "Training: loss = 1.55487, acc = 0.42857 | Val: loss = 1.47770, acc = 0.79400\n",
            "Training: loss = 1.44212, acc = 0.52143 | Val: loss = 1.47099, acc = 0.79200\n",
            "Training: loss = 1.55720, acc = 0.41429 | Val: loss = 1.46495, acc = 0.79800\n",
            "Training: loss = 1.40420, acc = 0.54286 | Val: loss = 1.45898, acc = 0.80400\n",
            "Training: loss = 1.47037, acc = 0.52143 | Val: loss = 1.45178, acc = 0.80400\n",
            "Training: loss = 1.45966, acc = 0.51429 | Val: loss = 1.44406, acc = 0.80600\n",
            "Training: loss = 1.36441, acc = 0.55000 | Val: loss = 1.43719, acc = 0.81200\n",
            "Training: loss = 1.29666, acc = 0.62857 | Val: loss = 1.43072, acc = 0.80800\n",
            "Training: loss = 1.52617, acc = 0.44286 | Val: loss = 1.42670, acc = 0.81200\n",
            "Training: loss = 1.51697, acc = 0.46429 | Val: loss = 1.42397, acc = 0.81000\n",
            "Training: loss = 1.48224, acc = 0.44286 | Val: loss = 1.42180, acc = 0.80800\n",
            "Training: loss = 1.41076, acc = 0.52857 | Val: loss = 1.42080, acc = 0.80000\n",
            "Training: loss = 1.40147, acc = 0.52857 | Val: loss = 1.41965, acc = 0.80000\n",
            "Training: loss = 1.44851, acc = 0.52857 | Val: loss = 1.42009, acc = 0.80200\n",
            "Training: loss = 1.48730, acc = 0.52143 | Val: loss = 1.41958, acc = 0.80400\n",
            "Training: loss = 1.41503, acc = 0.54286 | Val: loss = 1.41975, acc = 0.80200\n",
            "Training: loss = 1.40646, acc = 0.57143 | Val: loss = 1.42045, acc = 0.80000\n",
            "Training: loss = 1.39711, acc = 0.52857 | Val: loss = 1.42085, acc = 0.80400\n",
            "Training: loss = 1.33656, acc = 0.63571 | Val: loss = 1.41996, acc = 0.80400\n",
            "Training: loss = 1.40324, acc = 0.46429 | Val: loss = 1.41694, acc = 0.80200\n",
            "Training: loss = 1.45497, acc = 0.50000 | Val: loss = 1.41401, acc = 0.80200\n",
            "Training: loss = 1.42682, acc = 0.48571 | Val: loss = 1.41190, acc = 0.80600\n",
            "Training: loss = 1.38419, acc = 0.51429 | Val: loss = 1.40980, acc = 0.79800\n",
            "Training: loss = 1.37801, acc = 0.49286 | Val: loss = 1.40700, acc = 0.79400\n",
            "Training: loss = 1.39436, acc = 0.51429 | Val: loss = 1.40519, acc = 0.80000\n",
            "Training: loss = 1.41695, acc = 0.47143 | Val: loss = 1.40177, acc = 0.79800\n",
            "Training: loss = 1.31630, acc = 0.60000 | Val: loss = 1.39715, acc = 0.79600\n",
            "Training: loss = 1.42122, acc = 0.53571 | Val: loss = 1.39429, acc = 0.79200\n",
            "Training: loss = 1.36011, acc = 0.52143 | Val: loss = 1.39250, acc = 0.79400\n",
            "Training: loss = 1.24156, acc = 0.58571 | Val: loss = 1.39020, acc = 0.80200\n",
            "Training: loss = 1.35566, acc = 0.52857 | Val: loss = 1.38877, acc = 0.80200\n",
            "Training: loss = 1.39649, acc = 0.50000 | Val: loss = 1.38938, acc = 0.80400\n",
            "Training: loss = 1.37743, acc = 0.52857 | Val: loss = 1.38902, acc = 0.79600\n",
            "Training: loss = 1.35299, acc = 0.51429 | Val: loss = 1.38635, acc = 0.79400\n",
            "Training: loss = 1.40547, acc = 0.56429 | Val: loss = 1.38162, acc = 0.80000\n",
            "Training: loss = 1.38736, acc = 0.48571 | Val: loss = 1.37940, acc = 0.79600\n",
            "Training: loss = 1.35535, acc = 0.52857 | Val: loss = 1.37762, acc = 0.78600\n",
            "Training: loss = 1.41682, acc = 0.48571 | Val: loss = 1.37472, acc = 0.78400\n",
            "Training: loss = 1.37882, acc = 0.46429 | Val: loss = 1.37226, acc = 0.78400\n",
            "Training: loss = 1.28779, acc = 0.60714 | Val: loss = 1.36908, acc = 0.78800\n",
            "Training: loss = 1.41493, acc = 0.48571 | Val: loss = 1.36519, acc = 0.79200\n",
            "Training: loss = 1.36923, acc = 0.48571 | Val: loss = 1.36121, acc = 0.79600\n",
            "Training: loss = 1.25301, acc = 0.58571 | Val: loss = 1.35561, acc = 0.79600\n",
            "Training: loss = 1.30798, acc = 0.61429 | Val: loss = 1.35130, acc = 0.79800\n",
            "Training: loss = 1.34470, acc = 0.54286 | Val: loss = 1.34621, acc = 0.80200\n",
            "Training: loss = 1.44156, acc = 0.50000 | Val: loss = 1.34288, acc = 0.80200\n",
            "Training: loss = 1.27159, acc = 0.60000 | Val: loss = 1.33895, acc = 0.80000\n",
            "Training: loss = 1.35842, acc = 0.51429 | Val: loss = 1.33427, acc = 0.80800\n",
            "Training: loss = 1.34614, acc = 0.52857 | Val: loss = 1.32777, acc = 0.80800\n",
            "Training: loss = 1.40924, acc = 0.51429 | Val: loss = 1.31979, acc = 0.81000\n",
            "Training: loss = 1.28443, acc = 0.59286 | Val: loss = 1.31278, acc = 0.81400\n",
            "Training: loss = 1.38599, acc = 0.51429 | Val: loss = 1.30475, acc = 0.81000\n",
            "Training: loss = 1.36793, acc = 0.52143 | Val: loss = 1.29804, acc = 0.81400\n",
            "Training: loss = 1.36423, acc = 0.50000 | Val: loss = 1.29204, acc = 0.81600\n",
            "Training: loss = 1.29960, acc = 0.55714 | Val: loss = 1.28733, acc = 0.82200\n",
            "Training: loss = 1.48396, acc = 0.44286 | Val: loss = 1.28464, acc = 0.82000\n",
            "Training: loss = 1.31637, acc = 0.50714 | Val: loss = 1.28336, acc = 0.81800\n",
            "Training: loss = 1.29386, acc = 0.55000 | Val: loss = 1.28314, acc = 0.82000\n",
            "Training: loss = 1.32258, acc = 0.52143 | Val: loss = 1.28415, acc = 0.81600\n",
            "Training: loss = 1.26415, acc = 0.52857 | Val: loss = 1.28527, acc = 0.81400\n",
            "Training: loss = 1.39665, acc = 0.52143 | Val: loss = 1.29001, acc = 0.80400\n",
            "Training: loss = 1.32794, acc = 0.50714 | Val: loss = 1.29368, acc = 0.80400\n",
            "Training: loss = 1.25961, acc = 0.56429 | Val: loss = 1.29828, acc = 0.80000\n",
            "Training: loss = 1.39416, acc = 0.50714 | Val: loss = 1.30224, acc = 0.80000\n",
            "Training: loss = 1.26742, acc = 0.55000 | Val: loss = 1.30635, acc = 0.80200\n",
            "Training: loss = 1.41031, acc = 0.46429 | Val: loss = 1.30997, acc = 0.80000\n",
            "Training: loss = 1.34642, acc = 0.59286 | Val: loss = 1.31225, acc = 0.78800\n",
            "Training: loss = 1.38557, acc = 0.51429 | Val: loss = 1.31495, acc = 0.79000\n",
            "Training: loss = 1.45827, acc = 0.48571 | Val: loss = 1.31548, acc = 0.79000\n",
            "Training: loss = 1.38968, acc = 0.53571 | Val: loss = 1.31382, acc = 0.78800\n",
            "Training: loss = 1.35318, acc = 0.48571 | Val: loss = 1.31190, acc = 0.78800\n",
            "Training: loss = 1.25666, acc = 0.54286 | Val: loss = 1.30680, acc = 0.78800\n",
            "Training: loss = 1.34861, acc = 0.46429 | Val: loss = 1.30049, acc = 0.79400\n",
            "Training: loss = 1.42582, acc = 0.47143 | Val: loss = 1.29520, acc = 0.79200\n",
            "Training: loss = 1.26183, acc = 0.57143 | Val: loss = 1.28973, acc = 0.79200\n",
            "Training: loss = 1.42317, acc = 0.50714 | Val: loss = 1.28526, acc = 0.79800\n",
            "Training: loss = 1.18533, acc = 0.64286 | Val: loss = 1.27955, acc = 0.79400\n",
            "Training: loss = 1.27954, acc = 0.57857 | Val: loss = 1.27388, acc = 0.79200\n",
            "Training: loss = 1.24771, acc = 0.60714 | Val: loss = 1.26720, acc = 0.80000\n",
            "Training: loss = 1.37484, acc = 0.52857 | Val: loss = 1.26088, acc = 0.80800\n",
            "Training: loss = 1.26681, acc = 0.55714 | Val: loss = 1.25424, acc = 0.81000\n",
            "Training: loss = 1.24520, acc = 0.57857 | Val: loss = 1.24823, acc = 0.81400\n",
            "Training: loss = 1.32582, acc = 0.53571 | Val: loss = 1.24536, acc = 0.81400\n",
            "Training: loss = 1.25520, acc = 0.56429 | Val: loss = 1.24374, acc = 0.81400\n",
            "Training: loss = 1.22168, acc = 0.55000 | Val: loss = 1.24422, acc = 0.80800\n",
            "Training: loss = 1.42174, acc = 0.47143 | Val: loss = 1.24422, acc = 0.80400\n",
            "Training: loss = 1.30227, acc = 0.50000 | Val: loss = 1.24256, acc = 0.80400\n",
            "Training: loss = 1.46710, acc = 0.46429 | Val: loss = 1.23877, acc = 0.80400\n",
            "Training: loss = 1.34217, acc = 0.53571 | Val: loss = 1.23591, acc = 0.80600\n",
            "Training: loss = 1.30570, acc = 0.56429 | Val: loss = 1.23470, acc = 0.81200\n",
            "Training: loss = 1.31808, acc = 0.50714 | Val: loss = 1.23448, acc = 0.81400\n",
            "Training: loss = 1.30474, acc = 0.50000 | Val: loss = 1.23427, acc = 0.80400\n",
            "Training: loss = 1.29899, acc = 0.52143 | Val: loss = 1.23466, acc = 0.79400\n",
            "Training: loss = 1.23632, acc = 0.55000 | Val: loss = 1.23518, acc = 0.79600\n",
            "Training: loss = 1.19782, acc = 0.59286 | Val: loss = 1.23511, acc = 0.79200\n",
            "Training: loss = 1.31103, acc = 0.55714 | Val: loss = 1.23602, acc = 0.78800\n",
            "Training: loss = 1.31042, acc = 0.50714 | Val: loss = 1.23738, acc = 0.78400\n",
            "Training: loss = 1.28528, acc = 0.55714 | Val: loss = 1.23924, acc = 0.78400\n",
            "Training: loss = 1.24921, acc = 0.55714 | Val: loss = 1.23672, acc = 0.78200\n",
            "Training: loss = 1.23148, acc = 0.55000 | Val: loss = 1.23376, acc = 0.78200\n",
            "Training: loss = 1.30916, acc = 0.52143 | Val: loss = 1.23080, acc = 0.78800\n",
            "Training: loss = 1.24317, acc = 0.60714 | Val: loss = 1.22864, acc = 0.78800\n",
            "Training: loss = 1.42531, acc = 0.46429 | Val: loss = 1.22611, acc = 0.79400\n",
            "Training: loss = 1.36386, acc = 0.50714 | Val: loss = 1.22147, acc = 0.80000\n",
            "Training: loss = 1.24569, acc = 0.57857 | Val: loss = 1.21890, acc = 0.79800\n",
            "Training: loss = 1.34116, acc = 0.52143 | Val: loss = 1.21542, acc = 0.80200\n",
            "Training: loss = 1.25021, acc = 0.52857 | Val: loss = 1.21322, acc = 0.80600\n",
            "Training: loss = 1.27699, acc = 0.57857 | Val: loss = 1.21170, acc = 0.80600\n",
            "Training: loss = 1.18624, acc = 0.62857 | Val: loss = 1.21117, acc = 0.80200\n",
            "Training: loss = 1.26469, acc = 0.52143 | Val: loss = 1.21209, acc = 0.81000\n",
            "Training: loss = 1.36885, acc = 0.47857 | Val: loss = 1.21402, acc = 0.80600\n",
            "Training: loss = 1.27695, acc = 0.50000 | Val: loss = 1.21711, acc = 0.80800\n",
            "Training: loss = 1.16061, acc = 0.60714 | Val: loss = 1.21986, acc = 0.81400\n",
            "Training: loss = 1.30110, acc = 0.57857 | Val: loss = 1.22071, acc = 0.81400\n",
            "Training: loss = 1.26512, acc = 0.56429 | Val: loss = 1.21985, acc = 0.81200\n",
            "Training: loss = 1.35561, acc = 0.47143 | Val: loss = 1.21698, acc = 0.81400\n",
            "Training: loss = 1.39953, acc = 0.47143 | Val: loss = 1.21244, acc = 0.81200\n",
            "Training: loss = 1.27306, acc = 0.52857 | Val: loss = 1.20825, acc = 0.80800\n",
            "Training: loss = 1.30342, acc = 0.56429 | Val: loss = 1.20641, acc = 0.80800\n",
            "Training: loss = 1.24428, acc = 0.55714 | Val: loss = 1.20405, acc = 0.80800\n",
            "Training: loss = 1.16539, acc = 0.62143 | Val: loss = 1.20267, acc = 0.80400\n",
            "Training: loss = 1.23979, acc = 0.60000 | Val: loss = 1.20034, acc = 0.80200\n",
            "Training: loss = 1.19916, acc = 0.62143 | Val: loss = 1.19785, acc = 0.80000\n",
            "Training: loss = 1.26437, acc = 0.55000 | Val: loss = 1.19529, acc = 0.80400\n",
            "Training: loss = 1.31703, acc = 0.49286 | Val: loss = 1.19263, acc = 0.80400\n",
            "Training: loss = 1.30262, acc = 0.49286 | Val: loss = 1.19107, acc = 0.80200\n",
            "Training: loss = 1.17206, acc = 0.61429 | Val: loss = 1.19078, acc = 0.80200\n",
            "Training: loss = 1.24493, acc = 0.53571 | Val: loss = 1.19046, acc = 0.80000\n",
            "Training: loss = 1.30237, acc = 0.53571 | Val: loss = 1.18896, acc = 0.80400\n",
            "Training: loss = 1.26381, acc = 0.53571 | Val: loss = 1.18889, acc = 0.80400\n",
            "Training: loss = 1.23871, acc = 0.59286 | Val: loss = 1.19091, acc = 0.80200\n",
            "Training: loss = 1.31807, acc = 0.54286 | Val: loss = 1.19301, acc = 0.80000\n",
            "Training: loss = 1.22631, acc = 0.55714 | Val: loss = 1.19806, acc = 0.79800\n",
            "Training: loss = 1.25104, acc = 0.56429 | Val: loss = 1.20154, acc = 0.79400\n",
            "Training: loss = 1.18546, acc = 0.59286 | Val: loss = 1.20296, acc = 0.79200\n",
            "Training: loss = 1.30253, acc = 0.51429 | Val: loss = 1.20463, acc = 0.79200\n",
            "Training: loss = 1.27503, acc = 0.53571 | Val: loss = 1.20436, acc = 0.79200\n",
            "Training: loss = 1.25739, acc = 0.54286 | Val: loss = 1.20298, acc = 0.79800\n",
            "Training: loss = 1.27369, acc = 0.51429 | Val: loss = 1.20051, acc = 0.79800\n",
            "Training: loss = 1.26096, acc = 0.57143 | Val: loss = 1.19400, acc = 0.79400\n",
            "Training: loss = 1.23313, acc = 0.50714 | Val: loss = 1.18823, acc = 0.79600\n",
            "Training: loss = 1.40223, acc = 0.47857 | Val: loss = 1.18024, acc = 0.79600\n",
            "Training: loss = 1.19230, acc = 0.60000 | Val: loss = 1.17375, acc = 0.80000\n",
            "Training: loss = 1.29939, acc = 0.52143 | Val: loss = 1.16753, acc = 0.80600\n",
            "Training: loss = 1.25755, acc = 0.52857 | Val: loss = 1.16272, acc = 0.80400\n",
            "Training: loss = 1.11293, acc = 0.60000 | Val: loss = 1.15879, acc = 0.80800\n",
            "Training: loss = 1.29983, acc = 0.50714 | Val: loss = 1.15691, acc = 0.81000\n",
            "Training: loss = 1.23672, acc = 0.56429 | Val: loss = 1.15666, acc = 0.80200\n",
            "Training: loss = 1.41094, acc = 0.50000 | Val: loss = 1.15820, acc = 0.80000\n",
            "Training: loss = 1.16632, acc = 0.57857 | Val: loss = 1.15969, acc = 0.80200\n",
            "Training: loss = 1.22651, acc = 0.55000 | Val: loss = 1.15996, acc = 0.79600\n",
            "Training: loss = 1.25176, acc = 0.52143 | Val: loss = 1.16072, acc = 0.80200\n",
            "Training: loss = 1.18941, acc = 0.62857 | Val: loss = 1.16175, acc = 0.80400\n",
            "Training: loss = 1.16289, acc = 0.62857 | Val: loss = 1.16200, acc = 0.80600\n",
            "Training: loss = 1.32312, acc = 0.50714 | Val: loss = 1.16099, acc = 0.80600\n",
            "Training: loss = 1.29277, acc = 0.56429 | Val: loss = 1.15962, acc = 0.80600\n",
            "Training: loss = 1.22608, acc = 0.55714 | Val: loss = 1.15966, acc = 0.80200\n",
            "Training: loss = 1.28856, acc = 0.53571 | Val: loss = 1.16043, acc = 0.80400\n",
            "Training: loss = 1.14975, acc = 0.56429 | Val: loss = 1.16215, acc = 0.80400\n",
            "Training: loss = 1.14994, acc = 0.58571 | Val: loss = 1.16304, acc = 0.80400\n",
            "Training: loss = 1.24557, acc = 0.55714 | Val: loss = 1.16356, acc = 0.80000\n",
            "Training: loss = 1.18570, acc = 0.63571 | Val: loss = 1.16344, acc = 0.80000\n",
            "Training: loss = 1.36503, acc = 0.49286 | Val: loss = 1.16383, acc = 0.80200\n",
            "Training: loss = 1.19131, acc = 0.58571 | Val: loss = 1.16266, acc = 0.80000\n",
            "Training: loss = 1.33386, acc = 0.50000 | Val: loss = 1.16245, acc = 0.79600\n",
            "Training: loss = 1.31487, acc = 0.51429 | Val: loss = 1.16030, acc = 0.79800\n",
            "Training: loss = 1.18268, acc = 0.56429 | Val: loss = 1.15801, acc = 0.80000\n",
            "Training: loss = 1.26925, acc = 0.57143 | Val: loss = 1.15343, acc = 0.80200\n",
            "Training: loss = 1.22495, acc = 0.55000 | Val: loss = 1.14979, acc = 0.80200\n",
            "Training: loss = 1.15268, acc = 0.60714 | Val: loss = 1.14794, acc = 0.80200\n",
            "Training: loss = 1.22971, acc = 0.54286 | Val: loss = 1.14530, acc = 0.80400\n",
            "Training: loss = 1.32897, acc = 0.55000 | Val: loss = 1.14312, acc = 0.80000\n",
            "Training: loss = 1.24598, acc = 0.57143 | Val: loss = 1.14329, acc = 0.80200\n",
            "Training: loss = 1.22910, acc = 0.56429 | Val: loss = 1.14274, acc = 0.80200\n",
            "Training: loss = 1.14770, acc = 0.62857 | Val: loss = 1.14282, acc = 0.80000\n",
            "Training: loss = 1.21954, acc = 0.55714 | Val: loss = 1.14142, acc = 0.80200\n",
            "Training: loss = 1.23227, acc = 0.62143 | Val: loss = 1.13798, acc = 0.80000\n",
            "Training: loss = 1.18069, acc = 0.57857 | Val: loss = 1.13389, acc = 0.79800\n",
            "Training: loss = 1.29570, acc = 0.53571 | Val: loss = 1.12991, acc = 0.80600\n",
            "Training: loss = 1.31760, acc = 0.52143 | Val: loss = 1.12688, acc = 0.80400\n",
            "Training: loss = 1.18727, acc = 0.56429 | Val: loss = 1.12416, acc = 0.80400\n",
            "Training: loss = 1.23838, acc = 0.53571 | Val: loss = 1.12323, acc = 0.80400\n",
            "Training: loss = 1.12123, acc = 0.62143 | Val: loss = 1.12331, acc = 0.80800\n",
            "Training: loss = 1.24235, acc = 0.50714 | Val: loss = 1.12705, acc = 0.80800\n",
            "Training: loss = 1.19848, acc = 0.57857 | Val: loss = 1.12980, acc = 0.81000\n",
            "Training: loss = 1.26476, acc = 0.50714 | Val: loss = 1.13380, acc = 0.80800\n",
            "Training: loss = 1.11719, acc = 0.61429 | Val: loss = 1.13640, acc = 0.80400\n",
            "Training: loss = 1.24998, acc = 0.55000 | Val: loss = 1.13942, acc = 0.80400\n",
            "Training: loss = 1.20551, acc = 0.56429 | Val: loss = 1.14198, acc = 0.80400\n",
            "Training: loss = 1.27810, acc = 0.55714 | Val: loss = 1.14359, acc = 0.80400\n",
            "Training: loss = 1.27585, acc = 0.54286 | Val: loss = 1.14214, acc = 0.80200\n",
            "Training: loss = 1.11568, acc = 0.60000 | Val: loss = 1.13807, acc = 0.80200\n",
            "Training: loss = 1.31684, acc = 0.50000 | Val: loss = 1.13424, acc = 0.80200\n",
            "Training: loss = 1.15116, acc = 0.57143 | Val: loss = 1.13072, acc = 0.80600\n",
            "Training: loss = 1.24746, acc = 0.52143 | Val: loss = 1.12819, acc = 0.80400\n",
            "Training: loss = 1.21710, acc = 0.55000 | Val: loss = 1.12670, acc = 0.80400\n",
            "Training: loss = 1.24199, acc = 0.56429 | Val: loss = 1.12521, acc = 0.80400\n",
            "Training: loss = 1.18173, acc = 0.56429 | Val: loss = 1.12264, acc = 0.80400\n",
            "Training: loss = 1.16822, acc = 0.54286 | Val: loss = 1.12319, acc = 0.80200\n",
            "Training: loss = 1.20050, acc = 0.57143 | Val: loss = 1.12417, acc = 0.80400\n",
            "Training: loss = 1.26772, acc = 0.54286 | Val: loss = 1.12693, acc = 0.80400\n",
            "Training: loss = 1.11499, acc = 0.60000 | Val: loss = 1.13001, acc = 0.80200\n",
            "Training: loss = 1.07735, acc = 0.62857 | Val: loss = 1.13062, acc = 0.80000\n",
            "Training: loss = 1.14501, acc = 0.63571 | Val: loss = 1.13097, acc = 0.80000\n",
            "Training: loss = 1.16925, acc = 0.57143 | Val: loss = 1.13000, acc = 0.79600\n",
            "Training: loss = 1.12441, acc = 0.60000 | Val: loss = 1.12979, acc = 0.79400\n",
            "Training: loss = 1.27658, acc = 0.50714 | Val: loss = 1.12910, acc = 0.79600\n",
            "Training: loss = 1.22014, acc = 0.55000 | Val: loss = 1.12894, acc = 0.79200\n",
            "Training: loss = 1.19809, acc = 0.60714 | Val: loss = 1.12861, acc = 0.78800\n",
            "Training: loss = 1.21860, acc = 0.57143 | Val: loss = 1.12744, acc = 0.79200\n",
            "Training: loss = 1.22029, acc = 0.52143 | Val: loss = 1.12502, acc = 0.79000\n",
            "Training: loss = 1.23360, acc = 0.55714 | Val: loss = 1.12185, acc = 0.79800\n",
            "Training: loss = 1.18407, acc = 0.55000 | Val: loss = 1.11897, acc = 0.80000\n",
            "Training: loss = 1.16401, acc = 0.57857 | Val: loss = 1.11588, acc = 0.80200\n",
            "Training: loss = 1.29301, acc = 0.48571 | Val: loss = 1.11291, acc = 0.80200\n",
            "Training: loss = 1.28882, acc = 0.54286 | Val: loss = 1.11067, acc = 0.80400\n",
            "Training: loss = 1.28730, acc = 0.53571 | Val: loss = 1.10678, acc = 0.80400\n",
            "Training: loss = 1.21831, acc = 0.57857 | Val: loss = 1.10384, acc = 0.80200\n",
            "Training: loss = 1.21767, acc = 0.54286 | Val: loss = 1.10160, acc = 0.81000\n",
            "Training: loss = 1.24394, acc = 0.55714 | Val: loss = 1.09965, acc = 0.81000\n",
            "Training: loss = 1.28286, acc = 0.55714 | Val: loss = 1.09704, acc = 0.80600\n",
            "Training: loss = 1.26801, acc = 0.50714 | Val: loss = 1.09504, acc = 0.80800\n",
            "Training: loss = 1.24400, acc = 0.56429 | Val: loss = 1.09317, acc = 0.80400\n",
            "Training: loss = 1.25303, acc = 0.53571 | Val: loss = 1.09369, acc = 0.81000\n",
            "Training: loss = 1.32560, acc = 0.54286 | Val: loss = 1.09355, acc = 0.80800\n",
            "Training: loss = 1.20634, acc = 0.53571 | Val: loss = 1.09360, acc = 0.81200\n",
            "Training: loss = 1.21895, acc = 0.55000 | Val: loss = 1.09444, acc = 0.80800\n",
            "Training: loss = 1.31182, acc = 0.52143 | Val: loss = 1.09602, acc = 0.80800\n",
            "Training: loss = 1.29280, acc = 0.54286 | Val: loss = 1.09597, acc = 0.81200\n",
            "Training: loss = 1.09837, acc = 0.63571 | Val: loss = 1.09563, acc = 0.81400\n",
            "Training: loss = 1.18334, acc = 0.57857 | Val: loss = 1.09693, acc = 0.80800\n",
            "Training: loss = 1.17876, acc = 0.55714 | Val: loss = 1.09897, acc = 0.80600\n",
            "Training: loss = 1.25274, acc = 0.47143 | Val: loss = 1.09981, acc = 0.80400\n",
            "Training: loss = 1.22228, acc = 0.59286 | Val: loss = 1.10009, acc = 0.80800\n",
            "Training: loss = 1.16105, acc = 0.58571 | Val: loss = 1.10060, acc = 0.81000\n",
            "Training: loss = 1.15352, acc = 0.55000 | Val: loss = 1.10414, acc = 0.80400\n",
            "Training: loss = 1.22529, acc = 0.54286 | Val: loss = 1.10784, acc = 0.80200\n",
            "Training: loss = 1.21135, acc = 0.58571 | Val: loss = 1.11046, acc = 0.80000\n",
            "Training: loss = 1.28133, acc = 0.52857 | Val: loss = 1.11318, acc = 0.80000\n",
            "Training: loss = 1.06860, acc = 0.65714 | Val: loss = 1.11422, acc = 0.80000\n",
            "Training: loss = 1.22165, acc = 0.54286 | Val: loss = 1.11468, acc = 0.79800\n",
            "Training: loss = 1.33981, acc = 0.49286 | Val: loss = 1.11516, acc = 0.80000\n",
            "Training: loss = 1.20865, acc = 0.52857 | Val: loss = 1.11367, acc = 0.80000\n",
            "Training: loss = 1.35920, acc = 0.45714 | Val: loss = 1.11326, acc = 0.79600\n",
            "Training: loss = 1.19019, acc = 0.57143 | Val: loss = 1.11146, acc = 0.79800\n",
            "Training: loss = 1.29913, acc = 0.50000 | Val: loss = 1.10993, acc = 0.79800\n",
            "Training: loss = 1.20339, acc = 0.54286 | Val: loss = 1.10970, acc = 0.79800\n",
            "Training: loss = 1.13989, acc = 0.56429 | Val: loss = 1.10857, acc = 0.79600\n",
            "Training: loss = 1.12228, acc = 0.62857 | Val: loss = 1.10817, acc = 0.79800\n",
            "Training: loss = 1.09564, acc = 0.63571 | Val: loss = 1.10631, acc = 0.79400\n",
            "Training: loss = 1.17492, acc = 0.57143 | Val: loss = 1.10569, acc = 0.79600\n",
            "Training: loss = 1.26707, acc = 0.50714 | Val: loss = 1.10261, acc = 0.80000\n",
            "Training: loss = 1.23131, acc = 0.53571 | Val: loss = 1.09834, acc = 0.80200\n",
            "Training: loss = 1.18201, acc = 0.58571 | Val: loss = 1.09343, acc = 0.80000\n",
            "Training: loss = 1.16243, acc = 0.57143 | Val: loss = 1.08845, acc = 0.80200\n",
            "Training: loss = 1.13844, acc = 0.60714 | Val: loss = 1.08426, acc = 0.80800\n",
            "Training: loss = 1.17121, acc = 0.58571 | Val: loss = 1.08062, acc = 0.81000\n",
            "Training: loss = 1.26710, acc = 0.55714 | Val: loss = 1.07700, acc = 0.80600\n",
            "Training: loss = 1.09871, acc = 0.56429 | Val: loss = 1.07465, acc = 0.80600\n",
            "Training: loss = 1.25583, acc = 0.52143 | Val: loss = 1.07297, acc = 0.80400\n",
            "Training: loss = 1.18382, acc = 0.56429 | Val: loss = 1.07054, acc = 0.80600\n",
            "Training: loss = 1.15223, acc = 0.57857 | Val: loss = 1.06954, acc = 0.80800\n",
            "Training: loss = 1.28635, acc = 0.53571 | Val: loss = 1.07014, acc = 0.80800\n",
            "Training: loss = 1.26768, acc = 0.53571 | Val: loss = 1.07203, acc = 0.81200\n",
            "Training: loss = 1.16319, acc = 0.59286 | Val: loss = 1.07405, acc = 0.80600\n",
            "Training: loss = 1.22835, acc = 0.50714 | Val: loss = 1.07582, acc = 0.80800\n",
            "Training: loss = 1.23723, acc = 0.53571 | Val: loss = 1.07754, acc = 0.80600\n",
            "Training: loss = 1.17953, acc = 0.56429 | Val: loss = 1.07871, acc = 0.80800\n",
            "Training: loss = 1.23142, acc = 0.50714 | Val: loss = 1.07851, acc = 0.80600\n",
            "Training: loss = 1.20316, acc = 0.57857 | Val: loss = 1.07794, acc = 0.80400\n",
            "Training: loss = 1.23535, acc = 0.48571 | Val: loss = 1.07804, acc = 0.80200\n",
            "Training: loss = 1.14502, acc = 0.61429 | Val: loss = 1.07664, acc = 0.80200\n",
            "Training: loss = 1.21896, acc = 0.57143 | Val: loss = 1.07569, acc = 0.80600\n",
            "Training: loss = 1.23167, acc = 0.55000 | Val: loss = 1.07444, acc = 0.80800\n",
            "Training: loss = 1.31366, acc = 0.55000 | Val: loss = 1.07327, acc = 0.80600\n",
            "Training: loss = 1.08967, acc = 0.59286 | Val: loss = 1.06990, acc = 0.80600\n",
            "Training: loss = 1.16979, acc = 0.61429 | Val: loss = 1.06637, acc = 0.80600\n",
            "Training: loss = 1.14934, acc = 0.59286 | Val: loss = 1.06436, acc = 0.80600\n",
            "Training: loss = 1.20105, acc = 0.55714 | Val: loss = 1.06288, acc = 0.80800\n",
            "Training: loss = 1.20556, acc = 0.54286 | Val: loss = 1.06206, acc = 0.80800\n",
            "Training: loss = 1.27931, acc = 0.52143 | Val: loss = 1.06304, acc = 0.80800\n",
            "Training: loss = 1.22502, acc = 0.56429 | Val: loss = 1.06593, acc = 0.80600\n",
            "Training: loss = 1.32694, acc = 0.54286 | Val: loss = 1.06785, acc = 0.80800\n",
            "Training: loss = 1.25794, acc = 0.56429 | Val: loss = 1.06960, acc = 0.80800\n",
            "Training: loss = 1.23404, acc = 0.54286 | Val: loss = 1.07001, acc = 0.80600\n",
            "Training: loss = 1.14885, acc = 0.62143 | Val: loss = 1.07149, acc = 0.80800\n",
            "Training: loss = 1.28990, acc = 0.50714 | Val: loss = 1.07080, acc = 0.80800\n",
            "Training: loss = 1.25074, acc = 0.51429 | Val: loss = 1.07091, acc = 0.81000\n",
            "Training: loss = 1.18813, acc = 0.52143 | Val: loss = 1.06963, acc = 0.80800\n",
            "Training: loss = 1.11086, acc = 0.57143 | Val: loss = 1.06939, acc = 0.80800\n",
            "Training: loss = 1.11155, acc = 0.58571 | Val: loss = 1.07068, acc = 0.80800\n",
            "Training: loss = 1.17440, acc = 0.61429 | Val: loss = 1.07362, acc = 0.81000\n",
            "Training: loss = 1.24290, acc = 0.53571 | Val: loss = 1.07660, acc = 0.80400\n",
            "Training: loss = 1.21257, acc = 0.51429 | Val: loss = 1.08034, acc = 0.80200\n",
            "Training: loss = 1.35817, acc = 0.54286 | Val: loss = 1.08149, acc = 0.80000\n",
            "Training: loss = 1.13448, acc = 0.60000 | Val: loss = 1.07921, acc = 0.79600\n",
            "Training: loss = 1.15319, acc = 0.60714 | Val: loss = 1.07679, acc = 0.79800\n",
            "Training: loss = 1.16030, acc = 0.57143 | Val: loss = 1.07617, acc = 0.80000\n",
            "Training: loss = 1.14051, acc = 0.57143 | Val: loss = 1.07575, acc = 0.80000\n",
            "Training: loss = 1.17991, acc = 0.60714 | Val: loss = 1.07425, acc = 0.80200\n",
            "Training: loss = 1.19235, acc = 0.57143 | Val: loss = 1.07278, acc = 0.80000\n",
            "Training: loss = 1.16979, acc = 0.60000 | Val: loss = 1.07324, acc = 0.79800\n",
            "Training: loss = 1.10741, acc = 0.60714 | Val: loss = 1.07314, acc = 0.80000\n",
            "Training: loss = 1.20623, acc = 0.56429 | Val: loss = 1.07473, acc = 0.80000\n",
            "Training: loss = 1.34991, acc = 0.48571 | Val: loss = 1.07917, acc = 0.79600\n",
            "Training: loss = 1.22103, acc = 0.53571 | Val: loss = 1.08144, acc = 0.79400\n",
            "Training: loss = 1.15730, acc = 0.58571 | Val: loss = 1.08380, acc = 0.79600\n",
            "Training: loss = 1.22068, acc = 0.55000 | Val: loss = 1.08455, acc = 0.79800\n",
            "Training: loss = 1.30147, acc = 0.50000 | Val: loss = 1.08261, acc = 0.79800\n",
            "Training: loss = 1.17776, acc = 0.58571 | Val: loss = 1.08005, acc = 0.79600\n",
            "Training: loss = 1.17984, acc = 0.60000 | Val: loss = 1.07488, acc = 0.79600\n",
            "Training: loss = 1.20282, acc = 0.60000 | Val: loss = 1.06972, acc = 0.79600\n",
            "Training: loss = 1.11277, acc = 0.60000 | Val: loss = 1.06561, acc = 0.79600\n",
            "Training: loss = 1.38452, acc = 0.52143 | Val: loss = 1.06000, acc = 0.79600\n",
            "Training: loss = 1.32362, acc = 0.51429 | Val: loss = 1.05652, acc = 0.79600\n",
            "Training: loss = 1.18489, acc = 0.54286 | Val: loss = 1.05545, acc = 0.79600\n",
            "Training: loss = 1.22212, acc = 0.61429 | Val: loss = 1.05512, acc = 0.80200\n",
            "Training: loss = 1.10929, acc = 0.55714 | Val: loss = 1.05407, acc = 0.80200\n",
            "Training: loss = 1.16017, acc = 0.60714 | Val: loss = 1.05187, acc = 0.80400\n",
            "Training: loss = 1.32645, acc = 0.50000 | Val: loss = 1.05258, acc = 0.80000\n",
            "Training: loss = 1.19708, acc = 0.55000 | Val: loss = 1.05259, acc = 0.80000\n",
            "Training: loss = 1.25280, acc = 0.55714 | Val: loss = 1.05224, acc = 0.80600\n",
            "Training: loss = 1.14591, acc = 0.60000 | Val: loss = 1.05428, acc = 0.81000\n",
            "Training: loss = 1.17223, acc = 0.55714 | Val: loss = 1.05676, acc = 0.81400\n",
            "Training: loss = 1.12626, acc = 0.57143 | Val: loss = 1.06008, acc = 0.81400\n",
            "Training: loss = 1.14599, acc = 0.61429 | Val: loss = 1.06206, acc = 0.81400\n",
            "Training: loss = 1.20026, acc = 0.58571 | Val: loss = 1.06330, acc = 0.81400\n",
            "Training: loss = 1.25144, acc = 0.52857 | Val: loss = 1.06342, acc = 0.81400\n",
            "Training: loss = 1.29273, acc = 0.50714 | Val: loss = 1.06285, acc = 0.81200\n",
            "Training: loss = 1.23978, acc = 0.53571 | Val: loss = 1.06382, acc = 0.81000\n",
            "Training: loss = 1.11237, acc = 0.62143 | Val: loss = 1.06495, acc = 0.80400\n",
            "Training: loss = 1.28717, acc = 0.46429 | Val: loss = 1.06605, acc = 0.80400\n",
            "Training: loss = 1.32923, acc = 0.45714 | Val: loss = 1.06801, acc = 0.80200\n",
            "Training: loss = 1.22429, acc = 0.52857 | Val: loss = 1.06842, acc = 0.80400\n",
            "Training: loss = 1.15797, acc = 0.57857 | Val: loss = 1.06904, acc = 0.80600\n",
            "Training: loss = 1.27727, acc = 0.50000 | Val: loss = 1.06965, acc = 0.80400\n",
            "Training: loss = 1.17530, acc = 0.57143 | Val: loss = 1.06914, acc = 0.80400\n",
            "Training: loss = 1.21765, acc = 0.55000 | Val: loss = 1.07108, acc = 0.80000\n",
            "Training: loss = 1.23220, acc = 0.57857 | Val: loss = 1.07127, acc = 0.80200\n",
            "Training: loss = 1.14743, acc = 0.60000 | Val: loss = 1.07109, acc = 0.79600\n",
            "Training: loss = 1.19896, acc = 0.56429 | Val: loss = 1.06939, acc = 0.79600\n",
            "Training: loss = 1.18844, acc = 0.52857 | Val: loss = 1.06770, acc = 0.80000\n",
            "Training: loss = 1.25570, acc = 0.54286 | Val: loss = 1.06436, acc = 0.79600\n",
            "Training: loss = 1.37678, acc = 0.47143 | Val: loss = 1.06315, acc = 0.79600\n",
            "Training: loss = 1.13840, acc = 0.57857 | Val: loss = 1.06102, acc = 0.80000\n",
            "Training: loss = 1.20320, acc = 0.57857 | Val: loss = 1.05915, acc = 0.80200\n",
            "Training: loss = 1.23642, acc = 0.53571 | Val: loss = 1.05747, acc = 0.80400\n",
            "Training: loss = 1.16406, acc = 0.55714 | Val: loss = 1.05589, acc = 0.80800\n",
            "Training: loss = 1.28145, acc = 0.51429 | Val: loss = 1.05377, acc = 0.80600\n",
            "Training: loss = 1.25440, acc = 0.52857 | Val: loss = 1.05310, acc = 0.80400\n",
            "Training: loss = 1.25344, acc = 0.53571 | Val: loss = 1.05282, acc = 0.80400\n",
            "Training: loss = 1.19304, acc = 0.52857 | Val: loss = 1.05399, acc = 0.80600\n",
            "Training: loss = 1.06627, acc = 0.67143 | Val: loss = 1.05511, acc = 0.80600\n",
            "Training: loss = 1.14691, acc = 0.61429 | Val: loss = 1.05713, acc = 0.80400\n",
            "Training: loss = 1.17402, acc = 0.62857 | Val: loss = 1.05914, acc = 0.80400\n",
            "Training: loss = 1.16979, acc = 0.57143 | Val: loss = 1.05839, acc = 0.80400\n",
            "Training: loss = 1.13657, acc = 0.59286 | Val: loss = 1.06044, acc = 0.80200\n",
            "Training: loss = 1.13961, acc = 0.60714 | Val: loss = 1.06273, acc = 0.81000\n",
            "Training: loss = 1.24974, acc = 0.53571 | Val: loss = 1.06487, acc = 0.80000\n",
            "Training: loss = 1.09438, acc = 0.62143 | Val: loss = 1.06554, acc = 0.80000\n",
            "Training: loss = 1.20226, acc = 0.52857 | Val: loss = 1.06878, acc = 0.79600\n",
            "Training: loss = 1.19499, acc = 0.54286 | Val: loss = 1.07222, acc = 0.79600\n",
            "Training: loss = 1.24830, acc = 0.57143 | Val: loss = 1.07476, acc = 0.79200\n",
            "Training: loss = 1.19590, acc = 0.53571 | Val: loss = 1.07856, acc = 0.79000\n",
            "Training: loss = 1.23878, acc = 0.52143 | Val: loss = 1.08092, acc = 0.78800\n",
            "Training: loss = 1.20447, acc = 0.55000 | Val: loss = 1.08338, acc = 0.78600\n",
            "Training: loss = 1.27288, acc = 0.56429 | Val: loss = 1.08298, acc = 0.79000\n",
            "Training: loss = 1.26954, acc = 0.52143 | Val: loss = 1.08155, acc = 0.79000\n",
            "Training: loss = 1.17504, acc = 0.54286 | Val: loss = 1.07994, acc = 0.79400\n",
            "Training: loss = 1.22092, acc = 0.57857 | Val: loss = 1.07892, acc = 0.79600\n",
            "Training: loss = 1.30155, acc = 0.52143 | Val: loss = 1.07875, acc = 0.80400\n",
            "Training: loss = 1.10872, acc = 0.57857 | Val: loss = 1.07614, acc = 0.80000\n",
            "Training: loss = 1.14354, acc = 0.57143 | Val: loss = 1.07325, acc = 0.80000\n",
            "Training: loss = 1.07134, acc = 0.60714 | Val: loss = 1.06930, acc = 0.79800\n",
            "Training: loss = 1.14019, acc = 0.58571 | Val: loss = 1.06445, acc = 0.80000\n",
            "Training: loss = 1.18580, acc = 0.57857 | Val: loss = 1.05956, acc = 0.79600\n",
            "Training: loss = 1.08711, acc = 0.60714 | Val: loss = 1.05585, acc = 0.79800\n",
            "Training: loss = 1.28299, acc = 0.50714 | Val: loss = 1.05171, acc = 0.79400\n",
            "Training: loss = 1.21167, acc = 0.56429 | Val: loss = 1.04841, acc = 0.80000\n",
            "Training: loss = 1.18854, acc = 0.55714 | Val: loss = 1.04529, acc = 0.80000\n",
            "Training: loss = 1.15625, acc = 0.55714 | Val: loss = 1.04189, acc = 0.79800\n",
            "Training: loss = 1.23297, acc = 0.56429 | Val: loss = 1.04000, acc = 0.79800\n",
            "Training: loss = 1.22955, acc = 0.55000 | Val: loss = 1.03862, acc = 0.80000\n",
            "Training: loss = 1.23471, acc = 0.54286 | Val: loss = 1.03776, acc = 0.80200\n",
            "Training: loss = 1.28353, acc = 0.52857 | Val: loss = 1.03664, acc = 0.80200\n",
            "Training: loss = 1.11169, acc = 0.59286 | Val: loss = 1.03907, acc = 0.80400\n",
            "Training: loss = 1.13841, acc = 0.58571 | Val: loss = 1.04026, acc = 0.80200\n",
            "Training: loss = 1.30740, acc = 0.52857 | Val: loss = 1.03904, acc = 0.80000\n",
            "Training: loss = 1.24019, acc = 0.56429 | Val: loss = 1.04001, acc = 0.79600\n",
            "Training: loss = 1.23736, acc = 0.53571 | Val: loss = 1.04006, acc = 0.79800\n",
            "Training: loss = 1.10721, acc = 0.59286 | Val: loss = 1.04104, acc = 0.80000\n",
            "Training: loss = 1.20619, acc = 0.55000 | Val: loss = 1.04256, acc = 0.79600\n",
            "Training: loss = 1.22063, acc = 0.55714 | Val: loss = 1.04329, acc = 0.80000\n",
            "Training: loss = 1.20637, acc = 0.55000 | Val: loss = 1.04461, acc = 0.79400\n",
            "Training: loss = 1.19209, acc = 0.51429 | Val: loss = 1.04404, acc = 0.79600\n",
            "Training: loss = 1.20706, acc = 0.55714 | Val: loss = 1.04479, acc = 0.79400\n",
            "Training: loss = 1.11737, acc = 0.56429 | Val: loss = 1.04507, acc = 0.79000\n",
            "Training: loss = 1.04262, acc = 0.65000 | Val: loss = 1.04527, acc = 0.79800\n",
            "Training: loss = 1.26163, acc = 0.58571 | Val: loss = 1.04313, acc = 0.79800\n",
            "Training: loss = 1.20390, acc = 0.56429 | Val: loss = 1.04151, acc = 0.80200\n",
            "Training: loss = 1.23340, acc = 0.54286 | Val: loss = 1.04010, acc = 0.80200\n",
            "Training: loss = 1.31549, acc = 0.57143 | Val: loss = 1.03655, acc = 0.80200\n",
            "Training: loss = 1.23289, acc = 0.55000 | Val: loss = 1.03362, acc = 0.80400\n",
            "Training: loss = 1.13209, acc = 0.60000 | Val: loss = 1.03026, acc = 0.80400\n",
            "Training: loss = 1.22146, acc = 0.51429 | Val: loss = 1.02775, acc = 0.80600\n",
            "Training: loss = 1.36110, acc = 0.51429 | Val: loss = 1.02720, acc = 0.80600\n",
            "Training: loss = 1.25781, acc = 0.55714 | Val: loss = 1.02856, acc = 0.81000\n",
            "Training: loss = 1.19511, acc = 0.59286 | Val: loss = 1.03203, acc = 0.80600\n",
            "Training: loss = 1.12344, acc = 0.55714 | Val: loss = 1.03639, acc = 0.80600\n",
            "Training: loss = 1.20087, acc = 0.57857 | Val: loss = 1.04254, acc = 0.80600\n",
            "Training: loss = 1.01640, acc = 0.65714 | Val: loss = 1.04507, acc = 0.80600\n",
            "Training: loss = 1.26429, acc = 0.50714 | Val: loss = 1.04564, acc = 0.80600\n",
            "Training: loss = 1.08033, acc = 0.60714 | Val: loss = 1.04884, acc = 0.80200\n",
            "Training: loss = 1.08880, acc = 0.57857 | Val: loss = 1.05096, acc = 0.80000\n",
            "Training: loss = 1.13856, acc = 0.60000 | Val: loss = 1.05233, acc = 0.80200\n",
            "Training: loss = 1.15108, acc = 0.65000 | Val: loss = 1.05114, acc = 0.79800\n",
            "Training: loss = 1.06940, acc = 0.60714 | Val: loss = 1.04834, acc = 0.79600\n",
            "Training: loss = 1.23361, acc = 0.54286 | Val: loss = 1.04352, acc = 0.79800\n",
            "Training: loss = 1.20586, acc = 0.55000 | Val: loss = 1.04051, acc = 0.80400\n",
            "Training: loss = 1.21340, acc = 0.58571 | Val: loss = 1.03664, acc = 0.80200\n",
            "Training: loss = 1.15646, acc = 0.59286 | Val: loss = 1.03425, acc = 0.80600\n",
            "Training: loss = 1.20281, acc = 0.55714 | Val: loss = 1.03291, acc = 0.80800\n",
            "Training: loss = 1.16086, acc = 0.56429 | Val: loss = 1.03022, acc = 0.81000\n",
            "Training: loss = 1.23410, acc = 0.55714 | Val: loss = 1.02901, acc = 0.81000\n",
            "Training: loss = 1.13243, acc = 0.57857 | Val: loss = 1.02857, acc = 0.81000\n",
            "Training: loss = 1.17452, acc = 0.57857 | Val: loss = 1.03021, acc = 0.80800\n",
            "Training: loss = 1.14434, acc = 0.58571 | Val: loss = 1.03296, acc = 0.80600\n",
            "Training: loss = 1.17259, acc = 0.58571 | Val: loss = 1.03438, acc = 0.80800\n",
            "Training: loss = 1.12743, acc = 0.59286 | Val: loss = 1.03736, acc = 0.80800\n",
            "Training: loss = 1.18907, acc = 0.55714 | Val: loss = 1.03801, acc = 0.80600\n",
            "Training: loss = 1.06837, acc = 0.62857 | Val: loss = 1.03658, acc = 0.80200\n",
            "Training: loss = 1.19078, acc = 0.57857 | Val: loss = 1.03644, acc = 0.79400\n",
            "Training: loss = 1.26644, acc = 0.55000 | Val: loss = 1.03780, acc = 0.79600\n",
            "Training: loss = 1.25250, acc = 0.51429 | Val: loss = 1.03794, acc = 0.79400\n",
            "Training: loss = 1.24939, acc = 0.54286 | Val: loss = 1.03635, acc = 0.79200\n",
            "Training: loss = 1.12552, acc = 0.54286 | Val: loss = 1.03540, acc = 0.79200\n",
            "Training: loss = 1.32519, acc = 0.50000 | Val: loss = 1.03416, acc = 0.79400\n",
            "Training: loss = 1.12308, acc = 0.58571 | Val: loss = 1.03336, acc = 0.79400\n",
            "Training: loss = 1.24745, acc = 0.57143 | Val: loss = 1.03271, acc = 0.79200\n",
            "Training: loss = 1.18073, acc = 0.55000 | Val: loss = 1.03407, acc = 0.79400\n",
            "Training: loss = 1.14512, acc = 0.58571 | Val: loss = 1.03544, acc = 0.79400\n",
            "Training: loss = 1.25799, acc = 0.57143 | Val: loss = 1.03847, acc = 0.79400\n",
            "Training: loss = 1.12135, acc = 0.58571 | Val: loss = 1.04301, acc = 0.79400\n",
            "Training: loss = 1.16512, acc = 0.57143 | Val: loss = 1.04622, acc = 0.79400\n",
            "Training: loss = 1.21767, acc = 0.58571 | Val: loss = 1.04922, acc = 0.78600\n",
            "Training: loss = 1.12265, acc = 0.65714 | Val: loss = 1.05151, acc = 0.79400\n",
            "Training: loss = 1.12337, acc = 0.55714 | Val: loss = 1.05288, acc = 0.79200\n",
            "Training: loss = 1.08102, acc = 0.61429 | Val: loss = 1.05312, acc = 0.79400\n",
            "Training: loss = 1.21080, acc = 0.54286 | Val: loss = 1.05230, acc = 0.79800\n",
            "Training: loss = 1.27233, acc = 0.54286 | Val: loss = 1.04968, acc = 0.80000\n",
            "Training: loss = 1.26361, acc = 0.59286 | Val: loss = 1.05121, acc = 0.79600\n",
            "Training: loss = 1.09124, acc = 0.59286 | Val: loss = 1.05215, acc = 0.79600\n",
            "Training: loss = 1.23551, acc = 0.48571 | Val: loss = 1.05305, acc = 0.79600\n",
            "Training: loss = 1.23038, acc = 0.60714 | Val: loss = 1.05450, acc = 0.79400\n",
            "Training: loss = 1.16856, acc = 0.56429 | Val: loss = 1.05508, acc = 0.79200\n",
            "Training: loss = 1.20101, acc = 0.54286 | Val: loss = 1.05299, acc = 0.78800\n",
            "Training: loss = 1.33823, acc = 0.47143 | Val: loss = 1.05258, acc = 0.78600\n",
            "Training: loss = 1.20176, acc = 0.54286 | Val: loss = 1.04958, acc = 0.79000\n",
            "Training: loss = 1.23064, acc = 0.55000 | Val: loss = 1.04785, acc = 0.79000\n",
            "Training: loss = 1.26074, acc = 0.52143 | Val: loss = 1.04562, acc = 0.79200\n",
            "Training: loss = 1.04364, acc = 0.63571 | Val: loss = 1.04006, acc = 0.79800\n",
            "Training: loss = 1.20387, acc = 0.59286 | Val: loss = 1.03676, acc = 0.79800\n",
            "Training: loss = 1.20227, acc = 0.55714 | Val: loss = 1.03563, acc = 0.79600\n",
            "Training: loss = 1.12656, acc = 0.56429 | Val: loss = 1.03483, acc = 0.79800\n",
            "Training: loss = 1.22591, acc = 0.55000 | Val: loss = 1.03438, acc = 0.80000\n",
            "Training: loss = 1.14439, acc = 0.55714 | Val: loss = 1.03376, acc = 0.80400\n",
            "Training: loss = 1.16391, acc = 0.57143 | Val: loss = 1.03188, acc = 0.80600\n",
            "Training: loss = 1.07243, acc = 0.62143 | Val: loss = 1.03086, acc = 0.80400\n",
            "Training: loss = 1.10621, acc = 0.55714 | Val: loss = 1.03102, acc = 0.80400\n",
            "Training: loss = 1.18630, acc = 0.57857 | Val: loss = 1.02990, acc = 0.80400\n",
            "Training: loss = 1.19534, acc = 0.55714 | Val: loss = 1.03055, acc = 0.80400\n",
            "Training: loss = 1.28161, acc = 0.49286 | Val: loss = 1.03066, acc = 0.80600\n",
            "Training: loss = 1.21148, acc = 0.50000 | Val: loss = 1.03234, acc = 0.80600\n",
            "Training: loss = 1.23852, acc = 0.50000 | Val: loss = 1.03397, acc = 0.81000\n",
            "Training: loss = 1.20490, acc = 0.57143 | Val: loss = 1.03481, acc = 0.80600\n",
            "Training: loss = 1.23372, acc = 0.53571 | Val: loss = 1.03455, acc = 0.80400\n",
            "Training: loss = 1.21550, acc = 0.57857 | Val: loss = 1.03799, acc = 0.80000\n",
            "Training: loss = 1.16632, acc = 0.55000 | Val: loss = 1.03943, acc = 0.80200\n",
            "Training: loss = 1.20141, acc = 0.56429 | Val: loss = 1.04100, acc = 0.80000\n",
            "Training: loss = 1.31776, acc = 0.49286 | Val: loss = 1.04098, acc = 0.79800\n",
            "Training: loss = 1.09358, acc = 0.60714 | Val: loss = 1.03909, acc = 0.79600\n",
            "Training: loss = 1.14017, acc = 0.61429 | Val: loss = 1.03729, acc = 0.79400\n",
            "Training: loss = 1.11083, acc = 0.59286 | Val: loss = 1.03680, acc = 0.79200\n",
            "Training: loss = 1.19346, acc = 0.52857 | Val: loss = 1.03593, acc = 0.79400\n",
            "Training: loss = 1.21061, acc = 0.53571 | Val: loss = 1.03393, acc = 0.79600\n",
            "Training: loss = 1.32183, acc = 0.49286 | Val: loss = 1.03073, acc = 0.79400\n",
            "Training: loss = 1.16839, acc = 0.60000 | Val: loss = 1.02961, acc = 0.79400\n",
            "Training: loss = 1.21910, acc = 0.56429 | Val: loss = 1.02979, acc = 0.80200\n",
            "Training: loss = 1.13463, acc = 0.60714 | Val: loss = 1.03054, acc = 0.80000\n",
            "Training: loss = 1.19720, acc = 0.56429 | Val: loss = 1.03063, acc = 0.79800\n",
            "Training: loss = 1.18533, acc = 0.55000 | Val: loss = 1.03074, acc = 0.79600\n",
            "Training: loss = 1.13958, acc = 0.57143 | Val: loss = 1.03235, acc = 0.79800\n",
            "Training: loss = 1.19709, acc = 0.57857 | Val: loss = 1.03362, acc = 0.79600\n",
            "Training: loss = 1.17919, acc = 0.57857 | Val: loss = 1.03388, acc = 0.79600\n",
            "Training: loss = 1.15747, acc = 0.55714 | Val: loss = 1.03341, acc = 0.79800\n",
            "Training: loss = 1.05803, acc = 0.59286 | Val: loss = 1.03346, acc = 0.79600\n",
            "Training: loss = 1.26549, acc = 0.47857 | Val: loss = 1.03281, acc = 0.79600\n",
            "Training: loss = 1.22741, acc = 0.51429 | Val: loss = 1.03157, acc = 0.79600\n",
            "Training: loss = 1.19689, acc = 0.56429 | Val: loss = 1.02959, acc = 0.79800\n",
            "Training: loss = 1.24269, acc = 0.50714 | Val: loss = 1.02869, acc = 0.79400\n",
            "Training: loss = 1.23164, acc = 0.51429 | Val: loss = 1.03015, acc = 0.80000\n",
            "Training: loss = 1.24228, acc = 0.55714 | Val: loss = 1.03151, acc = 0.80000\n",
            "Training: loss = 1.27734, acc = 0.55000 | Val: loss = 1.03520, acc = 0.79600\n",
            "Training: loss = 1.22338, acc = 0.53571 | Val: loss = 1.03928, acc = 0.79600\n",
            "Training: loss = 1.14191, acc = 0.59286 | Val: loss = 1.04407, acc = 0.79600\n",
            "Training: loss = 1.09468, acc = 0.60714 | Val: loss = 1.04658, acc = 0.79400\n",
            "Early stop! Min loss:  1.0272002220153809 , Max accuracy:  0.8219998478889465\n",
            "Early stop model validation loss:  1.2873326539993286 , accuracy:  0.8219998478889465\n",
            "INFO:tensorflow:Restoring parameters from /content/GAT/pre_trained/cora/mod_cora.ckpt\n",
            "Test loss: 1.2636394500732422 ; Test accuracy: 0.8299991488456726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IT782UmMofD"
      },
      "source": [
        "## Test driving: [Spektral](https://graphneural.network/) - A very cool GNN library recognized by names such as François Chollet.\n",
        "\n",
        "We'll probably use this library to run our experiments in the future :)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TcH4QsPMyPJ",
        "outputId": "3e7ba3e1-4571-4873-e7fe-9b0dababa09e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "!pip install --quiet spektral"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spektral in /usr/local/lib/python3.6/dist-packages (0.6.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from spektral) (0.22.2.post1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from spektral) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from spektral) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from spektral) (2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from spektral) (1.18.5)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from spektral) (2.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from spektral) (1.4.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from spektral) (4.2.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from spektral) (1.0.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->spektral) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->spektral) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->spektral) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->spektral) (3.0.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->spektral) (4.4.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (3.3.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (2.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (0.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (0.35.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->spektral) (1.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->spektral) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->spektral) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow>=2.1.0->spektral) (50.3.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (1.7.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (1.7.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.1.0->spektral) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuiy9gqTNJXf",
        "outputId": "7b9356f6-219a-4a1f-b601-c5b1ed6d48d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import spektral as spk\n",
        "\n",
        "\n",
        "\n",
        "adj, features, labels, train, val, test = spk.datasets.citation.load_data(dataset_name='cora', normalize_features=True, random_split=True)\n",
        "\n",
        "# Converting one-hot encoding into categorical \n",
        "# values with the indexes of each dataset partition\n",
        "idx_train, idx_val, idx_test = np.where(train)[0], np.where(val)[0], np.where(test)[0]\n",
        "\n",
        "\n",
        "adj = torch.FloatTensor(adj.todense())\n",
        "features = torch.FloatTensor(features.todense())\n",
        "labels_a = torch.LongTensor(np.where(labels)[1])\n",
        "idx_train = torch.LongTensor(idx_train)\n",
        "idx_val = torch.LongTensor(idx_val)\n",
        "idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading citeseer dataset\n",
            "Pre-processing node features\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/spektral/datasets/citation.py:142: RuntimeWarning: divide by zero encountered in power\n",
            "  r_inv = np.power(rowsum, -1).flatten()\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}