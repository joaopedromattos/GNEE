{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GATEventEmbedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNlp9nY1qE+9uAFOCn0LFno",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "117b7c7767e74333bcc86c5250490d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f1e0e8bb337a48e38ffa064bfb60f197",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3218c2261f434a1cbcb3bee872b78835",
              "IPY_MODEL_25b3880a97c94c60b69503bb60de1fcc"
            ]
          }
        },
        "f1e0e8bb337a48e38ffa064bfb60f197": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3218c2261f434a1cbcb3bee872b78835": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_579f8f79a14840ec869488b056c533d4",
            "_dom_classes": [],
            "description": "Batches: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8031e9eb93ec498d9143e0aa5d609fd8"
          }
        },
        "25b3880a97c94c60b69503bb60de1fcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0bda39ed08644b5bbf7c317c302b4f64",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5/5 [00:00&lt;00:00, 13.10it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d0e2d63b7c664e1f9ba4bbcb6b551603"
          }
        },
        "579f8f79a14840ec869488b056c533d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8031e9eb93ec498d9143e0aa5d609fd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0bda39ed08644b5bbf7c317c302b4f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d0e2d63b7c664e1f9ba4bbcb6b551603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "633d1f12b35b43d19caf159dfc3b4e52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_896db5124d6b4681909f2a71003dbee6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ea88a73a21a4403e9daa0e40a9c58769",
              "IPY_MODEL_c086264800ba4fdcb6d48a1380f0d76e"
            ]
          }
        },
        "896db5124d6b4681909f2a71003dbee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ea88a73a21a4403e9daa0e40a9c58769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7ed893bc73984c5fae253c350696ba3b",
            "_dom_classes": [],
            "description": "Iteration 15 | Energy = 28.544935884589414: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 15,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 15,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5a4ed5a30f084968aa03d34d12196749"
          }
        },
        "c086264800ba4fdcb6d48a1380f0d76e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4790502c49b549b9b87e3a040cd2a1e9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 15/15 [00:10&lt;00:00,  1.43it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1f400e125ee548688301cf353b0e8f5a"
          }
        },
        "7ed893bc73984c5fae253c350696ba3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5a4ed5a30f084968aa03d34d12196749": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4790502c49b549b9b87e3a040cd2a1e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1f400e125ee548688301cf353b0e8f5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joaopedromattos/pyGAT/blob/master/GATEventEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNq4clhCly9v"
      },
      "source": [
        "# GAT Event Embedding\n",
        "This notebook establishes a training pipeline for our Event Embedding model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs6EH6Ufysax"
      },
      "source": [
        "## Installing our libraries and required scripts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EuLsjGYmA8w",
        "outputId": "d1d1dbfa-03ae-4b90-9fbc-fe4a203773e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/joaopedromattos/pyGAT\n",
        "!pip install --quiet spektral\n",
        "!pip install git+https://github.com/rmarcacini/sentence-transformers\n",
        "!pip install gdown\n",
        "!gdown https://drive.google.com/uc?id=1NV5t1YhyyOzMF5zAovfbSLdZZLvqrfZ_\n",
        "!unzip distiluse-base-multilingual-cased.zip -d language_model"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pyGAT'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 195 (delta 16), reused 25 (delta 9), pack-reused 163\u001b[K\n",
            "Receiving objects: 100% (195/195), 298.73 KiB | 551.00 KiB/s, done.\n",
            "Resolving deltas: 100% (105/105), done.\n",
            "\u001b[K     |████████████████████████████████| 102kB 2.6MB/s \n",
            "\u001b[?25hCollecting git+https://github.com/rmarcacini/sentence-transformers\n",
            "  Cloning https://github.com/rmarcacini/sentence-transformers to /tmp/pip-req-build-3nms56fs\n",
            "  Running command git clone -q https://github.com/rmarcacini/sentence-transformers /tmp/pip-req-build-3nms56fs\n",
            "Collecting transformers<3.2.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.6) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.6) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.6) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.6) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.6) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.6) (3.2.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (0.7)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 17.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 19.9MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 26.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->sentence-transformers==0.3.6) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers==0.3.6) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers==0.3.6) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.2.0,>=3.1.0->sentence-transformers==0.3.6) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.6-cp36-none-any.whl size=101874 sha256=206bc1a453185d28fdc7a833f05b68c79eb24929b2876755420c50c062aca75f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o1qp2m6v/wheels/88/3c/66/55ee9fb698480d5a5116a8257c15dc363323e4922fb8ad361b\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=3e69ca825e72bd0921f3d8d36eb30919ef4d4013d44702f25b843729f879b6e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.3.6 sentencepiece-0.1.94 tokenizers-0.8.1rc2 transformers-3.1.0\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NV5t1YhyyOzMF5zAovfbSLdZZLvqrfZ_\n",
            "To: /content/distiluse-base-multilingual-cased.zip\n",
            "504MB [00:02, 172MB/s]\n",
            "Archive:  distiluse-base-multilingual-cased.zip\n",
            "   creating: language_model/0_DistilBERT/\n",
            " extracting: language_model/0_DistilBERT/added_tokens.json  \n",
            "  inflating: language_model/0_DistilBERT/config.json  \n",
            "  inflating: language_model/0_DistilBERT/pytorch_model.bin  \n",
            "  inflating: language_model/0_DistilBERT/sentence_distilbert_config.json  \n",
            "  inflating: language_model/0_DistilBERT/special_tokens_map.json  \n",
            "  inflating: language_model/0_DistilBERT/tokenizer_config.json  \n",
            "  inflating: language_model/0_DistilBERT/vocab.txt  \n",
            "   creating: language_model/1_Pooling/\n",
            "  inflating: language_model/1_Pooling/config.json  \n",
            "   creating: language_model/2_Dense/\n",
            "  inflating: language_model/2_Dense/config.json  \n",
            "  inflating: language_model/2_Dense/pytorch_model.bin  \n",
            " extracting: language_model/config.json  \n",
            "  inflating: language_model/modules.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7djUzxZiokMc"
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir('./pyGAT')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjRxJgFJmK0r",
        "outputId": "4d722e6c-5cd5-443d-e9cd-c7ed6f3c28f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import bigquery_storage\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, LabelEncoder\n",
        "from event_graph_utils import mount_graph, regularization, process_event_dataset_from_networkx\n",
        "from train import GAT_wrapper\n",
        "\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-LUqDCJt0yf"
      },
      "source": [
        "## Retrieving events from GDELT in order to create our graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF07dKfumkeo"
      },
      "source": [
        "%%bigquery --project gat-event-embedding df\n",
        "SELECT REGEXP_EXTRACT(Extras, '<PAGE_TITLE>(.*)</PAGE_TITLE>') text, * FROM `gdelt-bq.gdeltv2.gkg_partitioned` WHERE DATE(_PARTITIONTIME) = \"2020-10-05\" LIMIT 150"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5-euv7HvyYd"
      },
      "source": [
        "## Creating our graph and applying a regularization process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw77butTmztc",
        "outputId": "e1b443b5-7a1e-4871-c022-35cbcfff317a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "117b7c7767e74333bcc86c5250490d8a",
            "f1e0e8bb337a48e38ffa064bfb60f197",
            "3218c2261f434a1cbcb3bee872b78835",
            "25b3880a97c94c60b69503bb60de1fcc",
            "579f8f79a14840ec869488b056c533d4",
            "8031e9eb93ec498d9143e0aa5d609fd8",
            "0bda39ed08644b5bbf7c317c302b4f64",
            "d0e2d63b7c664e1f9ba4bbcb6b551603"
          ]
        }
      },
      "source": [
        "G = mount_graph(df)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating graph...\n",
            "2020-11-04 14:34:35 - Load pretrained SentenceTransformer: ../language_model\n",
            "2020-11-04 14:34:35 - Load SentenceTransformer from folder: ../language_model\n",
            "2020-11-04 14:34:39 - Use pytorch device: cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "117b7c7767e74333bcc86c5250490d8a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Batches', max=5.0, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Graph loaded: OK - \t Nodes: 1251 \t  edges: 3725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGwCJOVe1VR4",
        "outputId": "0b328c72-26d3-4605-f7b1-b8ebab5d2cca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "633d1f12b35b43d19caf159dfc3b4e52",
            "896db5124d6b4681909f2a71003dbee6",
            "ea88a73a21a4403e9daa0e40a9c58769",
            "c086264800ba4fdcb6d48a1380f0d76e",
            "7ed893bc73984c5fae253c350696ba3b",
            "5a4ed5a30f084968aa03d34d12196749",
            "4790502c49b549b9b87e3a040cd2a1e9",
            "1f400e125ee548688301cf353b0e8f5a"
          ]
        }
      },
      "source": [
        "G = regularization(G, 512)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "633d1f12b35b43d19caf159dfc3b4e52",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tjDMCGCv5LQ"
      },
      "source": [
        "## Visualization of the label distribuition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L43H9ZiKr4Kc",
        "outputId": "6ea57330-a463-48ef-cd6c-6bae8242df4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "_, _, labels, _, _, _ = process_event_dataset_from_networkx(G)\n",
        "pd.value_counts(labels).plot(kind='bar', title=\"label dist\", xlabel=\"Labels\");"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMp2dH2ytvcl"
      },
      "source": [
        "## Saving our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D875ToIFk7Hh"
      },
      "source": [
        "nx.write_gpickle(G, 'dataset.gpickle')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRFC2tNBwBpR"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxsy4972pyvK",
        "outputId": "d7fc873c-ad04-4643-80b4-46e36cde93ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gat = GAT_wrapper({\"alpha\": 0.2, \"cuda\": True, \"dropout\": 0.6, \"epochs\": 10_000, \"fastmode\": False, \"hidden\": 8, \"lr\": 0.005, \"nb_heads\": 8, \"no_cuda\": False, \"patience\": 100, \"seed\": 72, \"sparse\": False, \"weight_decay\": 0.0005})\n",
        "gat.train_pipeline(G, custom_function=True, function=process_event_dataset_from_networkx)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LOAD DATA]: cora\n",
            "Epoch: 0001 loss_train: 3.5177 acc_train: 0.0213 loss_val: 3.3968 acc_val: 0.1920 time: 0.5406s\n",
            "Epoch: 0002 loss_train: 3.4097 acc_train: 0.0760 loss_val: 3.3079 acc_val: 0.3240 time: 0.4412s\n",
            "Epoch: 0003 loss_train: 3.2905 acc_train: 0.1680 loss_val: 3.2196 acc_val: 0.4240 time: 0.4287s\n",
            "Epoch: 0004 loss_train: 3.2392 acc_train: 0.2280 loss_val: 3.1327 acc_val: 0.4680 time: 0.4190s\n",
            "Epoch: 0005 loss_train: 3.1742 acc_train: 0.2067 loss_val: 3.0470 acc_val: 0.4680 time: 0.4273s\n",
            "Epoch: 0006 loss_train: 3.0928 acc_train: 0.2480 loss_val: 2.9630 acc_val: 0.4680 time: 0.5195s\n",
            "Epoch: 0007 loss_train: 2.9887 acc_train: 0.2840 loss_val: 2.8806 acc_val: 0.4520 time: 0.4939s\n",
            "Epoch: 0008 loss_train: 2.9598 acc_train: 0.2800 loss_val: 2.8008 acc_val: 0.4520 time: 0.4212s\n",
            "Epoch: 0009 loss_train: 2.8604 acc_train: 0.2960 loss_val: 2.7244 acc_val: 0.4520 time: 0.4266s\n",
            "Epoch: 0010 loss_train: 2.8205 acc_train: 0.3080 loss_val: 2.6531 acc_val: 0.4520 time: 0.5250s\n",
            "Epoch: 0011 loss_train: 2.7971 acc_train: 0.2907 loss_val: 2.5860 acc_val: 0.4520 time: 0.5120s\n",
            "Epoch: 0012 loss_train: 2.7003 acc_train: 0.3333 loss_val: 2.5240 acc_val: 0.4520 time: 0.4343s\n",
            "Epoch: 0013 loss_train: 2.7116 acc_train: 0.3133 loss_val: 2.4693 acc_val: 0.4720 time: 0.4292s\n",
            "Epoch: 0014 loss_train: 2.7106 acc_train: 0.3147 loss_val: 2.4184 acc_val: 0.4840 time: 0.4294s\n",
            "Epoch: 0015 loss_train: 2.6193 acc_train: 0.3400 loss_val: 2.3700 acc_val: 0.5000 time: 0.4317s\n",
            "Epoch: 0016 loss_train: 2.5791 acc_train: 0.3307 loss_val: 2.3237 acc_val: 0.5080 time: 0.4334s\n",
            "Epoch: 0017 loss_train: 2.5242 acc_train: 0.3387 loss_val: 2.2794 acc_val: 0.5400 time: 0.4235s\n",
            "Epoch: 0018 loss_train: 2.4592 acc_train: 0.3707 loss_val: 2.2373 acc_val: 0.5520 time: 0.4352s\n",
            "Epoch: 0019 loss_train: 2.5121 acc_train: 0.3533 loss_val: 2.1971 acc_val: 0.5520 time: 0.5027s\n",
            "Epoch: 0020 loss_train: 2.4595 acc_train: 0.3640 loss_val: 2.1580 acc_val: 0.5520 time: 0.4947s\n",
            "Epoch: 0021 loss_train: 2.4753 acc_train: 0.3680 loss_val: 2.1201 acc_val: 0.5560 time: 0.4338s\n",
            "Epoch: 0022 loss_train: 2.4261 acc_train: 0.3800 loss_val: 2.0836 acc_val: 0.5600 time: 0.4305s\n",
            "Epoch: 0023 loss_train: 2.3348 acc_train: 0.4293 loss_val: 2.0470 acc_val: 0.5600 time: 0.4300s\n",
            "Epoch: 0024 loss_train: 2.4042 acc_train: 0.3827 loss_val: 2.0108 acc_val: 0.5600 time: 0.4313s\n",
            "Epoch: 0025 loss_train: 2.2448 acc_train: 0.4240 loss_val: 1.9743 acc_val: 0.5880 time: 0.4226s\n",
            "Epoch: 0026 loss_train: 2.2945 acc_train: 0.4480 loss_val: 1.9372 acc_val: 0.5960 time: 0.4248s\n",
            "Epoch: 0027 loss_train: 2.2591 acc_train: 0.4120 loss_val: 1.9006 acc_val: 0.5960 time: 0.4302s\n",
            "Epoch: 0028 loss_train: 2.2966 acc_train: 0.4267 loss_val: 1.8654 acc_val: 0.6480 time: 0.4344s\n",
            "Epoch: 0029 loss_train: 2.2184 acc_train: 0.4187 loss_val: 1.8307 acc_val: 0.6440 time: 0.4306s\n",
            "Epoch: 0030 loss_train: 2.1950 acc_train: 0.4320 loss_val: 1.7964 acc_val: 0.6440 time: 0.4302s\n",
            "Epoch: 0031 loss_train: 2.1004 acc_train: 0.4693 loss_val: 1.7624 acc_val: 0.6440 time: 0.4331s\n",
            "Epoch: 0032 loss_train: 2.1354 acc_train: 0.4440 loss_val: 1.7293 acc_val: 0.6560 time: 0.4296s\n",
            "Epoch: 0033 loss_train: 2.1388 acc_train: 0.4627 loss_val: 1.6983 acc_val: 0.6600 time: 0.4246s\n",
            "Epoch: 0034 loss_train: 2.1546 acc_train: 0.4627 loss_val: 1.6691 acc_val: 0.6600 time: 0.4181s\n",
            "Epoch: 0035 loss_train: 2.0838 acc_train: 0.4533 loss_val: 1.6414 acc_val: 0.6720 time: 0.4339s\n",
            "Epoch: 0036 loss_train: 2.1003 acc_train: 0.4600 loss_val: 1.6135 acc_val: 0.7000 time: 0.4316s\n",
            "Epoch: 0037 loss_train: 2.1311 acc_train: 0.4560 loss_val: 1.5858 acc_val: 0.7160 time: 0.4354s\n",
            "Epoch: 0038 loss_train: 2.1014 acc_train: 0.4613 loss_val: 1.5592 acc_val: 0.7280 time: 0.4323s\n",
            "Epoch: 0039 loss_train: 2.0989 acc_train: 0.4613 loss_val: 1.5331 acc_val: 0.7280 time: 0.4323s\n",
            "Epoch: 0040 loss_train: 2.0567 acc_train: 0.4840 loss_val: 1.5081 acc_val: 0.7400 time: 0.4265s\n",
            "Epoch: 0041 loss_train: 2.0500 acc_train: 0.4973 loss_val: 1.4840 acc_val: 0.7480 time: 0.4310s\n",
            "Epoch: 0042 loss_train: 2.0683 acc_train: 0.4773 loss_val: 1.4608 acc_val: 0.7520 time: 0.4357s\n",
            "Epoch: 0043 loss_train: 2.0476 acc_train: 0.4640 loss_val: 1.4380 acc_val: 0.7520 time: 0.4314s\n",
            "Epoch: 0044 loss_train: 1.9375 acc_train: 0.5013 loss_val: 1.4162 acc_val: 0.7520 time: 0.4333s\n",
            "Epoch: 0045 loss_train: 2.0048 acc_train: 0.4707 loss_val: 1.3955 acc_val: 0.7600 time: 0.4508s\n",
            "Epoch: 0046 loss_train: 2.0071 acc_train: 0.5013 loss_val: 1.3751 acc_val: 0.7640 time: 0.4138s\n",
            "Epoch: 0047 loss_train: 1.9129 acc_train: 0.5040 loss_val: 1.3552 acc_val: 0.7640 time: 0.4264s\n",
            "Epoch: 0048 loss_train: 1.9461 acc_train: 0.5053 loss_val: 1.3362 acc_val: 0.7560 time: 0.4269s\n",
            "Epoch: 0049 loss_train: 1.8988 acc_train: 0.5227 loss_val: 1.3173 acc_val: 0.7600 time: 0.4382s\n",
            "Epoch: 0050 loss_train: 1.8881 acc_train: 0.5160 loss_val: 1.2995 acc_val: 0.7600 time: 0.4267s\n",
            "Epoch: 0051 loss_train: 1.8948 acc_train: 0.5147 loss_val: 1.2820 acc_val: 0.7600 time: 0.4998s\n",
            "Epoch: 0052 loss_train: 1.9806 acc_train: 0.4800 loss_val: 1.2651 acc_val: 0.7600 time: 0.4755s\n",
            "Epoch: 0053 loss_train: 1.8537 acc_train: 0.5360 loss_val: 1.2485 acc_val: 0.7600 time: 0.4360s\n",
            "Epoch: 0054 loss_train: 1.9314 acc_train: 0.5053 loss_val: 1.2321 acc_val: 0.7600 time: 0.4346s\n",
            "Epoch: 0055 loss_train: 1.9368 acc_train: 0.5093 loss_val: 1.2160 acc_val: 0.7600 time: 0.4366s\n",
            "Epoch: 0056 loss_train: 1.7103 acc_train: 0.5733 loss_val: 1.2004 acc_val: 0.7600 time: 0.4351s\n",
            "Epoch: 0057 loss_train: 1.9156 acc_train: 0.5080 loss_val: 1.1842 acc_val: 0.7640 time: 0.4210s\n",
            "Epoch: 0058 loss_train: 1.8682 acc_train: 0.5173 loss_val: 1.1683 acc_val: 0.7640 time: 0.4305s\n",
            "Epoch: 0059 loss_train: 1.7660 acc_train: 0.5493 loss_val: 1.1534 acc_val: 0.7640 time: 0.4317s\n",
            "Epoch: 0060 loss_train: 1.8482 acc_train: 0.5227 loss_val: 1.1386 acc_val: 0.7640 time: 0.5018s\n",
            "Epoch: 0061 loss_train: 1.9513 acc_train: 0.4867 loss_val: 1.1252 acc_val: 0.7640 time: 0.4791s\n",
            "Epoch: 0062 loss_train: 1.7751 acc_train: 0.5453 loss_val: 1.1121 acc_val: 0.7640 time: 0.5163s\n",
            "Epoch: 0063 loss_train: 1.8012 acc_train: 0.5267 loss_val: 1.0997 acc_val: 0.7640 time: 0.4624s\n",
            "Epoch: 0064 loss_train: 1.8842 acc_train: 0.5173 loss_val: 1.0881 acc_val: 0.7640 time: 0.5077s\n",
            "Epoch: 0065 loss_train: 1.7559 acc_train: 0.5467 loss_val: 1.0772 acc_val: 0.7640 time: 0.4926s\n",
            "Epoch: 0066 loss_train: 1.8331 acc_train: 0.5253 loss_val: 1.0668 acc_val: 0.7640 time: 0.4268s\n",
            "Epoch: 0067 loss_train: 1.8081 acc_train: 0.5173 loss_val: 1.0563 acc_val: 0.7680 time: 0.4335s\n",
            "Epoch: 0068 loss_train: 1.7887 acc_train: 0.5227 loss_val: 1.0460 acc_val: 0.7720 time: 0.4318s\n",
            "Epoch: 0069 loss_train: 1.8527 acc_train: 0.5240 loss_val: 1.0364 acc_val: 0.7720 time: 0.4278s\n",
            "Epoch: 0070 loss_train: 1.8168 acc_train: 0.5387 loss_val: 1.0264 acc_val: 0.7720 time: 0.4304s\n",
            "Epoch: 0071 loss_train: 1.8391 acc_train: 0.5187 loss_val: 1.0168 acc_val: 0.7760 time: 0.4349s\n",
            "Epoch: 0072 loss_train: 1.8414 acc_train: 0.5227 loss_val: 1.0077 acc_val: 0.7760 time: 0.4326s\n",
            "Epoch: 0073 loss_train: 1.7201 acc_train: 0.5560 loss_val: 0.9982 acc_val: 0.7840 time: 0.4325s\n",
            "Epoch: 0074 loss_train: 1.7053 acc_train: 0.5453 loss_val: 0.9900 acc_val: 0.7880 time: 0.4336s\n",
            "Epoch: 0075 loss_train: 1.7508 acc_train: 0.5480 loss_val: 0.9821 acc_val: 0.7880 time: 0.4312s\n",
            "Epoch: 0076 loss_train: 1.7113 acc_train: 0.5560 loss_val: 0.9747 acc_val: 0.7880 time: 0.4343s\n",
            "Epoch: 0077 loss_train: 1.7608 acc_train: 0.5467 loss_val: 0.9666 acc_val: 0.7880 time: 0.4957s\n",
            "Epoch: 0078 loss_train: 1.7690 acc_train: 0.5240 loss_val: 0.9590 acc_val: 0.7880 time: 0.5067s\n",
            "Epoch: 0079 loss_train: 1.7594 acc_train: 0.5427 loss_val: 0.9517 acc_val: 0.7880 time: 0.4300s\n",
            "Epoch: 0080 loss_train: 1.7673 acc_train: 0.5413 loss_val: 0.9445 acc_val: 0.7880 time: 0.4377s\n",
            "Epoch: 0081 loss_train: 1.7070 acc_train: 0.5493 loss_val: 0.9378 acc_val: 0.7880 time: 0.4550s\n",
            "Epoch: 0082 loss_train: 1.7649 acc_train: 0.5413 loss_val: 0.9310 acc_val: 0.7880 time: 0.5016s\n",
            "Epoch: 0083 loss_train: 1.7112 acc_train: 0.5547 loss_val: 0.9243 acc_val: 0.7880 time: 0.5151s\n",
            "Epoch: 0084 loss_train: 1.7828 acc_train: 0.5160 loss_val: 0.9177 acc_val: 0.7880 time: 0.4999s\n",
            "Epoch: 0085 loss_train: 1.6774 acc_train: 0.5547 loss_val: 0.9117 acc_val: 0.7880 time: 0.4225s\n",
            "Epoch: 0086 loss_train: 1.6380 acc_train: 0.5667 loss_val: 0.9059 acc_val: 0.7880 time: 0.4331s\n",
            "Epoch: 0087 loss_train: 1.6469 acc_train: 0.5533 loss_val: 0.9008 acc_val: 0.7880 time: 0.4310s\n",
            "Epoch: 0088 loss_train: 1.6226 acc_train: 0.5773 loss_val: 0.8956 acc_val: 0.8000 time: 0.4293s\n",
            "Epoch: 0089 loss_train: 1.6972 acc_train: 0.5533 loss_val: 0.8910 acc_val: 0.8000 time: 0.4281s\n",
            "Epoch: 0090 loss_train: 1.7337 acc_train: 0.5293 loss_val: 0.8862 acc_val: 0.8000 time: 0.5073s\n",
            "Epoch: 0091 loss_train: 1.6227 acc_train: 0.5680 loss_val: 0.8812 acc_val: 0.8040 time: 0.5043s\n",
            "Epoch: 0092 loss_train: 1.5966 acc_train: 0.5733 loss_val: 0.8765 acc_val: 0.8040 time: 0.4313s\n",
            "Epoch: 0093 loss_train: 1.5879 acc_train: 0.5773 loss_val: 0.8713 acc_val: 0.8040 time: 0.4287s\n",
            "Epoch: 0094 loss_train: 1.7757 acc_train: 0.5253 loss_val: 0.8654 acc_val: 0.8040 time: 0.4316s\n",
            "Epoch: 0095 loss_train: 1.7594 acc_train: 0.5280 loss_val: 0.8592 acc_val: 0.8040 time: 0.4311s\n",
            "Epoch: 0096 loss_train: 1.6924 acc_train: 0.5493 loss_val: 0.8532 acc_val: 0.8080 time: 0.4644s\n",
            "Epoch: 0097 loss_train: 1.7126 acc_train: 0.5387 loss_val: 0.8472 acc_val: 0.8080 time: 0.5102s\n",
            "Epoch: 0098 loss_train: 1.6901 acc_train: 0.5507 loss_val: 0.8409 acc_val: 0.8080 time: 0.4633s\n",
            "Epoch: 0099 loss_train: 1.6099 acc_train: 0.5667 loss_val: 0.8350 acc_val: 0.8080 time: 0.5019s\n",
            "Epoch: 0100 loss_train: 1.6424 acc_train: 0.5587 loss_val: 0.8291 acc_val: 0.8080 time: 0.4597s\n",
            "Epoch: 0101 loss_train: 1.6575 acc_train: 0.5600 loss_val: 0.8238 acc_val: 0.8160 time: 0.4337s\n",
            "Epoch: 0102 loss_train: 1.6055 acc_train: 0.5787 loss_val: 0.8190 acc_val: 0.8160 time: 0.4290s\n",
            "Epoch: 0103 loss_train: 1.6201 acc_train: 0.5627 loss_val: 0.8146 acc_val: 0.8200 time: 0.4482s\n",
            "Epoch: 0104 loss_train: 1.6006 acc_train: 0.5773 loss_val: 0.8104 acc_val: 0.8200 time: 0.5410s\n",
            "Epoch: 0105 loss_train: 1.6117 acc_train: 0.5653 loss_val: 0.8067 acc_val: 0.8200 time: 0.4306s\n",
            "Epoch: 0106 loss_train: 1.6587 acc_train: 0.5600 loss_val: 0.8034 acc_val: 0.8200 time: 0.4315s\n",
            "Epoch: 0107 loss_train: 1.6762 acc_train: 0.5520 loss_val: 0.7999 acc_val: 0.8200 time: 0.4322s\n",
            "Epoch: 0108 loss_train: 1.7607 acc_train: 0.5333 loss_val: 0.7961 acc_val: 0.8200 time: 0.4317s\n",
            "Epoch: 0109 loss_train: 1.6154 acc_train: 0.5653 loss_val: 0.7929 acc_val: 0.8160 time: 0.4356s\n",
            "Epoch: 0110 loss_train: 1.6707 acc_train: 0.5520 loss_val: 0.7898 acc_val: 0.8160 time: 0.4275s\n",
            "Epoch: 0111 loss_train: 1.5824 acc_train: 0.5867 loss_val: 0.7872 acc_val: 0.8160 time: 0.4297s\n",
            "Epoch: 0112 loss_train: 1.6082 acc_train: 0.5840 loss_val: 0.7848 acc_val: 0.8160 time: 0.4330s\n",
            "Epoch: 0113 loss_train: 1.6260 acc_train: 0.5787 loss_val: 0.7821 acc_val: 0.8200 time: 0.4305s\n",
            "Epoch: 0114 loss_train: 1.6406 acc_train: 0.5667 loss_val: 0.7784 acc_val: 0.8200 time: 0.4329s\n",
            "Epoch: 0115 loss_train: 1.5946 acc_train: 0.5760 loss_val: 0.7745 acc_val: 0.8200 time: 0.4313s\n",
            "Epoch: 0116 loss_train: 1.6094 acc_train: 0.5733 loss_val: 0.7706 acc_val: 0.8200 time: 0.4317s\n",
            "Epoch: 0117 loss_train: 1.5636 acc_train: 0.5907 loss_val: 0.7661 acc_val: 0.8160 time: 0.4335s\n",
            "Epoch: 0118 loss_train: 1.6329 acc_train: 0.5533 loss_val: 0.7623 acc_val: 0.8160 time: 0.4372s\n",
            "Epoch: 0119 loss_train: 1.5925 acc_train: 0.5947 loss_val: 0.7585 acc_val: 0.8160 time: 0.4342s\n",
            "Epoch: 0120 loss_train: 1.6328 acc_train: 0.5787 loss_val: 0.7550 acc_val: 0.8160 time: 0.4325s\n",
            "Epoch: 0121 loss_train: 1.6200 acc_train: 0.5680 loss_val: 0.7512 acc_val: 0.8160 time: 0.4959s\n",
            "Epoch: 0122 loss_train: 1.6716 acc_train: 0.5440 loss_val: 0.7481 acc_val: 0.8160 time: 0.4968s\n",
            "Epoch: 0123 loss_train: 1.6204 acc_train: 0.5773 loss_val: 0.7452 acc_val: 0.8160 time: 0.4344s\n",
            "Epoch: 0124 loss_train: 1.5977 acc_train: 0.5760 loss_val: 0.7427 acc_val: 0.8160 time: 0.4251s\n",
            "Epoch: 0125 loss_train: 1.5736 acc_train: 0.5760 loss_val: 0.7398 acc_val: 0.8160 time: 0.4391s\n",
            "Epoch: 0126 loss_train: 1.6145 acc_train: 0.5640 loss_val: 0.7372 acc_val: 0.8160 time: 0.4316s\n",
            "Epoch: 0127 loss_train: 1.6594 acc_train: 0.5480 loss_val: 0.7351 acc_val: 0.8200 time: 0.4343s\n",
            "Epoch: 0128 loss_train: 1.5283 acc_train: 0.5840 loss_val: 0.7327 acc_val: 0.8240 time: 0.4317s\n",
            "Epoch: 0129 loss_train: 1.6908 acc_train: 0.5387 loss_val: 0.7310 acc_val: 0.8240 time: 0.4271s\n",
            "Epoch: 0130 loss_train: 1.6138 acc_train: 0.5693 loss_val: 0.7291 acc_val: 0.8240 time: 0.4292s\n",
            "Epoch: 0131 loss_train: 1.5692 acc_train: 0.5893 loss_val: 0.7267 acc_val: 0.8240 time: 0.4215s\n",
            "Epoch: 0132 loss_train: 1.5623 acc_train: 0.5813 loss_val: 0.7239 acc_val: 0.8240 time: 0.4308s\n",
            "Epoch: 0133 loss_train: 1.6254 acc_train: 0.5760 loss_val: 0.7214 acc_val: 0.8240 time: 0.4323s\n",
            "Epoch: 0134 loss_train: 1.6726 acc_train: 0.5480 loss_val: 0.7190 acc_val: 0.8280 time: 0.4807s\n",
            "Epoch: 0135 loss_train: 1.6352 acc_train: 0.5627 loss_val: 0.7164 acc_val: 0.8280 time: 0.5102s\n",
            "Epoch: 0136 loss_train: 1.5409 acc_train: 0.5920 loss_val: 0.7144 acc_val: 0.8280 time: 0.4487s\n",
            "Epoch: 0137 loss_train: 1.6184 acc_train: 0.5640 loss_val: 0.7119 acc_val: 0.8280 time: 0.4318s\n",
            "Epoch: 0138 loss_train: 1.5689 acc_train: 0.5813 loss_val: 0.7089 acc_val: 0.8240 time: 0.4435s\n",
            "Epoch: 0139 loss_train: 1.6765 acc_train: 0.5307 loss_val: 0.7061 acc_val: 0.8200 time: 0.5090s\n",
            "Epoch: 0140 loss_train: 1.5445 acc_train: 0.5827 loss_val: 0.7042 acc_val: 0.8200 time: 0.4843s\n",
            "Epoch: 0141 loss_train: 1.5540 acc_train: 0.5827 loss_val: 0.7019 acc_val: 0.8200 time: 0.4289s\n",
            "Epoch: 0142 loss_train: 1.4748 acc_train: 0.6053 loss_val: 0.7002 acc_val: 0.8200 time: 0.4267s\n",
            "Epoch: 0143 loss_train: 1.5228 acc_train: 0.5987 loss_val: 0.6983 acc_val: 0.8200 time: 0.4248s\n",
            "Epoch: 0144 loss_train: 1.5752 acc_train: 0.5760 loss_val: 0.6966 acc_val: 0.8200 time: 0.4201s\n",
            "Epoch: 0145 loss_train: 1.6389 acc_train: 0.5533 loss_val: 0.6952 acc_val: 0.8280 time: 0.4246s\n",
            "Epoch: 0146 loss_train: 1.5041 acc_train: 0.6027 loss_val: 0.6939 acc_val: 0.8280 time: 0.4268s\n",
            "Epoch: 0147 loss_train: 1.6071 acc_train: 0.5587 loss_val: 0.6932 acc_val: 0.8280 time: 0.4293s\n",
            "Epoch: 0148 loss_train: 1.5700 acc_train: 0.5853 loss_val: 0.6930 acc_val: 0.8240 time: 0.4331s\n",
            "Epoch: 0149 loss_train: 1.5899 acc_train: 0.5733 loss_val: 0.6926 acc_val: 0.8240 time: 0.4340s\n",
            "Epoch: 0150 loss_train: 1.5779 acc_train: 0.5707 loss_val: 0.6924 acc_val: 0.8280 time: 0.4310s\n",
            "Epoch: 0151 loss_train: 1.6058 acc_train: 0.5693 loss_val: 0.6918 acc_val: 0.8280 time: 0.4302s\n",
            "Epoch: 0152 loss_train: 1.6044 acc_train: 0.5733 loss_val: 0.6908 acc_val: 0.8280 time: 0.4278s\n",
            "Epoch: 0153 loss_train: 1.5717 acc_train: 0.5840 loss_val: 0.6893 acc_val: 0.8280 time: 0.4323s\n",
            "Epoch: 0154 loss_train: 1.5368 acc_train: 0.5907 loss_val: 0.6874 acc_val: 0.8280 time: 0.4219s\n",
            "Epoch: 0155 loss_train: 1.5161 acc_train: 0.5880 loss_val: 0.6855 acc_val: 0.8280 time: 0.4339s\n",
            "Epoch: 0156 loss_train: 1.6045 acc_train: 0.5653 loss_val: 0.6837 acc_val: 0.8280 time: 0.4295s\n",
            "Epoch: 0157 loss_train: 1.5832 acc_train: 0.5787 loss_val: 0.6817 acc_val: 0.8240 time: 0.4358s\n",
            "Epoch: 0158 loss_train: 1.6460 acc_train: 0.5560 loss_val: 0.6798 acc_val: 0.8200 time: 0.4375s\n",
            "Epoch: 0159 loss_train: 1.4722 acc_train: 0.6013 loss_val: 0.6776 acc_val: 0.8200 time: 0.4294s\n",
            "Epoch: 0160 loss_train: 1.4928 acc_train: 0.5840 loss_val: 0.6755 acc_val: 0.8200 time: 0.4352s\n",
            "Epoch: 0161 loss_train: 1.5112 acc_train: 0.6027 loss_val: 0.6736 acc_val: 0.8200 time: 0.4433s\n",
            "Epoch: 0162 loss_train: 1.5902 acc_train: 0.5600 loss_val: 0.6716 acc_val: 0.8200 time: 0.4338s\n",
            "Epoch: 0163 loss_train: 1.4994 acc_train: 0.5920 loss_val: 0.6696 acc_val: 0.8200 time: 0.4304s\n",
            "Epoch: 0164 loss_train: 1.6336 acc_train: 0.5587 loss_val: 0.6678 acc_val: 0.8200 time: 0.4359s\n",
            "Epoch: 0165 loss_train: 1.5418 acc_train: 0.5933 loss_val: 0.6660 acc_val: 0.8200 time: 0.4219s\n",
            "Epoch: 0166 loss_train: 1.5182 acc_train: 0.5880 loss_val: 0.6642 acc_val: 0.8240 time: 0.4262s\n",
            "Epoch: 0167 loss_train: 1.5463 acc_train: 0.5947 loss_val: 0.6623 acc_val: 0.8240 time: 0.4316s\n",
            "Epoch: 0168 loss_train: 1.6478 acc_train: 0.5547 loss_val: 0.6600 acc_val: 0.8240 time: 0.4317s\n",
            "Epoch: 0169 loss_train: 1.6293 acc_train: 0.5627 loss_val: 0.6580 acc_val: 0.8200 time: 0.4336s\n",
            "Epoch: 0170 loss_train: 1.4661 acc_train: 0.6200 loss_val: 0.6558 acc_val: 0.8200 time: 0.4211s\n",
            "Epoch: 0171 loss_train: 1.5379 acc_train: 0.5947 loss_val: 0.6535 acc_val: 0.8200 time: 0.4317s\n",
            "Epoch: 0172 loss_train: 1.6483 acc_train: 0.5413 loss_val: 0.6517 acc_val: 0.8280 time: 0.4328s\n",
            "Epoch: 0173 loss_train: 1.5756 acc_train: 0.5733 loss_val: 0.6509 acc_val: 0.8280 time: 0.4222s\n",
            "Epoch: 0174 loss_train: 1.5483 acc_train: 0.5773 loss_val: 0.6510 acc_val: 0.8320 time: 0.4269s\n",
            "Epoch: 0175 loss_train: 1.5984 acc_train: 0.5547 loss_val: 0.6512 acc_val: 0.8400 time: 0.4233s\n",
            "Epoch: 0176 loss_train: 1.6256 acc_train: 0.5573 loss_val: 0.6508 acc_val: 0.8360 time: 0.4294s\n",
            "Epoch: 0177 loss_train: 1.5963 acc_train: 0.5760 loss_val: 0.6499 acc_val: 0.8360 time: 0.4325s\n",
            "Epoch: 0178 loss_train: 1.4882 acc_train: 0.6067 loss_val: 0.6485 acc_val: 0.8360 time: 0.4332s\n",
            "Epoch: 0179 loss_train: 1.5518 acc_train: 0.5747 loss_val: 0.6476 acc_val: 0.8360 time: 0.4313s\n",
            "Epoch: 0180 loss_train: 1.5136 acc_train: 0.5880 loss_val: 0.6460 acc_val: 0.8320 time: 0.4257s\n",
            "Epoch: 0181 loss_train: 1.5541 acc_train: 0.5773 loss_val: 0.6445 acc_val: 0.8280 time: 0.4322s\n",
            "Epoch: 0182 loss_train: 1.5476 acc_train: 0.5880 loss_val: 0.6433 acc_val: 0.8280 time: 0.5034s\n",
            "Epoch: 0183 loss_train: 1.5309 acc_train: 0.5707 loss_val: 0.6425 acc_val: 0.8280 time: 0.5015s\n",
            "Epoch: 0184 loss_train: 1.5233 acc_train: 0.5867 loss_val: 0.6416 acc_val: 0.8320 time: 0.4345s\n",
            "Epoch: 0185 loss_train: 1.5759 acc_train: 0.5827 loss_val: 0.6404 acc_val: 0.8320 time: 0.4335s\n",
            "Epoch: 0186 loss_train: 1.4242 acc_train: 0.6253 loss_val: 0.6392 acc_val: 0.8280 time: 0.4221s\n",
            "Epoch: 0187 loss_train: 1.5349 acc_train: 0.6000 loss_val: 0.6384 acc_val: 0.8240 time: 0.4337s\n",
            "Epoch: 0188 loss_train: 1.5593 acc_train: 0.5747 loss_val: 0.6383 acc_val: 0.8240 time: 0.4341s\n",
            "Epoch: 0189 loss_train: 1.5136 acc_train: 0.5960 loss_val: 0.6394 acc_val: 0.8280 time: 0.4278s\n",
            "Epoch: 0190 loss_train: 1.4966 acc_train: 0.6213 loss_val: 0.6405 acc_val: 0.8360 time: 0.4329s\n",
            "Epoch: 0191 loss_train: 1.5787 acc_train: 0.5760 loss_val: 0.6412 acc_val: 0.8400 time: 0.4358s\n",
            "Epoch: 0192 loss_train: 1.5274 acc_train: 0.6027 loss_val: 0.6412 acc_val: 0.8400 time: 0.4326s\n",
            "Epoch: 0193 loss_train: 1.5315 acc_train: 0.5867 loss_val: 0.6415 acc_val: 0.8360 time: 0.4876s\n",
            "Epoch: 0194 loss_train: 1.5623 acc_train: 0.5827 loss_val: 0.6416 acc_val: 0.8400 time: 0.5003s\n",
            "Epoch: 0195 loss_train: 1.4701 acc_train: 0.6000 loss_val: 0.6413 acc_val: 0.8400 time: 0.4267s\n",
            "Epoch: 0196 loss_train: 1.5079 acc_train: 0.5840 loss_val: 0.6402 acc_val: 0.8360 time: 0.4346s\n",
            "Epoch: 0197 loss_train: 1.5452 acc_train: 0.5760 loss_val: 0.6384 acc_val: 0.8360 time: 0.4333s\n",
            "Epoch: 0198 loss_train: 1.5022 acc_train: 0.5813 loss_val: 0.6364 acc_val: 0.8360 time: 0.4441s\n",
            "Epoch: 0199 loss_train: 1.6191 acc_train: 0.5640 loss_val: 0.6345 acc_val: 0.8360 time: 0.4347s\n",
            "Epoch: 0200 loss_train: 1.4923 acc_train: 0.5947 loss_val: 0.6327 acc_val: 0.8360 time: 0.4358s\n",
            "Epoch: 0201 loss_train: 1.5113 acc_train: 0.5880 loss_val: 0.6311 acc_val: 0.8400 time: 0.4343s\n",
            "Epoch: 0202 loss_train: 1.5211 acc_train: 0.5920 loss_val: 0.6298 acc_val: 0.8440 time: 0.5063s\n",
            "Epoch: 0203 loss_train: 1.5075 acc_train: 0.5893 loss_val: 0.6292 acc_val: 0.8440 time: 0.4954s\n",
            "Epoch: 0204 loss_train: 1.5628 acc_train: 0.5707 loss_val: 0.6284 acc_val: 0.8400 time: 0.4309s\n",
            "Epoch: 0205 loss_train: 1.5513 acc_train: 0.5693 loss_val: 0.6268 acc_val: 0.8360 time: 0.4299s\n",
            "Epoch: 0206 loss_train: 1.5260 acc_train: 0.5627 loss_val: 0.6255 acc_val: 0.8360 time: 0.4293s\n",
            "Epoch: 0207 loss_train: 1.5706 acc_train: 0.5600 loss_val: 0.6248 acc_val: 0.8360 time: 0.4313s\n",
            "Epoch: 0208 loss_train: 1.4668 acc_train: 0.6000 loss_val: 0.6239 acc_val: 0.8360 time: 0.4284s\n",
            "Epoch: 0209 loss_train: 1.3970 acc_train: 0.6147 loss_val: 0.6234 acc_val: 0.8400 time: 0.4335s\n",
            "Epoch: 0210 loss_train: 1.5410 acc_train: 0.5907 loss_val: 0.6236 acc_val: 0.8400 time: 0.4312s\n",
            "Epoch: 0211 loss_train: 1.4746 acc_train: 0.5907 loss_val: 0.6240 acc_val: 0.8360 time: 0.4830s\n",
            "Epoch: 0212 loss_train: 1.5002 acc_train: 0.5933 loss_val: 0.6251 acc_val: 0.8400 time: 0.5089s\n",
            "Epoch: 0213 loss_train: 1.5763 acc_train: 0.5693 loss_val: 0.6257 acc_val: 0.8400 time: 0.4965s\n",
            "Epoch: 0214 loss_train: 1.5630 acc_train: 0.5840 loss_val: 0.6258 acc_val: 0.8360 time: 0.4767s\n",
            "Epoch: 0215 loss_train: 1.5259 acc_train: 0.5760 loss_val: 0.6252 acc_val: 0.8360 time: 0.4968s\n",
            "Epoch: 0216 loss_train: 1.5950 acc_train: 0.5707 loss_val: 0.6244 acc_val: 0.8360 time: 0.5076s\n",
            "Epoch: 0217 loss_train: 1.5723 acc_train: 0.5707 loss_val: 0.6238 acc_val: 0.8360 time: 0.4267s\n",
            "Epoch: 0218 loss_train: 1.5659 acc_train: 0.5680 loss_val: 0.6223 acc_val: 0.8280 time: 0.4275s\n",
            "Epoch: 0219 loss_train: 1.4723 acc_train: 0.5947 loss_val: 0.6204 acc_val: 0.8320 time: 0.4100s\n",
            "Epoch: 0220 loss_train: 1.5223 acc_train: 0.5800 loss_val: 0.6184 acc_val: 0.8280 time: 0.5177s\n",
            "Epoch: 0221 loss_train: 1.4946 acc_train: 0.5933 loss_val: 0.6175 acc_val: 0.8280 time: 0.4184s\n",
            "Epoch: 0222 loss_train: 1.5069 acc_train: 0.6013 loss_val: 0.6165 acc_val: 0.8400 time: 0.4306s\n",
            "Epoch: 0223 loss_train: 1.5992 acc_train: 0.5640 loss_val: 0.6157 acc_val: 0.8440 time: 0.4279s\n",
            "Epoch: 0224 loss_train: 1.5505 acc_train: 0.5933 loss_val: 0.6142 acc_val: 0.8440 time: 0.4270s\n",
            "Epoch: 0225 loss_train: 1.4227 acc_train: 0.6040 loss_val: 0.6124 acc_val: 0.8440 time: 0.4167s\n",
            "Epoch: 0226 loss_train: 1.4017 acc_train: 0.6280 loss_val: 0.6101 acc_val: 0.8480 time: 0.4223s\n",
            "Epoch: 0227 loss_train: 1.5286 acc_train: 0.5787 loss_val: 0.6071 acc_val: 0.8480 time: 0.4254s\n",
            "Epoch: 0228 loss_train: 1.4676 acc_train: 0.6080 loss_val: 0.6044 acc_val: 0.8480 time: 0.4206s\n",
            "Epoch: 0229 loss_train: 1.5186 acc_train: 0.5880 loss_val: 0.6027 acc_val: 0.8560 time: 0.4267s\n",
            "Epoch: 0230 loss_train: 1.5656 acc_train: 0.5680 loss_val: 0.6022 acc_val: 0.8560 time: 0.4461s\n",
            "Epoch: 0231 loss_train: 1.6156 acc_train: 0.5560 loss_val: 0.6024 acc_val: 0.8560 time: 0.5124s\n",
            "Epoch: 0232 loss_train: 1.4961 acc_train: 0.5933 loss_val: 0.6013 acc_val: 0.8560 time: 0.4421s\n",
            "Epoch: 0233 loss_train: 1.5325 acc_train: 0.5840 loss_val: 0.5996 acc_val: 0.8560 time: 0.4321s\n",
            "Epoch: 0234 loss_train: 1.4676 acc_train: 0.5987 loss_val: 0.5975 acc_val: 0.8440 time: 0.4223s\n",
            "Epoch: 0235 loss_train: 1.3510 acc_train: 0.6400 loss_val: 0.5959 acc_val: 0.8400 time: 0.4319s\n",
            "Epoch: 0236 loss_train: 1.4651 acc_train: 0.6120 loss_val: 0.5960 acc_val: 0.8400 time: 0.4253s\n",
            "Epoch: 0237 loss_train: 1.4709 acc_train: 0.6000 loss_val: 0.5965 acc_val: 0.8320 time: 0.4228s\n",
            "Epoch: 0238 loss_train: 1.5359 acc_train: 0.5787 loss_val: 0.5976 acc_val: 0.8360 time: 0.4281s\n",
            "Epoch: 0239 loss_train: 1.4527 acc_train: 0.5973 loss_val: 0.5985 acc_val: 0.8360 time: 0.4170s\n",
            "Epoch: 0240 loss_train: 1.4233 acc_train: 0.6067 loss_val: 0.5989 acc_val: 0.8360 time: 0.4996s\n",
            "Epoch: 0241 loss_train: 1.5179 acc_train: 0.5813 loss_val: 0.5986 acc_val: 0.8360 time: 0.4674s\n",
            "Epoch: 0242 loss_train: 1.4358 acc_train: 0.6160 loss_val: 0.5969 acc_val: 0.8360 time: 0.4296s\n",
            "Epoch: 0243 loss_train: 1.5876 acc_train: 0.5520 loss_val: 0.5948 acc_val: 0.8400 time: 0.4253s\n",
            "Epoch: 0244 loss_train: 1.4541 acc_train: 0.6027 loss_val: 0.5916 acc_val: 0.8400 time: 0.4541s\n",
            "Epoch: 0245 loss_train: 1.4474 acc_train: 0.6147 loss_val: 0.5886 acc_val: 0.8400 time: 0.4295s\n",
            "Epoch: 0246 loss_train: 1.4065 acc_train: 0.6280 loss_val: 0.5863 acc_val: 0.8440 time: 0.4245s\n",
            "Epoch: 0247 loss_train: 1.5497 acc_train: 0.5747 loss_val: 0.5851 acc_val: 0.8560 time: 0.4312s\n",
            "Epoch: 0248 loss_train: 1.4596 acc_train: 0.5933 loss_val: 0.5840 acc_val: 0.8560 time: 0.4315s\n",
            "Epoch: 0249 loss_train: 1.5538 acc_train: 0.5787 loss_val: 0.5838 acc_val: 0.8560 time: 0.4242s\n",
            "Epoch: 0250 loss_train: 1.5043 acc_train: 0.6053 loss_val: 0.5835 acc_val: 0.8520 time: 0.4196s\n",
            "Epoch: 0251 loss_train: 1.5173 acc_train: 0.5987 loss_val: 0.5829 acc_val: 0.8520 time: 0.4316s\n",
            "Epoch: 0252 loss_train: 1.5309 acc_train: 0.5693 loss_val: 0.5829 acc_val: 0.8440 time: 0.4343s\n",
            "Epoch: 0253 loss_train: 1.4454 acc_train: 0.5947 loss_val: 0.5833 acc_val: 0.8440 time: 0.4339s\n",
            "Epoch: 0254 loss_train: 1.5410 acc_train: 0.5893 loss_val: 0.5841 acc_val: 0.8400 time: 0.4305s\n",
            "Epoch: 0255 loss_train: 1.4427 acc_train: 0.6080 loss_val: 0.5854 acc_val: 0.8360 time: 0.4345s\n",
            "Epoch: 0256 loss_train: 1.5703 acc_train: 0.5707 loss_val: 0.5866 acc_val: 0.8400 time: 0.4310s\n",
            "Epoch: 0257 loss_train: 1.4523 acc_train: 0.6027 loss_val: 0.5870 acc_val: 0.8360 time: 0.4272s\n",
            "Epoch: 0258 loss_train: 1.4823 acc_train: 0.5973 loss_val: 0.5876 acc_val: 0.8360 time: 0.5095s\n",
            "Epoch: 0259 loss_train: 1.5499 acc_train: 0.5653 loss_val: 0.5884 acc_val: 0.8360 time: 0.4856s\n",
            "Epoch: 0260 loss_train: 1.4151 acc_train: 0.6173 loss_val: 0.5888 acc_val: 0.8360 time: 0.5024s\n",
            "Epoch: 0261 loss_train: 1.5684 acc_train: 0.5707 loss_val: 0.5885 acc_val: 0.8360 time: 0.5249s\n",
            "Epoch: 0262 loss_train: 1.5565 acc_train: 0.5707 loss_val: 0.5887 acc_val: 0.8400 time: 0.5154s\n",
            "Epoch: 0263 loss_train: 1.4731 acc_train: 0.5987 loss_val: 0.5892 acc_val: 0.8400 time: 0.4264s\n",
            "Epoch: 0264 loss_train: 1.4830 acc_train: 0.5987 loss_val: 0.5895 acc_val: 0.8400 time: 0.4301s\n",
            "Epoch: 0265 loss_train: 1.5854 acc_train: 0.5613 loss_val: 0.5888 acc_val: 0.8440 time: 0.4202s\n",
            "Epoch: 0266 loss_train: 1.4051 acc_train: 0.6067 loss_val: 0.5883 acc_val: 0.8560 time: 0.4297s\n",
            "Epoch: 0267 loss_train: 1.4187 acc_train: 0.6147 loss_val: 0.5887 acc_val: 0.8560 time: 0.4256s\n",
            "Epoch: 0268 loss_train: 1.3829 acc_train: 0.6107 loss_val: 0.5892 acc_val: 0.8480 time: 0.4282s\n",
            "Epoch: 0269 loss_train: 1.5326 acc_train: 0.5840 loss_val: 0.5900 acc_val: 0.8480 time: 0.4177s\n",
            "Epoch: 0270 loss_train: 1.5368 acc_train: 0.5787 loss_val: 0.5912 acc_val: 0.8520 time: 0.4261s\n",
            "Epoch: 0271 loss_train: 1.4324 acc_train: 0.6093 loss_val: 0.5920 acc_val: 0.8520 time: 0.5131s\n",
            "Epoch: 0272 loss_train: 1.4511 acc_train: 0.6213 loss_val: 0.5915 acc_val: 0.8520 time: 0.4730s\n",
            "Epoch: 0273 loss_train: 1.5425 acc_train: 0.5827 loss_val: 0.5900 acc_val: 0.8520 time: 0.4301s\n",
            "Epoch: 0274 loss_train: 1.5710 acc_train: 0.5773 loss_val: 0.5881 acc_val: 0.8520 time: 0.4239s\n",
            "Epoch: 0275 loss_train: 1.6197 acc_train: 0.5640 loss_val: 0.5870 acc_val: 0.8520 time: 0.5128s\n",
            "Epoch: 0276 loss_train: 1.5136 acc_train: 0.5960 loss_val: 0.5862 acc_val: 0.8440 time: 0.5090s\n",
            "Epoch: 0277 loss_train: 1.4248 acc_train: 0.6133 loss_val: 0.5855 acc_val: 0.8400 time: 0.4242s\n",
            "Epoch: 0278 loss_train: 1.5210 acc_train: 0.5827 loss_val: 0.5846 acc_val: 0.8360 time: 0.4287s\n",
            "Epoch: 0279 loss_train: 1.4611 acc_train: 0.6067 loss_val: 0.5839 acc_val: 0.8280 time: 0.4287s\n",
            "Epoch: 0280 loss_train: 1.4787 acc_train: 0.6027 loss_val: 0.5829 acc_val: 0.8280 time: 0.4300s\n",
            "Epoch: 0281 loss_train: 1.5389 acc_train: 0.5747 loss_val: 0.5815 acc_val: 0.8320 time: 0.4244s\n",
            "Epoch: 0282 loss_train: 1.4746 acc_train: 0.6027 loss_val: 0.5799 acc_val: 0.8280 time: 0.4251s\n",
            "Epoch: 0283 loss_train: 1.4338 acc_train: 0.6067 loss_val: 0.5785 acc_val: 0.8360 time: 0.4246s\n",
            "Epoch: 0284 loss_train: 1.5447 acc_train: 0.5893 loss_val: 0.5774 acc_val: 0.8360 time: 0.4291s\n",
            "Epoch: 0285 loss_train: 1.4500 acc_train: 0.6133 loss_val: 0.5754 acc_val: 0.8400 time: 0.4273s\n",
            "Epoch: 0286 loss_train: 1.4889 acc_train: 0.5840 loss_val: 0.5746 acc_val: 0.8480 time: 0.4218s\n",
            "Epoch: 0287 loss_train: 1.4163 acc_train: 0.6080 loss_val: 0.5735 acc_val: 0.8480 time: 0.4279s\n",
            "Epoch: 0288 loss_train: 1.5124 acc_train: 0.5973 loss_val: 0.5718 acc_val: 0.8480 time: 0.4537s\n",
            "Epoch: 0289 loss_train: 1.4933 acc_train: 0.5840 loss_val: 0.5696 acc_val: 0.8480 time: 0.5043s\n",
            "Epoch: 0290 loss_train: 1.4310 acc_train: 0.6187 loss_val: 0.5673 acc_val: 0.8360 time: 0.4939s\n",
            "Epoch: 0291 loss_train: 1.4138 acc_train: 0.6227 loss_val: 0.5655 acc_val: 0.8400 time: 0.5221s\n",
            "Epoch: 0292 loss_train: 1.4872 acc_train: 0.5747 loss_val: 0.5642 acc_val: 0.8400 time: 0.5192s\n",
            "Epoch: 0293 loss_train: 1.4430 acc_train: 0.6000 loss_val: 0.5634 acc_val: 0.8400 time: 0.5085s\n",
            "Epoch: 0294 loss_train: 1.3933 acc_train: 0.6253 loss_val: 0.5643 acc_val: 0.8400 time: 0.4210s\n",
            "Epoch: 0295 loss_train: 1.4671 acc_train: 0.5920 loss_val: 0.5644 acc_val: 0.8560 time: 0.4241s\n",
            "Epoch: 0296 loss_train: 1.3543 acc_train: 0.6200 loss_val: 0.5641 acc_val: 0.8560 time: 0.4211s\n",
            "Epoch: 0297 loss_train: 1.5545 acc_train: 0.5787 loss_val: 0.5638 acc_val: 0.8560 time: 0.4276s\n",
            "Epoch: 0298 loss_train: 1.4768 acc_train: 0.5933 loss_val: 0.5640 acc_val: 0.8560 time: 0.4268s\n",
            "Epoch: 0299 loss_train: 1.5433 acc_train: 0.5813 loss_val: 0.5634 acc_val: 0.8560 time: 0.5190s\n",
            "Epoch: 0300 loss_train: 1.4396 acc_train: 0.6000 loss_val: 0.5635 acc_val: 0.8560 time: 0.5259s\n",
            "Epoch: 0301 loss_train: 1.4740 acc_train: 0.6013 loss_val: 0.5640 acc_val: 0.8600 time: 0.5144s\n",
            "Epoch: 0302 loss_train: 1.4584 acc_train: 0.5987 loss_val: 0.5639 acc_val: 0.8600 time: 0.5050s\n",
            "Epoch: 0303 loss_train: 1.4080 acc_train: 0.6267 loss_val: 0.5638 acc_val: 0.8640 time: 0.4258s\n",
            "Epoch: 0304 loss_train: 1.5760 acc_train: 0.5800 loss_val: 0.5628 acc_val: 0.8640 time: 0.4210s\n",
            "Epoch: 0305 loss_train: 1.3648 acc_train: 0.6413 loss_val: 0.5632 acc_val: 0.8600 time: 0.4258s\n",
            "Epoch: 0306 loss_train: 1.5491 acc_train: 0.5720 loss_val: 0.5631 acc_val: 0.8600 time: 0.4303s\n",
            "Epoch: 0307 loss_train: 1.4746 acc_train: 0.5987 loss_val: 0.5636 acc_val: 0.8600 time: 0.4234s\n",
            "Epoch: 0308 loss_train: 1.5444 acc_train: 0.5707 loss_val: 0.5636 acc_val: 0.8560 time: 0.4325s\n",
            "Epoch: 0309 loss_train: 1.4197 acc_train: 0.6133 loss_val: 0.5640 acc_val: 0.8560 time: 0.4420s\n",
            "Epoch: 0310 loss_train: 1.4349 acc_train: 0.6027 loss_val: 0.5654 acc_val: 0.8440 time: 0.4168s\n",
            "Epoch: 0311 loss_train: 1.4343 acc_train: 0.6200 loss_val: 0.5657 acc_val: 0.8400 time: 0.4294s\n",
            "Epoch: 0312 loss_train: 1.4516 acc_train: 0.5907 loss_val: 0.5656 acc_val: 0.8400 time: 0.5052s\n",
            "Epoch: 0313 loss_train: 1.4356 acc_train: 0.6107 loss_val: 0.5650 acc_val: 0.8440 time: 0.5030s\n",
            "Epoch: 0314 loss_train: 1.4802 acc_train: 0.6000 loss_val: 0.5637 acc_val: 0.8440 time: 0.5084s\n",
            "Epoch: 0315 loss_train: 1.5001 acc_train: 0.6040 loss_val: 0.5625 acc_val: 0.8480 time: 0.5067s\n",
            "Epoch: 0316 loss_train: 1.5384 acc_train: 0.5787 loss_val: 0.5603 acc_val: 0.8640 time: 0.4303s\n",
            "Epoch: 0317 loss_train: 1.5505 acc_train: 0.5760 loss_val: 0.5581 acc_val: 0.8600 time: 0.4335s\n",
            "Epoch: 0318 loss_train: 1.3708 acc_train: 0.6320 loss_val: 0.5561 acc_val: 0.8600 time: 0.4300s\n",
            "Epoch: 0319 loss_train: 1.3280 acc_train: 0.6453 loss_val: 0.5547 acc_val: 0.8640 time: 0.4266s\n",
            "Epoch: 0320 loss_train: 1.4873 acc_train: 0.5947 loss_val: 0.5534 acc_val: 0.8640 time: 0.4157s\n",
            "Epoch: 0321 loss_train: 1.4960 acc_train: 0.5907 loss_val: 0.5519 acc_val: 0.8640 time: 0.5120s\n",
            "Epoch: 0322 loss_train: 1.4932 acc_train: 0.6053 loss_val: 0.5495 acc_val: 0.8720 time: 0.4938s\n",
            "Epoch: 0323 loss_train: 1.3826 acc_train: 0.6213 loss_val: 0.5466 acc_val: 0.8640 time: 0.4262s\n",
            "Epoch: 0324 loss_train: 1.5194 acc_train: 0.5840 loss_val: 0.5446 acc_val: 0.8640 time: 0.4398s\n",
            "Epoch: 0325 loss_train: 1.5223 acc_train: 0.5747 loss_val: 0.5433 acc_val: 0.8680 time: 0.4239s\n",
            "Epoch: 0326 loss_train: 1.4954 acc_train: 0.5933 loss_val: 0.5423 acc_val: 0.8640 time: 0.4263s\n",
            "Epoch: 0327 loss_train: 1.4248 acc_train: 0.6120 loss_val: 0.5418 acc_val: 0.8640 time: 0.4782s\n",
            "Epoch: 0328 loss_train: 1.5082 acc_train: 0.5853 loss_val: 0.5415 acc_val: 0.8640 time: 0.5014s\n",
            "Epoch: 0329 loss_train: 1.4540 acc_train: 0.5987 loss_val: 0.5415 acc_val: 0.8680 time: 0.4287s\n",
            "Epoch: 0330 loss_train: 1.4845 acc_train: 0.5987 loss_val: 0.5419 acc_val: 0.8680 time: 0.4308s\n",
            "Epoch: 0331 loss_train: 1.5629 acc_train: 0.5507 loss_val: 0.5431 acc_val: 0.8640 time: 0.4289s\n",
            "Epoch: 0332 loss_train: 1.4334 acc_train: 0.6080 loss_val: 0.5444 acc_val: 0.8600 time: 0.4272s\n",
            "Epoch: 0333 loss_train: 1.5165 acc_train: 0.5920 loss_val: 0.5467 acc_val: 0.8560 time: 0.4263s\n",
            "Epoch: 0334 loss_train: 1.4969 acc_train: 0.6067 loss_val: 0.5490 acc_val: 0.8560 time: 0.4323s\n",
            "Epoch: 0335 loss_train: 1.4564 acc_train: 0.6040 loss_val: 0.5509 acc_val: 0.8560 time: 0.4306s\n",
            "Epoch: 0336 loss_train: 1.5148 acc_train: 0.5920 loss_val: 0.5528 acc_val: 0.8520 time: 0.4834s\n",
            "Epoch: 0337 loss_train: 1.4714 acc_train: 0.5893 loss_val: 0.5545 acc_val: 0.8520 time: 0.4759s\n",
            "Epoch: 0338 loss_train: 1.4274 acc_train: 0.6093 loss_val: 0.5556 acc_val: 0.8560 time: 0.4125s\n",
            "Epoch: 0339 loss_train: 1.5183 acc_train: 0.5920 loss_val: 0.5556 acc_val: 0.8520 time: 0.4220s\n",
            "Epoch: 0340 loss_train: 1.4963 acc_train: 0.5987 loss_val: 0.5555 acc_val: 0.8480 time: 0.4399s\n",
            "Epoch: 0341 loss_train: 1.4932 acc_train: 0.5867 loss_val: 0.5548 acc_val: 0.8480 time: 0.4219s\n",
            "Epoch: 0342 loss_train: 1.4920 acc_train: 0.5880 loss_val: 0.5525 acc_val: 0.8480 time: 0.4297s\n",
            "Epoch: 0343 loss_train: 1.4957 acc_train: 0.6053 loss_val: 0.5511 acc_val: 0.8400 time: 0.4317s\n",
            "Epoch: 0344 loss_train: 1.4482 acc_train: 0.6040 loss_val: 0.5500 acc_val: 0.8480 time: 0.4307s\n",
            "Epoch: 0345 loss_train: 1.3734 acc_train: 0.6253 loss_val: 0.5483 acc_val: 0.8480 time: 0.4859s\n",
            "Epoch: 0346 loss_train: 1.4558 acc_train: 0.6093 loss_val: 0.5475 acc_val: 0.8440 time: 0.4992s\n",
            "Epoch: 0347 loss_train: 1.4546 acc_train: 0.6053 loss_val: 0.5475 acc_val: 0.8480 time: 0.4716s\n",
            "Epoch: 0348 loss_train: 1.4584 acc_train: 0.6093 loss_val: 0.5473 acc_val: 0.8440 time: 0.4899s\n",
            "Epoch: 0349 loss_train: 1.4540 acc_train: 0.5880 loss_val: 0.5462 acc_val: 0.8440 time: 0.4304s\n",
            "Epoch: 0350 loss_train: 1.4979 acc_train: 0.6040 loss_val: 0.5454 acc_val: 0.8480 time: 0.4256s\n",
            "Epoch: 0351 loss_train: 1.4798 acc_train: 0.6000 loss_val: 0.5435 acc_val: 0.8560 time: 0.4238s\n",
            "Epoch: 0352 loss_train: 1.4847 acc_train: 0.5973 loss_val: 0.5412 acc_val: 0.8560 time: 0.4281s\n",
            "Epoch: 0353 loss_train: 1.4119 acc_train: 0.6053 loss_val: 0.5389 acc_val: 0.8560 time: 0.4290s\n",
            "Epoch: 0354 loss_train: 1.4116 acc_train: 0.6107 loss_val: 0.5371 acc_val: 0.8600 time: 0.4304s\n",
            "Epoch: 0355 loss_train: 1.4633 acc_train: 0.6000 loss_val: 0.5363 acc_val: 0.8600 time: 0.4289s\n",
            "Epoch: 0356 loss_train: 1.4816 acc_train: 0.6107 loss_val: 0.5367 acc_val: 0.8600 time: 0.4977s\n",
            "Epoch: 0357 loss_train: 1.5373 acc_train: 0.5880 loss_val: 0.5379 acc_val: 0.8600 time: 0.5247s\n",
            "Epoch: 0358 loss_train: 1.4741 acc_train: 0.6013 loss_val: 0.5386 acc_val: 0.8640 time: 0.4322s\n",
            "Epoch: 0359 loss_train: 1.4288 acc_train: 0.6147 loss_val: 0.5382 acc_val: 0.8640 time: 0.4220s\n",
            "Epoch: 0360 loss_train: 1.5113 acc_train: 0.5827 loss_val: 0.5375 acc_val: 0.8600 time: 0.4289s\n",
            "Epoch: 0361 loss_train: 1.5410 acc_train: 0.5720 loss_val: 0.5379 acc_val: 0.8560 time: 0.4296s\n",
            "Epoch: 0362 loss_train: 1.5042 acc_train: 0.5813 loss_val: 0.5376 acc_val: 0.8560 time: 0.4242s\n",
            "Epoch: 0363 loss_train: 1.4557 acc_train: 0.6160 loss_val: 0.5373 acc_val: 0.8560 time: 0.4325s\n",
            "Epoch: 0364 loss_train: 1.5261 acc_train: 0.5867 loss_val: 0.5388 acc_val: 0.8600 time: 0.4322s\n",
            "Epoch: 0365 loss_train: 1.4481 acc_train: 0.6027 loss_val: 0.5407 acc_val: 0.8560 time: 0.4267s\n",
            "Epoch: 0366 loss_train: 1.5016 acc_train: 0.5840 loss_val: 0.5429 acc_val: 0.8520 time: 0.4232s\n",
            "Epoch: 0367 loss_train: 1.5253 acc_train: 0.5667 loss_val: 0.5444 acc_val: 0.8520 time: 0.4141s\n",
            "Epoch: 0368 loss_train: 1.4431 acc_train: 0.6173 loss_val: 0.5463 acc_val: 0.8560 time: 0.4345s\n",
            "Epoch: 0369 loss_train: 1.4493 acc_train: 0.6027 loss_val: 0.5469 acc_val: 0.8560 time: 0.4255s\n",
            "Epoch: 0370 loss_train: 1.5107 acc_train: 0.5893 loss_val: 0.5481 acc_val: 0.8560 time: 0.4265s\n",
            "Epoch: 0371 loss_train: 1.4252 acc_train: 0.6093 loss_val: 0.5490 acc_val: 0.8520 time: 0.4285s\n",
            "Epoch: 0372 loss_train: 1.5852 acc_train: 0.5507 loss_val: 0.5504 acc_val: 0.8520 time: 0.4299s\n",
            "Epoch: 0373 loss_train: 1.4023 acc_train: 0.6280 loss_val: 0.5519 acc_val: 0.8520 time: 0.4207s\n",
            "Epoch: 0374 loss_train: 1.4670 acc_train: 0.6107 loss_val: 0.5528 acc_val: 0.8520 time: 0.4342s\n",
            "Epoch: 0375 loss_train: 1.5530 acc_train: 0.5640 loss_val: 0.5530 acc_val: 0.8440 time: 0.4313s\n",
            "Epoch: 0376 loss_train: 1.4303 acc_train: 0.6120 loss_val: 0.5526 acc_val: 0.8480 time: 0.4285s\n",
            "Epoch: 0377 loss_train: 1.4385 acc_train: 0.6013 loss_val: 0.5520 acc_val: 0.8360 time: 0.4339s\n",
            "Epoch: 0378 loss_train: 1.4695 acc_train: 0.5920 loss_val: 0.5516 acc_val: 0.8400 time: 0.4307s\n",
            "Epoch: 0379 loss_train: 1.4489 acc_train: 0.6000 loss_val: 0.5513 acc_val: 0.8400 time: 0.4957s\n",
            "Epoch: 0380 loss_train: 1.3968 acc_train: 0.6067 loss_val: 0.5512 acc_val: 0.8360 time: 0.5088s\n",
            "Epoch: 0381 loss_train: 1.4688 acc_train: 0.5867 loss_val: 0.5505 acc_val: 0.8400 time: 0.4331s\n",
            "Epoch: 0382 loss_train: 1.4759 acc_train: 0.5933 loss_val: 0.5490 acc_val: 0.8480 time: 0.4238s\n",
            "Epoch: 0383 loss_train: 1.4633 acc_train: 0.5933 loss_val: 0.5478 acc_val: 0.8560 time: 0.4319s\n",
            "Epoch: 0384 loss_train: 1.4049 acc_train: 0.6120 loss_val: 0.5461 acc_val: 0.8640 time: 0.4470s\n",
            "Epoch: 0385 loss_train: 1.5427 acc_train: 0.5760 loss_val: 0.5450 acc_val: 0.8680 time: 0.4565s\n",
            "Epoch: 0386 loss_train: 1.4246 acc_train: 0.6187 loss_val: 0.5438 acc_val: 0.8680 time: 0.5190s\n",
            "Epoch: 0387 loss_train: 1.4943 acc_train: 0.5907 loss_val: 0.5421 acc_val: 0.8680 time: 0.4207s\n",
            "Epoch: 0388 loss_train: 1.3715 acc_train: 0.6227 loss_val: 0.5403 acc_val: 0.8680 time: 0.4207s\n",
            "Epoch: 0389 loss_train: 1.4560 acc_train: 0.6027 loss_val: 0.5401 acc_val: 0.8640 time: 0.4288s\n",
            "Epoch: 0390 loss_train: 1.4564 acc_train: 0.6067 loss_val: 0.5408 acc_val: 0.8600 time: 0.4989s\n",
            "Epoch: 0391 loss_train: 1.4689 acc_train: 0.5987 loss_val: 0.5432 acc_val: 0.8560 time: 0.4808s\n",
            "Epoch: 0392 loss_train: 1.4582 acc_train: 0.5960 loss_val: 0.5462 acc_val: 0.8520 time: 0.4332s\n",
            "Epoch: 0393 loss_train: 1.3723 acc_train: 0.6267 loss_val: 0.5487 acc_val: 0.8520 time: 0.4260s\n",
            "Epoch: 0394 loss_train: 1.4667 acc_train: 0.5947 loss_val: 0.5493 acc_val: 0.8520 time: 0.4831s\n",
            "Epoch: 0395 loss_train: 1.5153 acc_train: 0.5907 loss_val: 0.5492 acc_val: 0.8520 time: 0.5089s\n",
            "Epoch: 0396 loss_train: 1.4395 acc_train: 0.6080 loss_val: 0.5466 acc_val: 0.8520 time: 0.4295s\n",
            "Epoch: 0397 loss_train: 1.4078 acc_train: 0.6040 loss_val: 0.5435 acc_val: 0.8560 time: 0.4342s\n",
            "Epoch: 0398 loss_train: 1.4489 acc_train: 0.5987 loss_val: 0.5433 acc_val: 0.8640 time: 0.4294s\n",
            "Epoch: 0399 loss_train: 1.4847 acc_train: 0.5947 loss_val: 0.5421 acc_val: 0.8680 time: 0.4272s\n",
            "Epoch: 0400 loss_train: 1.5354 acc_train: 0.5813 loss_val: 0.5415 acc_val: 0.8680 time: 0.4193s\n",
            "Epoch: 0401 loss_train: 1.3769 acc_train: 0.6267 loss_val: 0.5408 acc_val: 0.8680 time: 0.5122s\n",
            "Epoch: 0402 loss_train: 1.4213 acc_train: 0.6200 loss_val: 0.5400 acc_val: 0.8640 time: 0.4348s\n",
            "Epoch: 0403 loss_train: 1.4628 acc_train: 0.6027 loss_val: 0.5401 acc_val: 0.8560 time: 0.5100s\n",
            "Epoch: 0404 loss_train: 1.3964 acc_train: 0.6267 loss_val: 0.5411 acc_val: 0.8520 time: 0.4974s\n",
            "Epoch: 0405 loss_train: 1.3430 acc_train: 0.6333 loss_val: 0.5421 acc_val: 0.8520 time: 0.4282s\n",
            "Epoch: 0406 loss_train: 1.3979 acc_train: 0.6187 loss_val: 0.5428 acc_val: 0.8520 time: 0.4188s\n",
            "Epoch: 0407 loss_train: 1.4864 acc_train: 0.5933 loss_val: 0.5427 acc_val: 0.8480 time: 0.4244s\n",
            "Epoch: 0408 loss_train: 1.4842 acc_train: 0.6040 loss_val: 0.5423 acc_val: 0.8520 time: 0.4281s\n",
            "Epoch: 0409 loss_train: 1.4415 acc_train: 0.6093 loss_val: 0.5409 acc_val: 0.8520 time: 0.4207s\n",
            "Epoch: 0410 loss_train: 1.4545 acc_train: 0.6027 loss_val: 0.5386 acc_val: 0.8560 time: 0.4212s\n",
            "Epoch: 0411 loss_train: 1.3846 acc_train: 0.6173 loss_val: 0.5363 acc_val: 0.8560 time: 0.4326s\n",
            "Epoch: 0412 loss_train: 1.5489 acc_train: 0.5667 loss_val: 0.5334 acc_val: 0.8600 time: 0.5069s\n",
            "Epoch: 0413 loss_train: 1.4611 acc_train: 0.5893 loss_val: 0.5325 acc_val: 0.8560 time: 0.4898s\n",
            "Epoch: 0414 loss_train: 1.4249 acc_train: 0.6080 loss_val: 0.5324 acc_val: 0.8600 time: 0.5034s\n",
            "Epoch: 0415 loss_train: 1.5082 acc_train: 0.5880 loss_val: 0.5337 acc_val: 0.8640 time: 0.5217s\n",
            "Epoch: 0416 loss_train: 1.4575 acc_train: 0.6027 loss_val: 0.5350 acc_val: 0.8600 time: 0.4297s\n",
            "Epoch: 0417 loss_train: 1.4536 acc_train: 0.6093 loss_val: 0.5352 acc_val: 0.8640 time: 0.4271s\n",
            "Epoch: 0418 loss_train: 1.4555 acc_train: 0.6027 loss_val: 0.5344 acc_val: 0.8600 time: 0.4288s\n",
            "Epoch: 0419 loss_train: 1.4230 acc_train: 0.6133 loss_val: 0.5338 acc_val: 0.8560 time: 0.4312s\n",
            "Epoch: 0420 loss_train: 1.4374 acc_train: 0.6053 loss_val: 0.5340 acc_val: 0.8560 time: 0.4788s\n",
            "Epoch: 0421 loss_train: 1.4432 acc_train: 0.6160 loss_val: 0.5343 acc_val: 0.8600 time: 0.5047s\n",
            "Epoch: 0422 loss_train: 1.3764 acc_train: 0.6107 loss_val: 0.5340 acc_val: 0.8560 time: 0.4241s\n",
            "Epoch: 0423 loss_train: 1.4666 acc_train: 0.5973 loss_val: 0.5334 acc_val: 0.8600 time: 0.4280s\n",
            "Epoch: 0424 loss_train: 1.4524 acc_train: 0.5973 loss_val: 0.5327 acc_val: 0.8600 time: 0.4224s\n",
            "Epoch: 0425 loss_train: 1.4488 acc_train: 0.5973 loss_val: 0.5313 acc_val: 0.8600 time: 0.4990s\n",
            "Epoch: 0426 loss_train: 1.4746 acc_train: 0.6013 loss_val: 0.5294 acc_val: 0.8600 time: 0.4622s\n",
            "Epoch: 0427 loss_train: 1.4779 acc_train: 0.5773 loss_val: 0.5284 acc_val: 0.8680 time: 0.4326s\n",
            "Epoch: 0428 loss_train: 1.4992 acc_train: 0.5827 loss_val: 0.5280 acc_val: 0.8680 time: 0.4320s\n",
            "Epoch: 0429 loss_train: 1.5402 acc_train: 0.5760 loss_val: 0.5268 acc_val: 0.8600 time: 0.4355s\n",
            "Epoch: 0430 loss_train: 1.4364 acc_train: 0.6053 loss_val: 0.5257 acc_val: 0.8600 time: 0.4295s\n",
            "Epoch: 0431 loss_train: 1.4861 acc_train: 0.5867 loss_val: 0.5269 acc_val: 0.8560 time: 0.4208s\n",
            "Epoch: 0432 loss_train: 1.5654 acc_train: 0.5747 loss_val: 0.5288 acc_val: 0.8560 time: 0.4268s\n",
            "Epoch: 0433 loss_train: 1.3902 acc_train: 0.6200 loss_val: 0.5297 acc_val: 0.8560 time: 0.4676s\n",
            "Epoch: 0434 loss_train: 1.4800 acc_train: 0.5907 loss_val: 0.5300 acc_val: 0.8520 time: 0.4332s\n",
            "Epoch: 0435 loss_train: 1.5270 acc_train: 0.5800 loss_val: 0.5286 acc_val: 0.8560 time: 0.4400s\n",
            "Epoch: 0436 loss_train: 1.4390 acc_train: 0.6120 loss_val: 0.5262 acc_val: 0.8600 time: 0.4331s\n",
            "Epoch: 0437 loss_train: 1.5456 acc_train: 0.5827 loss_val: 0.5244 acc_val: 0.8640 time: 0.4322s\n",
            "Epoch: 0438 loss_train: 1.4123 acc_train: 0.6093 loss_val: 0.5237 acc_val: 0.8640 time: 0.4173s\n",
            "Epoch: 0439 loss_train: 1.5489 acc_train: 0.5707 loss_val: 0.5236 acc_val: 0.8640 time: 0.4319s\n",
            "Epoch: 0440 loss_train: 1.4897 acc_train: 0.5920 loss_val: 0.5245 acc_val: 0.8600 time: 0.4248s\n",
            "Epoch: 0441 loss_train: 1.3821 acc_train: 0.6253 loss_val: 0.5255 acc_val: 0.8600 time: 0.4330s\n",
            "Epoch: 0442 loss_train: 1.5983 acc_train: 0.5653 loss_val: 0.5268 acc_val: 0.8560 time: 0.4309s\n",
            "Epoch: 0443 loss_train: 1.4247 acc_train: 0.6093 loss_val: 0.5258 acc_val: 0.8600 time: 0.4354s\n",
            "Epoch: 0444 loss_train: 1.4705 acc_train: 0.5960 loss_val: 0.5224 acc_val: 0.8640 time: 0.4316s\n",
            "Epoch: 0445 loss_train: 1.3898 acc_train: 0.6240 loss_val: 0.5191 acc_val: 0.8720 time: 0.4858s\n",
            "Epoch: 0446 loss_train: 1.4032 acc_train: 0.6240 loss_val: 0.5170 acc_val: 0.8720 time: 0.5005s\n",
            "Epoch: 0447 loss_train: 1.5098 acc_train: 0.5973 loss_val: 0.5172 acc_val: 0.8720 time: 0.4350s\n",
            "Epoch: 0448 loss_train: 1.3904 acc_train: 0.6120 loss_val: 0.5189 acc_val: 0.8760 time: 0.4349s\n",
            "Epoch: 0449 loss_train: 1.4626 acc_train: 0.5933 loss_val: 0.5213 acc_val: 0.8720 time: 0.4326s\n",
            "Epoch: 0450 loss_train: 1.4929 acc_train: 0.5973 loss_val: 0.5235 acc_val: 0.8720 time: 0.4334s\n",
            "Epoch: 0451 loss_train: 1.5100 acc_train: 0.5907 loss_val: 0.5237 acc_val: 0.8640 time: 0.4335s\n",
            "Epoch: 0452 loss_train: 1.4893 acc_train: 0.5827 loss_val: 0.5233 acc_val: 0.8600 time: 0.4355s\n",
            "Epoch: 0453 loss_train: 1.4674 acc_train: 0.5987 loss_val: 0.5237 acc_val: 0.8560 time: 0.4323s\n",
            "Epoch: 0454 loss_train: 1.4661 acc_train: 0.5987 loss_val: 0.5246 acc_val: 0.8480 time: 0.4179s\n",
            "Epoch: 0455 loss_train: 1.4333 acc_train: 0.5827 loss_val: 0.5263 acc_val: 0.8520 time: 0.4375s\n",
            "Epoch: 0456 loss_train: 1.3694 acc_train: 0.6293 loss_val: 0.5271 acc_val: 0.8520 time: 0.4269s\n",
            "Epoch: 0457 loss_train: 1.3982 acc_train: 0.6147 loss_val: 0.5277 acc_val: 0.8480 time: 0.4314s\n",
            "Epoch: 0458 loss_train: 1.4933 acc_train: 0.5867 loss_val: 0.5276 acc_val: 0.8560 time: 0.4238s\n",
            "Epoch: 0459 loss_train: 1.3970 acc_train: 0.6133 loss_val: 0.5267 acc_val: 0.8600 time: 0.4346s\n",
            "Epoch: 0460 loss_train: 1.4394 acc_train: 0.6013 loss_val: 0.5259 acc_val: 0.8640 time: 0.4242s\n",
            "Epoch: 0461 loss_train: 1.5371 acc_train: 0.5733 loss_val: 0.5257 acc_val: 0.8680 time: 0.4173s\n",
            "Epoch: 0462 loss_train: 1.4487 acc_train: 0.6053 loss_val: 0.5246 acc_val: 0.8680 time: 0.4252s\n",
            "Epoch: 0463 loss_train: 1.4388 acc_train: 0.6187 loss_val: 0.5238 acc_val: 0.8680 time: 0.5150s\n",
            "Epoch: 0464 loss_train: 1.4228 acc_train: 0.6240 loss_val: 0.5228 acc_val: 0.8680 time: 0.5085s\n",
            "Epoch: 0465 loss_train: 1.4811 acc_train: 0.6040 loss_val: 0.5217 acc_val: 0.8640 time: 0.4370s\n",
            "Epoch: 0466 loss_train: 1.3471 acc_train: 0.6387 loss_val: 0.5208 acc_val: 0.8640 time: 0.4360s\n",
            "Epoch: 0467 loss_train: 1.5139 acc_train: 0.5800 loss_val: 0.5203 acc_val: 0.8640 time: 0.4188s\n",
            "Epoch: 0468 loss_train: 1.4738 acc_train: 0.5960 loss_val: 0.5198 acc_val: 0.8640 time: 0.4213s\n",
            "Epoch: 0469 loss_train: 1.4737 acc_train: 0.5973 loss_val: 0.5198 acc_val: 0.8640 time: 0.4173s\n",
            "Epoch: 0470 loss_train: 1.4236 acc_train: 0.5987 loss_val: 0.5215 acc_val: 0.8600 time: 0.5158s\n",
            "Epoch: 0471 loss_train: 1.4468 acc_train: 0.5987 loss_val: 0.5225 acc_val: 0.8640 time: 0.4601s\n",
            "Epoch: 0472 loss_train: 1.5032 acc_train: 0.5787 loss_val: 0.5227 acc_val: 0.8600 time: 0.4237s\n",
            "Epoch: 0473 loss_train: 1.4374 acc_train: 0.6067 loss_val: 0.5224 acc_val: 0.8600 time: 0.4321s\n",
            "Epoch: 0474 loss_train: 1.4140 acc_train: 0.6107 loss_val: 0.5222 acc_val: 0.8600 time: 0.4290s\n",
            "Epoch: 0475 loss_train: 1.4324 acc_train: 0.5973 loss_val: 0.5218 acc_val: 0.8600 time: 0.4288s\n",
            "Epoch: 0476 loss_train: 1.4118 acc_train: 0.6253 loss_val: 0.5223 acc_val: 0.8560 time: 0.4289s\n",
            "Epoch: 0477 loss_train: 1.3978 acc_train: 0.6200 loss_val: 0.5230 acc_val: 0.8600 time: 0.4270s\n",
            "Epoch: 0478 loss_train: 1.6066 acc_train: 0.5480 loss_val: 0.5246 acc_val: 0.8560 time: 0.4386s\n",
            "Epoch: 0479 loss_train: 1.4257 acc_train: 0.6040 loss_val: 0.5268 acc_val: 0.8560 time: 0.5033s\n",
            "Epoch: 0480 loss_train: 1.5789 acc_train: 0.5747 loss_val: 0.5289 acc_val: 0.8600 time: 0.4531s\n",
            "Epoch: 0481 loss_train: 1.4567 acc_train: 0.6000 loss_val: 0.5306 acc_val: 0.8600 time: 0.4305s\n",
            "Epoch: 0482 loss_train: 1.4872 acc_train: 0.6000 loss_val: 0.5325 acc_val: 0.8560 time: 0.4246s\n",
            "Epoch: 0483 loss_train: 1.4178 acc_train: 0.6080 loss_val: 0.5332 acc_val: 0.8560 time: 0.4242s\n",
            "Epoch: 0484 loss_train: 1.4870 acc_train: 0.5867 loss_val: 0.5342 acc_val: 0.8520 time: 0.4280s\n",
            "Epoch: 0485 loss_train: 1.4828 acc_train: 0.5960 loss_val: 0.5346 acc_val: 0.8520 time: 0.4269s\n",
            "Epoch: 0486 loss_train: 1.4137 acc_train: 0.6107 loss_val: 0.5328 acc_val: 0.8560 time: 0.4228s\n",
            "Epoch: 0487 loss_train: 1.5332 acc_train: 0.5867 loss_val: 0.5306 acc_val: 0.8600 time: 0.4249s\n",
            "Epoch: 0488 loss_train: 1.4017 acc_train: 0.6160 loss_val: 0.5281 acc_val: 0.8600 time: 0.4282s\n",
            "Epoch: 0489 loss_train: 1.4576 acc_train: 0.5947 loss_val: 0.5249 acc_val: 0.8640 time: 0.4342s\n",
            "Epoch: 0490 loss_train: 1.3891 acc_train: 0.6187 loss_val: 0.5224 acc_val: 0.8640 time: 0.4284s\n",
            "Epoch: 0491 loss_train: 1.4129 acc_train: 0.6027 loss_val: 0.5202 acc_val: 0.8640 time: 0.4238s\n",
            "Epoch: 0492 loss_train: 1.4353 acc_train: 0.6080 loss_val: 0.5188 acc_val: 0.8560 time: 0.4236s\n",
            "Epoch: 0493 loss_train: 1.4746 acc_train: 0.6053 loss_val: 0.5175 acc_val: 0.8640 time: 0.4204s\n",
            "Epoch: 0494 loss_train: 1.3805 acc_train: 0.6080 loss_val: 0.5151 acc_val: 0.8680 time: 0.4259s\n",
            "Epoch: 0495 loss_train: 1.3901 acc_train: 0.6240 loss_val: 0.5135 acc_val: 0.8680 time: 0.4263s\n",
            "Epoch: 0496 loss_train: 1.4748 acc_train: 0.5987 loss_val: 0.5113 acc_val: 0.8680 time: 0.4320s\n",
            "Epoch: 0497 loss_train: 1.4543 acc_train: 0.6093 loss_val: 0.5093 acc_val: 0.8680 time: 0.5243s\n",
            "Epoch: 0498 loss_train: 1.4032 acc_train: 0.6107 loss_val: 0.5069 acc_val: 0.8680 time: 0.4858s\n",
            "Epoch: 0499 loss_train: 1.5149 acc_train: 0.5747 loss_val: 0.5050 acc_val: 0.8600 time: 0.4890s\n",
            "Epoch: 0500 loss_train: 1.5217 acc_train: 0.5787 loss_val: 0.5039 acc_val: 0.8640 time: 0.5173s\n",
            "Epoch: 0501 loss_train: 1.4347 acc_train: 0.6147 loss_val: 0.5025 acc_val: 0.8640 time: 0.4175s\n",
            "Epoch: 0502 loss_train: 1.4851 acc_train: 0.5973 loss_val: 0.5032 acc_val: 0.8640 time: 0.4287s\n",
            "Epoch: 0503 loss_train: 1.3962 acc_train: 0.6200 loss_val: 0.5042 acc_val: 0.8640 time: 0.4978s\n",
            "Epoch: 0504 loss_train: 1.4290 acc_train: 0.5973 loss_val: 0.5048 acc_val: 0.8720 time: 0.5035s\n",
            "Epoch: 0505 loss_train: 1.4447 acc_train: 0.5920 loss_val: 0.5049 acc_val: 0.8720 time: 0.5011s\n",
            "Epoch: 0506 loss_train: 1.4089 acc_train: 0.6147 loss_val: 0.5061 acc_val: 0.8720 time: 0.5032s\n",
            "Epoch: 0507 loss_train: 1.3879 acc_train: 0.6213 loss_val: 0.5085 acc_val: 0.8760 time: 0.4304s\n",
            "Epoch: 0508 loss_train: 1.4993 acc_train: 0.5920 loss_val: 0.5122 acc_val: 0.8760 time: 0.4289s\n",
            "Epoch: 0509 loss_train: 1.4978 acc_train: 0.5787 loss_val: 0.5152 acc_val: 0.8800 time: 0.4428s\n",
            "Epoch: 0510 loss_train: 1.3981 acc_train: 0.6187 loss_val: 0.5175 acc_val: 0.8640 time: 0.5149s\n",
            "Epoch: 0511 loss_train: 1.4381 acc_train: 0.6080 loss_val: 0.5208 acc_val: 0.8640 time: 0.4410s\n",
            "Epoch: 0512 loss_train: 1.4041 acc_train: 0.6187 loss_val: 0.5253 acc_val: 0.8600 time: 0.5319s\n",
            "Epoch: 0513 loss_train: 1.4744 acc_train: 0.6040 loss_val: 0.5319 acc_val: 0.8520 time: 0.4633s\n",
            "Epoch: 0514 loss_train: 1.4506 acc_train: 0.5933 loss_val: 0.5372 acc_val: 0.8520 time: 0.4245s\n",
            "Epoch: 0515 loss_train: 1.4372 acc_train: 0.5987 loss_val: 0.5388 acc_val: 0.8520 time: 0.4292s\n",
            "Epoch: 0516 loss_train: 1.4974 acc_train: 0.5947 loss_val: 0.5385 acc_val: 0.8520 time: 0.4256s\n",
            "Epoch: 0517 loss_train: 1.4628 acc_train: 0.5947 loss_val: 0.5360 acc_val: 0.8600 time: 0.4302s\n",
            "Epoch: 0518 loss_train: 1.4552 acc_train: 0.5960 loss_val: 0.5318 acc_val: 0.8600 time: 0.4295s\n",
            "Epoch: 0519 loss_train: 1.4729 acc_train: 0.5960 loss_val: 0.5260 acc_val: 0.8600 time: 0.4145s\n",
            "Epoch: 0520 loss_train: 1.4281 acc_train: 0.6040 loss_val: 0.5211 acc_val: 0.8600 time: 0.4242s\n",
            "Epoch: 0521 loss_train: 1.4023 acc_train: 0.6200 loss_val: 0.5196 acc_val: 0.8560 time: 0.4314s\n",
            "Epoch: 0522 loss_train: 1.3327 acc_train: 0.6293 loss_val: 0.5173 acc_val: 0.8600 time: 0.4453s\n",
            "Epoch: 0523 loss_train: 1.4517 acc_train: 0.5987 loss_val: 0.5140 acc_val: 0.8640 time: 0.4248s\n",
            "Epoch: 0524 loss_train: 1.4995 acc_train: 0.5907 loss_val: 0.5110 acc_val: 0.8680 time: 0.4224s\n",
            "Epoch: 0525 loss_train: 1.6116 acc_train: 0.5627 loss_val: 0.5104 acc_val: 0.8600 time: 0.4275s\n",
            "Epoch: 0526 loss_train: 1.4816 acc_train: 0.5987 loss_val: 0.5112 acc_val: 0.8600 time: 0.4181s\n",
            "Epoch: 0527 loss_train: 1.3634 acc_train: 0.6240 loss_val: 0.5128 acc_val: 0.8600 time: 0.4277s\n",
            "Epoch: 0528 loss_train: 1.4445 acc_train: 0.6000 loss_val: 0.5145 acc_val: 0.8560 time: 0.4295s\n",
            "Epoch: 0529 loss_train: 1.4091 acc_train: 0.6093 loss_val: 0.5143 acc_val: 0.8640 time: 0.4337s\n",
            "Epoch: 0530 loss_train: 1.4935 acc_train: 0.5920 loss_val: 0.5137 acc_val: 0.8640 time: 0.5059s\n",
            "Epoch: 0531 loss_train: 1.4776 acc_train: 0.5973 loss_val: 0.5134 acc_val: 0.8680 time: 0.4905s\n",
            "Epoch: 0532 loss_train: 1.4007 acc_train: 0.6133 loss_val: 0.5140 acc_val: 0.8760 time: 0.4331s\n",
            "Epoch: 0533 loss_train: 1.4397 acc_train: 0.6120 loss_val: 0.5138 acc_val: 0.8760 time: 0.4200s\n",
            "Epoch: 0534 loss_train: 1.3425 acc_train: 0.6333 loss_val: 0.5143 acc_val: 0.8680 time: 0.5100s\n",
            "Epoch: 0535 loss_train: 1.5132 acc_train: 0.5800 loss_val: 0.5143 acc_val: 0.8640 time: 0.4698s\n",
            "Epoch: 0536 loss_train: 1.5111 acc_train: 0.5840 loss_val: 0.5150 acc_val: 0.8600 time: 0.4295s\n",
            "Epoch: 0537 loss_train: 1.4190 acc_train: 0.6000 loss_val: 0.5162 acc_val: 0.8640 time: 0.4258s\n",
            "Epoch: 0538 loss_train: 1.4507 acc_train: 0.5960 loss_val: 0.5166 acc_val: 0.8640 time: 0.4279s\n",
            "Epoch: 0539 loss_train: 1.4318 acc_train: 0.6013 loss_val: 0.5168 acc_val: 0.8640 time: 0.4308s\n",
            "Epoch: 0540 loss_train: 1.4615 acc_train: 0.5893 loss_val: 0.5168 acc_val: 0.8640 time: 0.4329s\n",
            "Epoch: 0541 loss_train: 1.4015 acc_train: 0.6213 loss_val: 0.5163 acc_val: 0.8640 time: 0.5223s\n",
            "Epoch: 0542 loss_train: 1.3641 acc_train: 0.6253 loss_val: 0.5159 acc_val: 0.8680 time: 0.4607s\n",
            "Epoch: 0543 loss_train: 1.4153 acc_train: 0.5987 loss_val: 0.5140 acc_val: 0.8680 time: 0.5218s\n",
            "Epoch: 0544 loss_train: 1.5036 acc_train: 0.5773 loss_val: 0.5122 acc_val: 0.8680 time: 0.4253s\n",
            "Epoch: 0545 loss_train: 1.4521 acc_train: 0.6053 loss_val: 0.5104 acc_val: 0.8640 time: 0.4298s\n",
            "Epoch: 0546 loss_train: 1.4850 acc_train: 0.5840 loss_val: 0.5102 acc_val: 0.8640 time: 0.4180s\n",
            "Epoch: 0547 loss_train: 1.4312 acc_train: 0.6160 loss_val: 0.5100 acc_val: 0.8760 time: 0.5054s\n",
            "Epoch: 0548 loss_train: 1.4204 acc_train: 0.6187 loss_val: 0.5096 acc_val: 0.8800 time: 0.5065s\n",
            "Epoch: 0549 loss_train: 1.4858 acc_train: 0.5867 loss_val: 0.5090 acc_val: 0.8720 time: 0.4269s\n",
            "Epoch: 0550 loss_train: 1.5409 acc_train: 0.5773 loss_val: 0.5096 acc_val: 0.8680 time: 0.4325s\n",
            "Epoch: 0551 loss_train: 1.4345 acc_train: 0.5987 loss_val: 0.5100 acc_val: 0.8680 time: 0.4205s\n",
            "Epoch: 0552 loss_train: 1.4648 acc_train: 0.5960 loss_val: 0.5106 acc_val: 0.8640 time: 0.4215s\n",
            "Epoch: 0553 loss_train: 1.3494 acc_train: 0.6453 loss_val: 0.5126 acc_val: 0.8640 time: 0.4750s\n",
            "Epoch: 0554 loss_train: 1.5011 acc_train: 0.5693 loss_val: 0.5144 acc_val: 0.8600 time: 0.5240s\n",
            "Epoch: 0555 loss_train: 1.4472 acc_train: 0.6093 loss_val: 0.5163 acc_val: 0.8600 time: 0.4551s\n",
            "Epoch: 0556 loss_train: 1.4435 acc_train: 0.5960 loss_val: 0.5148 acc_val: 0.8600 time: 0.5095s\n",
            "Epoch: 0557 loss_train: 1.3868 acc_train: 0.6040 loss_val: 0.5135 acc_val: 0.8640 time: 0.4350s\n",
            "Epoch: 0558 loss_train: 1.3660 acc_train: 0.6347 loss_val: 0.5125 acc_val: 0.8640 time: 0.4321s\n",
            "Epoch: 0559 loss_train: 1.3942 acc_train: 0.6040 loss_val: 0.5125 acc_val: 0.8680 time: 0.4202s\n",
            "Epoch: 0560 loss_train: 1.4524 acc_train: 0.5933 loss_val: 0.5125 acc_val: 0.8680 time: 0.4302s\n",
            "Epoch: 0561 loss_train: 1.4773 acc_train: 0.6040 loss_val: 0.5131 acc_val: 0.8720 time: 0.4271s\n",
            "Epoch: 0562 loss_train: 1.4648 acc_train: 0.5973 loss_val: 0.5125 acc_val: 0.8720 time: 0.4211s\n",
            "Epoch: 0563 loss_train: 1.5122 acc_train: 0.5733 loss_val: 0.5115 acc_val: 0.8720 time: 0.4213s\n",
            "Epoch: 0564 loss_train: 1.4225 acc_train: 0.6133 loss_val: 0.5116 acc_val: 0.8720 time: 0.4274s\n",
            "Epoch: 0565 loss_train: 1.4176 acc_train: 0.6133 loss_val: 0.5120 acc_val: 0.8680 time: 0.4387s\n",
            "Epoch: 0566 loss_train: 1.5123 acc_train: 0.5853 loss_val: 0.5136 acc_val: 0.8640 time: 0.4464s\n",
            "Epoch: 0567 loss_train: 1.3569 acc_train: 0.6280 loss_val: 0.5158 acc_val: 0.8600 time: 0.5044s\n",
            "Epoch: 0568 loss_train: 1.4186 acc_train: 0.6173 loss_val: 0.5191 acc_val: 0.8560 time: 0.4817s\n",
            "Epoch: 0569 loss_train: 1.4012 acc_train: 0.6107 loss_val: 0.5215 acc_val: 0.8560 time: 0.5106s\n",
            "Epoch: 0570 loss_train: 1.3807 acc_train: 0.6173 loss_val: 0.5223 acc_val: 0.8560 time: 0.5021s\n",
            "Epoch: 0571 loss_train: 1.4352 acc_train: 0.5920 loss_val: 0.5217 acc_val: 0.8600 time: 0.4220s\n",
            "Epoch: 0572 loss_train: 1.4639 acc_train: 0.5947 loss_val: 0.5203 acc_val: 0.8560 time: 0.4267s\n",
            "Epoch: 0573 loss_train: 1.3983 acc_train: 0.6253 loss_val: 0.5175 acc_val: 0.8520 time: 0.4253s\n",
            "Epoch: 0574 loss_train: 1.3809 acc_train: 0.6147 loss_val: 0.5141 acc_val: 0.8560 time: 0.4299s\n",
            "Epoch: 0575 loss_train: 1.4182 acc_train: 0.6147 loss_val: 0.5112 acc_val: 0.8560 time: 0.4194s\n",
            "Epoch: 0576 loss_train: 1.4007 acc_train: 0.6147 loss_val: 0.5078 acc_val: 0.8560 time: 0.4305s\n",
            "Epoch: 0577 loss_train: 1.4056 acc_train: 0.6080 loss_val: 0.5050 acc_val: 0.8600 time: 0.4295s\n",
            "Epoch: 0578 loss_train: 1.5240 acc_train: 0.5920 loss_val: 0.5034 acc_val: 0.8560 time: 0.4286s\n",
            "Epoch: 0579 loss_train: 1.3804 acc_train: 0.6320 loss_val: 0.5027 acc_val: 0.8600 time: 0.4298s\n",
            "Epoch: 0580 loss_train: 1.3936 acc_train: 0.6120 loss_val: 0.5034 acc_val: 0.8600 time: 0.4196s\n",
            "Epoch: 0581 loss_train: 1.4064 acc_train: 0.6133 loss_val: 0.5049 acc_val: 0.8640 time: 0.4190s\n",
            "Epoch: 0582 loss_train: 1.4469 acc_train: 0.6000 loss_val: 0.5056 acc_val: 0.8640 time: 0.4408s\n",
            "Epoch: 0583 loss_train: 1.4226 acc_train: 0.6080 loss_val: 0.5062 acc_val: 0.8560 time: 0.4259s\n",
            "Epoch: 0584 loss_train: 1.5194 acc_train: 0.5933 loss_val: 0.5056 acc_val: 0.8600 time: 0.4335s\n",
            "Epoch: 0585 loss_train: 1.4136 acc_train: 0.6053 loss_val: 0.5050 acc_val: 0.8600 time: 0.5163s\n",
            "Epoch: 0586 loss_train: 1.5699 acc_train: 0.5613 loss_val: 0.5053 acc_val: 0.8600 time: 0.4586s\n",
            "Epoch: 0587 loss_train: 1.4767 acc_train: 0.5947 loss_val: 0.5053 acc_val: 0.8560 time: 0.4279s\n",
            "Epoch: 0588 loss_train: 1.4875 acc_train: 0.5907 loss_val: 0.5061 acc_val: 0.8600 time: 0.4292s\n",
            "Epoch: 0589 loss_train: 1.4440 acc_train: 0.5987 loss_val: 0.5055 acc_val: 0.8680 time: 0.4304s\n",
            "Epoch: 0590 loss_train: 1.5363 acc_train: 0.5813 loss_val: 0.5039 acc_val: 0.8680 time: 0.4299s\n",
            "Epoch: 0591 loss_train: 1.4959 acc_train: 0.5947 loss_val: 0.5044 acc_val: 0.8680 time: 0.4332s\n",
            "Epoch: 0592 loss_train: 1.4342 acc_train: 0.6093 loss_val: 0.5039 acc_val: 0.8720 time: 0.4297s\n",
            "Epoch: 0593 loss_train: 1.4199 acc_train: 0.5933 loss_val: 0.5035 acc_val: 0.8720 time: 0.4289s\n",
            "Epoch: 0594 loss_train: 1.5193 acc_train: 0.5867 loss_val: 0.5030 acc_val: 0.8720 time: 0.4137s\n",
            "Epoch: 0595 loss_train: 1.5584 acc_train: 0.5760 loss_val: 0.5025 acc_val: 0.8760 time: 0.4287s\n",
            "Epoch: 0596 loss_train: 1.2971 acc_train: 0.6373 loss_val: 0.5021 acc_val: 0.8760 time: 0.4321s\n",
            "Epoch: 0597 loss_train: 1.4095 acc_train: 0.6187 loss_val: 0.5018 acc_val: 0.8760 time: 0.4198s\n",
            "Epoch: 0598 loss_train: 1.3789 acc_train: 0.6173 loss_val: 0.5015 acc_val: 0.8760 time: 0.4270s\n",
            "Epoch: 0599 loss_train: 1.3981 acc_train: 0.6147 loss_val: 0.5019 acc_val: 0.8760 time: 0.4259s\n",
            "Epoch: 0600 loss_train: 1.4946 acc_train: 0.5880 loss_val: 0.5021 acc_val: 0.8760 time: 0.4266s\n",
            "Epoch: 0601 loss_train: 1.3844 acc_train: 0.6227 loss_val: 0.5014 acc_val: 0.8760 time: 0.4745s\n",
            "Epoch: 0602 loss_train: 1.5423 acc_train: 0.5773 loss_val: 0.5018 acc_val: 0.8680 time: 0.4674s\n",
            "Epoch: 0603 loss_train: 1.4343 acc_train: 0.6147 loss_val: 0.5017 acc_val: 0.8680 time: 0.4241s\n",
            "Epoch: 0604 loss_train: 1.4203 acc_train: 0.6013 loss_val: 0.5010 acc_val: 0.8680 time: 0.4227s\n",
            "Epoch: 0605 loss_train: 1.3672 acc_train: 0.6240 loss_val: 0.5013 acc_val: 0.8640 time: 0.4172s\n",
            "Epoch: 0606 loss_train: 1.3893 acc_train: 0.6027 loss_val: 0.4992 acc_val: 0.8680 time: 0.4318s\n",
            "Epoch: 0607 loss_train: 1.4838 acc_train: 0.5987 loss_val: 0.4972 acc_val: 0.8680 time: 0.4241s\n",
            "Epoch: 0608 loss_train: 1.4635 acc_train: 0.5813 loss_val: 0.4953 acc_val: 0.8720 time: 0.4267s\n",
            "Epoch: 0609 loss_train: 1.4689 acc_train: 0.5853 loss_val: 0.4938 acc_val: 0.8800 time: 0.4193s\n",
            "Epoch: 0610 loss_train: 1.4659 acc_train: 0.5893 loss_val: 0.4931 acc_val: 0.8840 time: 0.4296s\n",
            "Epoch: 0611 loss_train: 1.5088 acc_train: 0.5987 loss_val: 0.4927 acc_val: 0.8840 time: 0.4288s\n",
            "Epoch: 0612 loss_train: 1.3843 acc_train: 0.6147 loss_val: 0.4930 acc_val: 0.8720 time: 0.4223s\n",
            "Epoch: 0613 loss_train: 1.5254 acc_train: 0.5800 loss_val: 0.4939 acc_val: 0.8720 time: 0.4284s\n",
            "Epoch: 0614 loss_train: 1.3985 acc_train: 0.6160 loss_val: 0.4950 acc_val: 0.8720 time: 0.4295s\n",
            "Epoch: 0615 loss_train: 1.4451 acc_train: 0.5947 loss_val: 0.4964 acc_val: 0.8680 time: 0.4302s\n",
            "Epoch: 0616 loss_train: 1.5270 acc_train: 0.5813 loss_val: 0.4990 acc_val: 0.8680 time: 0.4323s\n",
            "Epoch: 0617 loss_train: 1.4461 acc_train: 0.5987 loss_val: 0.5012 acc_val: 0.8680 time: 0.5022s\n",
            "Epoch: 0618 loss_train: 1.3809 acc_train: 0.6227 loss_val: 0.5032 acc_val: 0.8720 time: 0.4463s\n",
            "Epoch: 0619 loss_train: 1.4121 acc_train: 0.6133 loss_val: 0.5038 acc_val: 0.8720 time: 0.4267s\n",
            "Epoch: 0620 loss_train: 1.4295 acc_train: 0.6053 loss_val: 0.5037 acc_val: 0.8680 time: 0.4340s\n",
            "Epoch: 0621 loss_train: 1.4493 acc_train: 0.5947 loss_val: 0.5027 acc_val: 0.8680 time: 0.4228s\n",
            "Epoch: 0622 loss_train: 1.4629 acc_train: 0.6067 loss_val: 0.5018 acc_val: 0.8680 time: 0.4314s\n",
            "Epoch: 0623 loss_train: 1.4725 acc_train: 0.5960 loss_val: 0.5007 acc_val: 0.8680 time: 0.4299s\n",
            "Epoch: 0624 loss_train: 1.4866 acc_train: 0.5880 loss_val: 0.5005 acc_val: 0.8640 time: 0.4288s\n",
            "Epoch: 0625 loss_train: 1.5535 acc_train: 0.5773 loss_val: 0.5001 acc_val: 0.8640 time: 0.4325s\n",
            "Epoch: 0626 loss_train: 1.4507 acc_train: 0.5973 loss_val: 0.4991 acc_val: 0.8600 time: 0.5085s\n",
            "Epoch: 0627 loss_train: 1.4741 acc_train: 0.5987 loss_val: 0.4982 acc_val: 0.8600 time: 0.4630s\n",
            "Epoch: 0628 loss_train: 1.4731 acc_train: 0.5787 loss_val: 0.4989 acc_val: 0.8600 time: 0.5115s\n",
            "Epoch: 0629 loss_train: 1.5379 acc_train: 0.5813 loss_val: 0.4996 acc_val: 0.8600 time: 0.4617s\n",
            "Epoch: 0630 loss_train: 1.4516 acc_train: 0.6013 loss_val: 0.4998 acc_val: 0.8600 time: 0.5136s\n",
            "Epoch: 0631 loss_train: 1.5282 acc_train: 0.5827 loss_val: 0.4996 acc_val: 0.8600 time: 0.4503s\n",
            "Epoch: 0632 loss_train: 1.3659 acc_train: 0.6213 loss_val: 0.4973 acc_val: 0.8600 time: 0.4252s\n",
            "Epoch: 0633 loss_train: 1.4497 acc_train: 0.6053 loss_val: 0.4950 acc_val: 0.8680 time: 0.4318s\n",
            "Epoch: 0634 loss_train: 1.3994 acc_train: 0.6200 loss_val: 0.4918 acc_val: 0.8680 time: 0.4304s\n",
            "Epoch: 0635 loss_train: 1.3301 acc_train: 0.6280 loss_val: 0.4897 acc_val: 0.8640 time: 0.4291s\n",
            "Epoch: 0636 loss_train: 1.4612 acc_train: 0.5933 loss_val: 0.4888 acc_val: 0.8720 time: 0.4279s\n",
            "Epoch: 0637 loss_train: 1.5770 acc_train: 0.5627 loss_val: 0.4885 acc_val: 0.8720 time: 0.4279s\n",
            "Epoch: 0638 loss_train: 1.4350 acc_train: 0.6173 loss_val: 0.4890 acc_val: 0.8640 time: 0.4318s\n",
            "Epoch: 0639 loss_train: 1.5302 acc_train: 0.5813 loss_val: 0.4926 acc_val: 0.8640 time: 0.4274s\n",
            "Epoch: 0640 loss_train: 1.5327 acc_train: 0.5813 loss_val: 0.4997 acc_val: 0.8600 time: 0.4308s\n",
            "Epoch: 0641 loss_train: 1.4714 acc_train: 0.6000 loss_val: 0.5079 acc_val: 0.8600 time: 0.4299s\n",
            "Epoch: 0642 loss_train: 1.4363 acc_train: 0.5987 loss_val: 0.5154 acc_val: 0.8560 time: 0.4282s\n",
            "Epoch: 0643 loss_train: 1.4817 acc_train: 0.5973 loss_val: 0.5199 acc_val: 0.8560 time: 0.4291s\n",
            "Epoch: 0644 loss_train: 1.4382 acc_train: 0.6000 loss_val: 0.5214 acc_val: 0.8560 time: 0.4220s\n",
            "Epoch: 0645 loss_train: 1.4604 acc_train: 0.5893 loss_val: 0.5228 acc_val: 0.8520 time: 0.4418s\n",
            "Epoch: 0646 loss_train: 1.4322 acc_train: 0.6080 loss_val: 0.5239 acc_val: 0.8520 time: 0.4190s\n",
            "Epoch: 0647 loss_train: 1.3452 acc_train: 0.6387 loss_val: 0.5242 acc_val: 0.8560 time: 0.4430s\n",
            "Epoch: 0648 loss_train: 1.4667 acc_train: 0.5973 loss_val: 0.5229 acc_val: 0.8560 time: 0.4294s\n",
            "Epoch: 0649 loss_train: 1.3821 acc_train: 0.6173 loss_val: 0.5226 acc_val: 0.8600 time: 0.4293s\n",
            "Epoch: 0650 loss_train: 1.4993 acc_train: 0.5853 loss_val: 0.5227 acc_val: 0.8640 time: 0.5044s\n",
            "Epoch: 0651 loss_train: 1.4381 acc_train: 0.6133 loss_val: 0.5227 acc_val: 0.8640 time: 0.5191s\n",
            "Epoch: 0652 loss_train: 1.4843 acc_train: 0.5867 loss_val: 0.5222 acc_val: 0.8640 time: 0.4297s\n",
            "Epoch: 0653 loss_train: 1.4285 acc_train: 0.6080 loss_val: 0.5213 acc_val: 0.8640 time: 0.4312s\n",
            "Epoch: 0654 loss_train: 1.4229 acc_train: 0.6147 loss_val: 0.5200 acc_val: 0.8600 time: 0.4319s\n",
            "Epoch: 0655 loss_train: 1.5268 acc_train: 0.5733 loss_val: 0.5187 acc_val: 0.8600 time: 0.4702s\n",
            "Epoch: 0656 loss_train: 1.3697 acc_train: 0.6240 loss_val: 0.5159 acc_val: 0.8640 time: 0.4632s\n",
            "Epoch: 0657 loss_train: 1.4702 acc_train: 0.5867 loss_val: 0.5128 acc_val: 0.8600 time: 0.4226s\n",
            "Epoch: 0658 loss_train: 1.4660 acc_train: 0.5960 loss_val: 0.5101 acc_val: 0.8560 time: 0.4307s\n",
            "Epoch: 0659 loss_train: 1.4495 acc_train: 0.6040 loss_val: 0.5077 acc_val: 0.8520 time: 0.4295s\n",
            "Epoch: 0660 loss_train: 1.4663 acc_train: 0.6080 loss_val: 0.5055 acc_val: 0.8560 time: 0.4261s\n",
            "Epoch: 0661 loss_train: 1.4853 acc_train: 0.5880 loss_val: 0.5053 acc_val: 0.8560 time: 0.4244s\n",
            "Epoch: 0662 loss_train: 1.4991 acc_train: 0.5627 loss_val: 0.5041 acc_val: 0.8600 time: 0.4254s\n",
            "Epoch: 0663 loss_train: 1.5340 acc_train: 0.5573 loss_val: 0.5035 acc_val: 0.8600 time: 0.4835s\n",
            "Epoch: 0664 loss_train: 1.4430 acc_train: 0.5893 loss_val: 0.5027 acc_val: 0.8560 time: 0.5155s\n",
            "Epoch: 0665 loss_train: 1.4411 acc_train: 0.5853 loss_val: 0.5045 acc_val: 0.8560 time: 0.4533s\n",
            "Epoch: 0666 loss_train: 1.4058 acc_train: 0.6067 loss_val: 0.5061 acc_val: 0.8560 time: 0.4218s\n",
            "Epoch: 0667 loss_train: 1.5221 acc_train: 0.5667 loss_val: 0.5083 acc_val: 0.8560 time: 0.4205s\n",
            "Epoch: 0668 loss_train: 1.3490 acc_train: 0.6347 loss_val: 0.5089 acc_val: 0.8560 time: 0.4256s\n",
            "Epoch: 0669 loss_train: 1.5456 acc_train: 0.5893 loss_val: 0.5071 acc_val: 0.8640 time: 0.5112s\n",
            "Epoch: 0670 loss_train: 1.5033 acc_train: 0.5853 loss_val: 0.5061 acc_val: 0.8600 time: 0.4308s\n",
            "Epoch: 0671 loss_train: 1.4191 acc_train: 0.6120 loss_val: 0.5068 acc_val: 0.8600 time: 0.4289s\n",
            "Epoch: 0672 loss_train: 1.3946 acc_train: 0.6267 loss_val: 0.5079 acc_val: 0.8600 time: 0.5045s\n",
            "Epoch: 0673 loss_train: 1.5565 acc_train: 0.5627 loss_val: 0.5078 acc_val: 0.8640 time: 0.5270s\n",
            "Epoch: 0674 loss_train: 1.4313 acc_train: 0.5960 loss_val: 0.5073 acc_val: 0.8640 time: 0.4295s\n",
            "Epoch: 0675 loss_train: 1.3942 acc_train: 0.6187 loss_val: 0.5060 acc_val: 0.8680 time: 0.4207s\n",
            "Epoch: 0676 loss_train: 1.3886 acc_train: 0.6227 loss_val: 0.5038 acc_val: 0.8680 time: 0.4253s\n",
            "Epoch: 0677 loss_train: 1.4774 acc_train: 0.5893 loss_val: 0.5015 acc_val: 0.8680 time: 0.4305s\n",
            "Epoch: 0678 loss_train: 1.4614 acc_train: 0.5947 loss_val: 0.4997 acc_val: 0.8640 time: 0.4256s\n",
            "Epoch: 0679 loss_train: 1.3150 acc_train: 0.6320 loss_val: 0.4985 acc_val: 0.8680 time: 0.4283s\n",
            "Epoch: 0680 loss_train: 1.5024 acc_train: 0.5867 loss_val: 0.4968 acc_val: 0.8680 time: 0.4327s\n",
            "Epoch: 0681 loss_train: 1.4176 acc_train: 0.6147 loss_val: 0.4949 acc_val: 0.8720 time: 0.4287s\n",
            "Epoch: 0682 loss_train: 1.4426 acc_train: 0.5987 loss_val: 0.4929 acc_val: 0.8680 time: 0.4203s\n",
            "Epoch: 0683 loss_train: 1.4251 acc_train: 0.6053 loss_val: 0.4914 acc_val: 0.8720 time: 0.4316s\n",
            "Epoch: 0684 loss_train: 1.4912 acc_train: 0.6013 loss_val: 0.4908 acc_val: 0.8720 time: 0.4315s\n",
            "Epoch: 0685 loss_train: 1.4442 acc_train: 0.5960 loss_val: 0.4910 acc_val: 0.8720 time: 0.4254s\n",
            "Epoch: 0686 loss_train: 1.4049 acc_train: 0.6213 loss_val: 0.4916 acc_val: 0.8720 time: 0.4216s\n",
            "Epoch: 0687 loss_train: 1.3689 acc_train: 0.6227 loss_val: 0.4924 acc_val: 0.8720 time: 0.4415s\n",
            "Epoch: 0688 loss_train: 1.5466 acc_train: 0.5680 loss_val: 0.4945 acc_val: 0.8760 time: 0.4319s\n",
            "Epoch: 0689 loss_train: 1.4389 acc_train: 0.5973 loss_val: 0.4959 acc_val: 0.8720 time: 0.4236s\n",
            "Epoch: 0690 loss_train: 1.5027 acc_train: 0.5800 loss_val: 0.4977 acc_val: 0.8680 time: 0.4874s\n",
            "Epoch: 0691 loss_train: 1.3747 acc_train: 0.6253 loss_val: 0.4991 acc_val: 0.8680 time: 0.5110s\n",
            "Epoch: 0692 loss_train: 1.4264 acc_train: 0.6013 loss_val: 0.5014 acc_val: 0.8680 time: 0.4357s\n",
            "Epoch: 0693 loss_train: 1.4620 acc_train: 0.6000 loss_val: 0.5032 acc_val: 0.8600 time: 0.4300s\n",
            "Epoch: 0694 loss_train: 1.3514 acc_train: 0.6333 loss_val: 0.5042 acc_val: 0.8600 time: 0.4647s\n",
            "Epoch: 0695 loss_train: 1.4003 acc_train: 0.6227 loss_val: 0.5047 acc_val: 0.8600 time: 0.5026s\n",
            "Epoch: 0696 loss_train: 1.4405 acc_train: 0.5933 loss_val: 0.5044 acc_val: 0.8640 time: 0.4610s\n",
            "Epoch: 0697 loss_train: 1.4469 acc_train: 0.5933 loss_val: 0.5045 acc_val: 0.8640 time: 0.4295s\n",
            "Epoch: 0698 loss_train: 1.4637 acc_train: 0.5907 loss_val: 0.5047 acc_val: 0.8640 time: 0.4280s\n",
            "Epoch: 0699 loss_train: 1.4170 acc_train: 0.6160 loss_val: 0.5037 acc_val: 0.8640 time: 0.4276s\n",
            "Epoch: 0700 loss_train: 1.5471 acc_train: 0.5627 loss_val: 0.5026 acc_val: 0.8600 time: 0.4267s\n",
            "Epoch: 0701 loss_train: 1.4232 acc_train: 0.5987 loss_val: 0.5011 acc_val: 0.8600 time: 0.4221s\n",
            "Epoch: 0702 loss_train: 1.4356 acc_train: 0.6053 loss_val: 0.4980 acc_val: 0.8600 time: 0.4298s\n",
            "Epoch: 0703 loss_train: 1.4614 acc_train: 0.5880 loss_val: 0.4949 acc_val: 0.8600 time: 0.4298s\n",
            "Epoch: 0704 loss_train: 1.4457 acc_train: 0.6080 loss_val: 0.4933 acc_val: 0.8640 time: 0.4383s\n",
            "Epoch: 0705 loss_train: 1.4476 acc_train: 0.6040 loss_val: 0.4934 acc_val: 0.8640 time: 0.4613s\n",
            "Epoch: 0706 loss_train: 1.4984 acc_train: 0.5813 loss_val: 0.4939 acc_val: 0.8640 time: 0.4307s\n",
            "Epoch: 0707 loss_train: 1.3593 acc_train: 0.6200 loss_val: 0.4937 acc_val: 0.8680 time: 0.4233s\n",
            "Epoch: 0708 loss_train: 1.3930 acc_train: 0.6133 loss_val: 0.4930 acc_val: 0.8640 time: 0.4288s\n",
            "Epoch: 0709 loss_train: 1.5011 acc_train: 0.5867 loss_val: 0.4907 acc_val: 0.8680 time: 0.4213s\n",
            "Epoch: 0710 loss_train: 1.4235 acc_train: 0.5920 loss_val: 0.4876 acc_val: 0.8680 time: 0.4178s\n",
            "Epoch: 0711 loss_train: 1.4533 acc_train: 0.5987 loss_val: 0.4862 acc_val: 0.8760 time: 0.4189s\n",
            "Epoch: 0712 loss_train: 1.3847 acc_train: 0.6187 loss_val: 0.4861 acc_val: 0.8800 time: 0.4285s\n",
            "Epoch: 0713 loss_train: 1.4807 acc_train: 0.5933 loss_val: 0.4870 acc_val: 0.8760 time: 0.4323s\n",
            "Epoch: 0714 loss_train: 1.4286 acc_train: 0.6027 loss_val: 0.4894 acc_val: 0.8800 time: 0.4318s\n",
            "Epoch: 0715 loss_train: 1.5000 acc_train: 0.6013 loss_val: 0.4896 acc_val: 0.8760 time: 0.4318s\n",
            "Epoch: 0716 loss_train: 1.4008 acc_train: 0.6253 loss_val: 0.4894 acc_val: 0.8720 time: 0.4313s\n",
            "Epoch: 0717 loss_train: 1.4456 acc_train: 0.5973 loss_val: 0.4900 acc_val: 0.8720 time: 0.4278s\n",
            "Epoch: 0718 loss_train: 1.3554 acc_train: 0.6227 loss_val: 0.4938 acc_val: 0.8680 time: 0.4304s\n",
            "Epoch: 0719 loss_train: 1.3571 acc_train: 0.6347 loss_val: 0.4971 acc_val: 0.8680 time: 0.4492s\n",
            "Epoch: 0720 loss_train: 1.4712 acc_train: 0.5893 loss_val: 0.4997 acc_val: 0.8680 time: 0.5037s\n",
            "Epoch: 0721 loss_train: 1.4336 acc_train: 0.5947 loss_val: 0.5020 acc_val: 0.8720 time: 0.4922s\n",
            "Epoch: 0722 loss_train: 1.5052 acc_train: 0.5773 loss_val: 0.5033 acc_val: 0.8680 time: 0.5244s\n",
            "Epoch: 0723 loss_train: 1.4163 acc_train: 0.6200 loss_val: 0.5016 acc_val: 0.8680 time: 0.4431s\n",
            "Epoch: 0724 loss_train: 1.3678 acc_train: 0.6147 loss_val: 0.4990 acc_val: 0.8640 time: 0.4317s\n",
            "Epoch: 0725 loss_train: 1.2973 acc_train: 0.6440 loss_val: 0.4945 acc_val: 0.8640 time: 0.4404s\n",
            "Epoch: 0726 loss_train: 1.4662 acc_train: 0.5947 loss_val: 0.4907 acc_val: 0.8720 time: 0.4311s\n",
            "Epoch: 0727 loss_train: 1.5328 acc_train: 0.5813 loss_val: 0.4864 acc_val: 0.8800 time: 0.4280s\n",
            "Epoch: 0728 loss_train: 1.4163 acc_train: 0.6053 loss_val: 0.4820 acc_val: 0.8680 time: 0.4301s\n",
            "Epoch: 0729 loss_train: 1.3780 acc_train: 0.6053 loss_val: 0.4790 acc_val: 0.8720 time: 0.4241s\n",
            "Epoch: 0730 loss_train: 1.3807 acc_train: 0.6213 loss_val: 0.4778 acc_val: 0.8720 time: 0.4279s\n",
            "Epoch: 0731 loss_train: 1.5113 acc_train: 0.5693 loss_val: 0.4789 acc_val: 0.8720 time: 0.4294s\n",
            "Epoch: 0732 loss_train: 1.4753 acc_train: 0.6000 loss_val: 0.4813 acc_val: 0.8720 time: 0.4279s\n",
            "Epoch: 0733 loss_train: 1.3904 acc_train: 0.6093 loss_val: 0.4853 acc_val: 0.8720 time: 0.4186s\n",
            "Epoch: 0734 loss_train: 1.4327 acc_train: 0.6000 loss_val: 0.4880 acc_val: 0.8720 time: 0.4258s\n",
            "Epoch: 0735 loss_train: 1.3849 acc_train: 0.6267 loss_val: 0.4883 acc_val: 0.8680 time: 0.4303s\n",
            "Epoch: 0736 loss_train: 1.4580 acc_train: 0.6120 loss_val: 0.4884 acc_val: 0.8720 time: 0.4253s\n",
            "Epoch: 0737 loss_train: 1.4820 acc_train: 0.5973 loss_val: 0.4894 acc_val: 0.8680 time: 0.4273s\n",
            "Epoch: 0738 loss_train: 1.4824 acc_train: 0.5813 loss_val: 0.4904 acc_val: 0.8720 time: 0.4286s\n",
            "Epoch: 0739 loss_train: 1.4445 acc_train: 0.5973 loss_val: 0.4921 acc_val: 0.8720 time: 0.4265s\n",
            "Epoch: 0740 loss_train: 1.4716 acc_train: 0.5960 loss_val: 0.4943 acc_val: 0.8680 time: 0.4323s\n",
            "Epoch: 0741 loss_train: 1.3971 acc_train: 0.6133 loss_val: 0.4956 acc_val: 0.8680 time: 0.4292s\n",
            "Epoch: 0742 loss_train: 1.5484 acc_train: 0.5640 loss_val: 0.4965 acc_val: 0.8680 time: 0.4308s\n",
            "Epoch: 0743 loss_train: 1.4776 acc_train: 0.5947 loss_val: 0.4967 acc_val: 0.8680 time: 0.4280s\n",
            "Epoch: 0744 loss_train: 1.4837 acc_train: 0.5947 loss_val: 0.4967 acc_val: 0.8640 time: 0.4279s\n",
            "Epoch: 0745 loss_train: 1.4738 acc_train: 0.6013 loss_val: 0.4970 acc_val: 0.8600 time: 0.4240s\n",
            "Epoch: 0746 loss_train: 1.5095 acc_train: 0.5773 loss_val: 0.4962 acc_val: 0.8600 time: 0.4242s\n",
            "Epoch: 0747 loss_train: 1.4976 acc_train: 0.5880 loss_val: 0.4949 acc_val: 0.8640 time: 0.4286s\n",
            "Epoch: 0748 loss_train: 1.4806 acc_train: 0.5907 loss_val: 0.4971 acc_val: 0.8560 time: 0.4291s\n",
            "Epoch: 0749 loss_train: 1.3897 acc_train: 0.6173 loss_val: 0.4997 acc_val: 0.8520 time: 0.4250s\n",
            "Epoch: 0750 loss_train: 1.4648 acc_train: 0.5867 loss_val: 0.5006 acc_val: 0.8520 time: 0.4224s\n",
            "Epoch: 0751 loss_train: 1.4648 acc_train: 0.5907 loss_val: 0.5005 acc_val: 0.8520 time: 0.4296s\n",
            "Epoch: 0752 loss_train: 1.4721 acc_train: 0.6067 loss_val: 0.4995 acc_val: 0.8520 time: 0.4308s\n",
            "Epoch: 0753 loss_train: 1.4396 acc_train: 0.6120 loss_val: 0.4978 acc_val: 0.8600 time: 0.4368s\n",
            "Epoch: 0754 loss_train: 1.4991 acc_train: 0.5867 loss_val: 0.4971 acc_val: 0.8680 time: 0.4245s\n",
            "Epoch: 0755 loss_train: 1.5105 acc_train: 0.5933 loss_val: 0.4994 acc_val: 0.8720 time: 0.4265s\n",
            "Epoch: 0756 loss_train: 1.3573 acc_train: 0.6240 loss_val: 0.5002 acc_val: 0.8720 time: 0.4255s\n",
            "Epoch: 0757 loss_train: 1.4905 acc_train: 0.5773 loss_val: 0.5003 acc_val: 0.8720 time: 0.4256s\n",
            "Epoch: 0758 loss_train: 1.4735 acc_train: 0.5893 loss_val: 0.5009 acc_val: 0.8720 time: 0.4173s\n",
            "Epoch: 0759 loss_train: 1.4023 acc_train: 0.6160 loss_val: 0.5006 acc_val: 0.8680 time: 0.4280s\n",
            "Epoch: 0760 loss_train: 1.4773 acc_train: 0.6027 loss_val: 0.5003 acc_val: 0.8640 time: 0.4266s\n",
            "Epoch: 0761 loss_train: 1.4809 acc_train: 0.5867 loss_val: 0.5004 acc_val: 0.8720 time: 0.4264s\n",
            "Epoch: 0762 loss_train: 1.4401 acc_train: 0.6000 loss_val: 0.5005 acc_val: 0.8760 time: 0.4218s\n",
            "Epoch: 0763 loss_train: 1.3997 acc_train: 0.6173 loss_val: 0.5008 acc_val: 0.8760 time: 0.4334s\n",
            "Epoch: 0764 loss_train: 1.4976 acc_train: 0.5747 loss_val: 0.5007 acc_val: 0.8680 time: 0.4243s\n",
            "Epoch: 0765 loss_train: 1.4362 acc_train: 0.6107 loss_val: 0.4995 acc_val: 0.8680 time: 0.4249s\n",
            "Epoch: 0766 loss_train: 1.3999 acc_train: 0.6160 loss_val: 0.4991 acc_val: 0.8680 time: 0.4308s\n",
            "Epoch: 0767 loss_train: 1.4658 acc_train: 0.6013 loss_val: 0.4977 acc_val: 0.8640 time: 0.4355s\n",
            "Epoch: 0768 loss_train: 1.4319 acc_train: 0.6120 loss_val: 0.4984 acc_val: 0.8640 time: 0.4287s\n",
            "Epoch: 0769 loss_train: 1.4404 acc_train: 0.5920 loss_val: 0.4977 acc_val: 0.8640 time: 0.4374s\n",
            "Epoch: 0770 loss_train: 1.3647 acc_train: 0.6213 loss_val: 0.4970 acc_val: 0.8680 time: 0.4329s\n",
            "Epoch: 0771 loss_train: 1.4479 acc_train: 0.6040 loss_val: 0.4967 acc_val: 0.8720 time: 0.4205s\n",
            "Epoch: 0772 loss_train: 1.3998 acc_train: 0.6133 loss_val: 0.4968 acc_val: 0.8720 time: 0.5211s\n",
            "Epoch: 0773 loss_train: 1.3698 acc_train: 0.6253 loss_val: 0.4963 acc_val: 0.8720 time: 0.4849s\n",
            "Epoch: 0774 loss_train: 1.5172 acc_train: 0.5867 loss_val: 0.4960 acc_val: 0.8720 time: 0.5056s\n",
            "Epoch: 0775 loss_train: 1.3823 acc_train: 0.6227 loss_val: 0.4963 acc_val: 0.8760 time: 0.5033s\n",
            "Epoch: 0776 loss_train: 1.3801 acc_train: 0.6227 loss_val: 0.4953 acc_val: 0.8680 time: 0.5134s\n",
            "Epoch: 0777 loss_train: 1.3914 acc_train: 0.6173 loss_val: 0.4933 acc_val: 0.8720 time: 0.4927s\n",
            "Epoch: 0778 loss_train: 1.5261 acc_train: 0.5907 loss_val: 0.4915 acc_val: 0.8680 time: 0.4254s\n",
            "Epoch: 0779 loss_train: 1.4701 acc_train: 0.6013 loss_val: 0.4902 acc_val: 0.8600 time: 0.4300s\n",
            "Epoch: 0780 loss_train: 1.3366 acc_train: 0.6360 loss_val: 0.4900 acc_val: 0.8640 time: 0.4289s\n",
            "Epoch: 0781 loss_train: 1.4024 acc_train: 0.6227 loss_val: 0.4889 acc_val: 0.8640 time: 0.4186s\n",
            "Epoch: 0782 loss_train: 1.4391 acc_train: 0.6080 loss_val: 0.4885 acc_val: 0.8720 time: 0.4283s\n",
            "Epoch: 0783 loss_train: 1.4080 acc_train: 0.6093 loss_val: 0.4855 acc_val: 0.8680 time: 0.4290s\n",
            "Epoch: 0784 loss_train: 1.5137 acc_train: 0.5853 loss_val: 0.4838 acc_val: 0.8680 time: 0.4297s\n",
            "Epoch: 0785 loss_train: 1.5000 acc_train: 0.5827 loss_val: 0.4822 acc_val: 0.8640 time: 0.4256s\n",
            "Epoch: 0786 loss_train: 1.4556 acc_train: 0.5933 loss_val: 0.4794 acc_val: 0.8680 time: 0.4254s\n",
            "Epoch: 0787 loss_train: 1.4552 acc_train: 0.5920 loss_val: 0.4772 acc_val: 0.8720 time: 0.4330s\n",
            "Epoch: 0788 loss_train: 1.4512 acc_train: 0.6000 loss_val: 0.4756 acc_val: 0.8720 time: 0.4294s\n",
            "Epoch: 0789 loss_train: 1.4209 acc_train: 0.5973 loss_val: 0.4760 acc_val: 0.8720 time: 0.4200s\n",
            "Epoch: 0790 loss_train: 1.2916 acc_train: 0.6533 loss_val: 0.4778 acc_val: 0.8760 time: 0.4269s\n",
            "Epoch: 0791 loss_train: 1.4253 acc_train: 0.6013 loss_val: 0.4801 acc_val: 0.8760 time: 0.4436s\n",
            "Epoch: 0792 loss_train: 1.3459 acc_train: 0.6347 loss_val: 0.4819 acc_val: 0.8760 time: 0.4240s\n",
            "Epoch: 0793 loss_train: 1.4995 acc_train: 0.5787 loss_val: 0.4839 acc_val: 0.8760 time: 0.4297s\n",
            "Epoch: 0794 loss_train: 1.4456 acc_train: 0.6160 loss_val: 0.4856 acc_val: 0.8640 time: 0.4271s\n",
            "Epoch: 0795 loss_train: 1.3845 acc_train: 0.6187 loss_val: 0.4889 acc_val: 0.8640 time: 0.4210s\n",
            "Epoch: 0796 loss_train: 1.4437 acc_train: 0.5933 loss_val: 0.4918 acc_val: 0.8600 time: 0.4725s\n",
            "Epoch: 0797 loss_train: 1.3803 acc_train: 0.6173 loss_val: 0.4919 acc_val: 0.8600 time: 0.5037s\n",
            "Epoch: 0798 loss_train: 1.2956 acc_train: 0.6400 loss_val: 0.4898 acc_val: 0.8600 time: 0.4422s\n",
            "Epoch: 0799 loss_train: 1.4554 acc_train: 0.5987 loss_val: 0.4867 acc_val: 0.8640 time: 0.5144s\n",
            "Epoch: 0800 loss_train: 1.3747 acc_train: 0.6320 loss_val: 0.4836 acc_val: 0.8720 time: 0.4198s\n",
            "Epoch: 0801 loss_train: 1.4293 acc_train: 0.5947 loss_val: 0.4809 acc_val: 0.8720 time: 0.4286s\n",
            "Epoch: 0802 loss_train: 1.4345 acc_train: 0.5947 loss_val: 0.4785 acc_val: 0.8800 time: 0.4297s\n",
            "Epoch: 0803 loss_train: 1.3601 acc_train: 0.6173 loss_val: 0.4768 acc_val: 0.8880 time: 0.4351s\n",
            "Epoch: 0804 loss_train: 1.3770 acc_train: 0.6067 loss_val: 0.4748 acc_val: 0.9000 time: 0.4327s\n",
            "Epoch: 0805 loss_train: 1.3840 acc_train: 0.6067 loss_val: 0.4728 acc_val: 0.9000 time: 0.5155s\n",
            "Epoch: 0806 loss_train: 1.3682 acc_train: 0.6333 loss_val: 0.4708 acc_val: 0.9000 time: 0.4991s\n",
            "Epoch: 0807 loss_train: 1.4255 acc_train: 0.6027 loss_val: 0.4701 acc_val: 0.9000 time: 0.4303s\n",
            "Epoch: 0808 loss_train: 1.3772 acc_train: 0.6200 loss_val: 0.4706 acc_val: 0.8960 time: 0.4305s\n",
            "Epoch: 0809 loss_train: 1.4977 acc_train: 0.5867 loss_val: 0.4706 acc_val: 0.8920 time: 0.4819s\n",
            "Epoch: 0810 loss_train: 1.5076 acc_train: 0.5800 loss_val: 0.4723 acc_val: 0.8880 time: 0.5085s\n",
            "Epoch: 0811 loss_train: 1.4166 acc_train: 0.6133 loss_val: 0.4769 acc_val: 0.8840 time: 0.4287s\n",
            "Epoch: 0812 loss_train: 1.4510 acc_train: 0.5987 loss_val: 0.4821 acc_val: 0.8720 time: 0.4276s\n",
            "Epoch: 0813 loss_train: 1.4577 acc_train: 0.5947 loss_val: 0.4876 acc_val: 0.8680 time: 0.4629s\n",
            "Epoch: 0814 loss_train: 1.3867 acc_train: 0.6160 loss_val: 0.4925 acc_val: 0.8680 time: 0.4317s\n",
            "Epoch: 0815 loss_train: 1.4165 acc_train: 0.6147 loss_val: 0.4955 acc_val: 0.8680 time: 0.4340s\n",
            "Epoch: 0816 loss_train: 1.5170 acc_train: 0.5720 loss_val: 0.4958 acc_val: 0.8720 time: 0.4331s\n",
            "Epoch: 0817 loss_train: 1.4244 acc_train: 0.6187 loss_val: 0.4963 acc_val: 0.8720 time: 0.4299s\n",
            "Epoch: 0818 loss_train: 1.3883 acc_train: 0.6253 loss_val: 0.4964 acc_val: 0.8720 time: 0.4317s\n",
            "Epoch: 0819 loss_train: 1.5465 acc_train: 0.5773 loss_val: 0.4982 acc_val: 0.8720 time: 0.4234s\n",
            "Epoch: 0820 loss_train: 1.4682 acc_train: 0.5933 loss_val: 0.4993 acc_val: 0.8760 time: 0.4281s\n",
            "Epoch: 0821 loss_train: 1.3693 acc_train: 0.6267 loss_val: 0.5007 acc_val: 0.8800 time: 0.4313s\n",
            "Epoch: 0822 loss_train: 1.3653 acc_train: 0.6253 loss_val: 0.5012 acc_val: 0.8840 time: 0.4368s\n",
            "Epoch: 0823 loss_train: 1.3715 acc_train: 0.6227 loss_val: 0.5008 acc_val: 0.8840 time: 0.4346s\n",
            "Epoch: 0824 loss_train: 1.4124 acc_train: 0.6120 loss_val: 0.5006 acc_val: 0.8840 time: 0.4276s\n",
            "Epoch: 0825 loss_train: 1.4482 acc_train: 0.6013 loss_val: 0.4998 acc_val: 0.8760 time: 0.4359s\n",
            "Epoch: 0826 loss_train: 1.4177 acc_train: 0.6093 loss_val: 0.4975 acc_val: 0.8760 time: 0.4231s\n",
            "Epoch: 0827 loss_train: 1.4103 acc_train: 0.6133 loss_val: 0.4952 acc_val: 0.8680 time: 0.4311s\n",
            "Epoch: 0828 loss_train: 1.4321 acc_train: 0.6173 loss_val: 0.4925 acc_val: 0.8680 time: 0.4286s\n",
            "Epoch: 0829 loss_train: 1.4704 acc_train: 0.5907 loss_val: 0.4895 acc_val: 0.8720 time: 0.4337s\n",
            "Epoch: 0830 loss_train: 1.4508 acc_train: 0.6040 loss_val: 0.4870 acc_val: 0.8760 time: 0.4314s\n",
            "Epoch: 0831 loss_train: 1.4067 acc_train: 0.6227 loss_val: 0.4849 acc_val: 0.8800 time: 0.4332s\n",
            "Epoch: 0832 loss_train: 1.4388 acc_train: 0.5933 loss_val: 0.4835 acc_val: 0.8800 time: 0.4353s\n",
            "Epoch: 0833 loss_train: 1.4704 acc_train: 0.5893 loss_val: 0.4834 acc_val: 0.8840 time: 0.4290s\n",
            "Epoch: 0834 loss_train: 1.4674 acc_train: 0.5933 loss_val: 0.4838 acc_val: 0.8760 time: 0.4352s\n",
            "Epoch: 0835 loss_train: 1.4798 acc_train: 0.5853 loss_val: 0.4844 acc_val: 0.8720 time: 0.4331s\n",
            "Epoch: 0836 loss_train: 1.5322 acc_train: 0.5720 loss_val: 0.4847 acc_val: 0.8720 time: 0.4347s\n",
            "Epoch: 0837 loss_train: 1.4898 acc_train: 0.5853 loss_val: 0.4869 acc_val: 0.8720 time: 0.4343s\n",
            "Epoch: 0838 loss_train: 1.4117 acc_train: 0.6067 loss_val: 0.4895 acc_val: 0.8760 time: 0.4350s\n",
            "Epoch: 0839 loss_train: 1.4170 acc_train: 0.6040 loss_val: 0.4916 acc_val: 0.8760 time: 0.4435s\n",
            "Epoch: 0840 loss_train: 1.3813 acc_train: 0.6267 loss_val: 0.4928 acc_val: 0.8760 time: 0.4326s\n",
            "Epoch: 0841 loss_train: 1.3286 acc_train: 0.6267 loss_val: 0.4939 acc_val: 0.8800 time: 0.4315s\n",
            "Epoch: 0842 loss_train: 1.4090 acc_train: 0.6147 loss_val: 0.4935 acc_val: 0.8800 time: 0.4305s\n",
            "Epoch: 0843 loss_train: 1.3912 acc_train: 0.6227 loss_val: 0.4924 acc_val: 0.8800 time: 0.5009s\n",
            "Epoch: 0844 loss_train: 1.4902 acc_train: 0.5973 loss_val: 0.4917 acc_val: 0.8800 time: 0.5115s\n",
            "Epoch: 0845 loss_train: 1.5721 acc_train: 0.5693 loss_val: 0.4927 acc_val: 0.8760 time: 0.4263s\n",
            "Epoch: 0846 loss_train: 1.3560 acc_train: 0.6293 loss_val: 0.4940 acc_val: 0.8720 time: 0.4297s\n",
            "Epoch: 0847 loss_train: 1.3499 acc_train: 0.6280 loss_val: 0.4965 acc_val: 0.8720 time: 0.4336s\n",
            "Epoch: 0848 loss_train: 1.4757 acc_train: 0.6000 loss_val: 0.4989 acc_val: 0.8720 time: 0.4335s\n",
            "Epoch: 0849 loss_train: 1.4238 acc_train: 0.5987 loss_val: 0.5016 acc_val: 0.8720 time: 0.4244s\n",
            "Epoch: 0850 loss_train: 1.4412 acc_train: 0.5973 loss_val: 0.5044 acc_val: 0.8640 time: 0.4311s\n",
            "Epoch: 0851 loss_train: 1.3329 acc_train: 0.6373 loss_val: 0.5062 acc_val: 0.8640 time: 0.4439s\n",
            "Epoch: 0852 loss_train: 1.3908 acc_train: 0.6120 loss_val: 0.5064 acc_val: 0.8560 time: 0.4997s\n",
            "Epoch: 0853 loss_train: 1.4475 acc_train: 0.6000 loss_val: 0.5067 acc_val: 0.8560 time: 0.5132s\n",
            "Epoch: 0854 loss_train: 1.4804 acc_train: 0.5973 loss_val: 0.5058 acc_val: 0.8520 time: 0.4261s\n",
            "Epoch: 0855 loss_train: 1.4747 acc_train: 0.5947 loss_val: 0.5041 acc_val: 0.8520 time: 0.4270s\n",
            "Epoch: 0856 loss_train: 1.3531 acc_train: 0.6267 loss_val: 0.5026 acc_val: 0.8560 time: 0.4152s\n",
            "Epoch: 0857 loss_train: 1.4936 acc_train: 0.5787 loss_val: 0.5010 acc_val: 0.8560 time: 0.4232s\n",
            "Epoch: 0858 loss_train: 1.5398 acc_train: 0.5747 loss_val: 0.4996 acc_val: 0.8560 time: 0.4298s\n",
            "Epoch: 0859 loss_train: 1.3265 acc_train: 0.6240 loss_val: 0.4961 acc_val: 0.8560 time: 0.4320s\n",
            "Epoch: 0860 loss_train: 1.3249 acc_train: 0.6387 loss_val: 0.4922 acc_val: 0.8600 time: 0.4239s\n",
            "Epoch: 0861 loss_train: 1.3830 acc_train: 0.6240 loss_val: 0.4889 acc_val: 0.8680 time: 0.4328s\n",
            "Epoch: 0862 loss_train: 1.4707 acc_train: 0.5933 loss_val: 0.4860 acc_val: 0.8720 time: 0.4318s\n",
            "Epoch: 0863 loss_train: 1.4284 acc_train: 0.6013 loss_val: 0.4842 acc_val: 0.8760 time: 0.4297s\n",
            "Epoch: 0864 loss_train: 1.4269 acc_train: 0.6147 loss_val: 0.4826 acc_val: 0.8800 time: 0.4290s\n",
            "Epoch: 0865 loss_train: 1.4324 acc_train: 0.5960 loss_val: 0.4813 acc_val: 0.8880 time: 0.4139s\n",
            "Epoch: 0866 loss_train: 1.4414 acc_train: 0.6093 loss_val: 0.4801 acc_val: 0.8880 time: 0.4303s\n",
            "Epoch: 0867 loss_train: 1.3771 acc_train: 0.6200 loss_val: 0.4793 acc_val: 0.8880 time: 0.4217s\n",
            "Epoch: 0868 loss_train: 1.3928 acc_train: 0.6173 loss_val: 0.4786 acc_val: 0.8840 time: 0.4321s\n",
            "Epoch: 0869 loss_train: 1.4073 acc_train: 0.6093 loss_val: 0.4788 acc_val: 0.8840 time: 0.4291s\n",
            "Epoch: 0870 loss_train: 1.3352 acc_train: 0.6520 loss_val: 0.4803 acc_val: 0.8840 time: 0.4241s\n",
            "Epoch: 0871 loss_train: 1.4217 acc_train: 0.6067 loss_val: 0.4821 acc_val: 0.8760 time: 0.4321s\n",
            "Epoch: 0872 loss_train: 1.3911 acc_train: 0.6187 loss_val: 0.4833 acc_val: 0.8680 time: 0.4295s\n",
            "Epoch: 0873 loss_train: 1.4274 acc_train: 0.6080 loss_val: 0.4855 acc_val: 0.8600 time: 0.4329s\n",
            "Epoch: 0874 loss_train: 1.5263 acc_train: 0.5787 loss_val: 0.4873 acc_val: 0.8640 time: 0.4285s\n",
            "Epoch: 0875 loss_train: 1.3751 acc_train: 0.6213 loss_val: 0.4883 acc_val: 0.8640 time: 0.4909s\n",
            "Epoch: 0876 loss_train: 1.4123 acc_train: 0.6173 loss_val: 0.4884 acc_val: 0.8600 time: 0.4288s\n",
            "Epoch: 0877 loss_train: 1.4531 acc_train: 0.5987 loss_val: 0.4880 acc_val: 0.8600 time: 0.4308s\n",
            "Epoch: 0878 loss_train: 1.5040 acc_train: 0.5747 loss_val: 0.4892 acc_val: 0.8640 time: 0.4309s\n",
            "Epoch: 0879 loss_train: 1.4331 acc_train: 0.6040 loss_val: 0.4908 acc_val: 0.8680 time: 0.4180s\n",
            "Epoch: 0880 loss_train: 1.3387 acc_train: 0.6253 loss_val: 0.4920 acc_val: 0.8800 time: 0.4288s\n",
            "Epoch: 0881 loss_train: 1.5569 acc_train: 0.5760 loss_val: 0.4924 acc_val: 0.8800 time: 0.4280s\n",
            "Epoch: 0882 loss_train: 1.4002 acc_train: 0.6093 loss_val: 0.4917 acc_val: 0.8800 time: 0.4245s\n",
            "Epoch: 0883 loss_train: 1.4463 acc_train: 0.6027 loss_val: 0.4914 acc_val: 0.8800 time: 0.4321s\n",
            "Epoch: 0884 loss_train: 1.2679 acc_train: 0.6587 loss_val: 0.4906 acc_val: 0.8640 time: 0.4236s\n",
            "Epoch: 0885 loss_train: 1.5083 acc_train: 0.5760 loss_val: 0.4911 acc_val: 0.8560 time: 0.4302s\n",
            "Epoch: 0886 loss_train: 1.4179 acc_train: 0.5960 loss_val: 0.4908 acc_val: 0.8560 time: 0.4799s\n",
            "Epoch: 0887 loss_train: 1.4530 acc_train: 0.5960 loss_val: 0.4908 acc_val: 0.8560 time: 0.5011s\n",
            "Epoch: 0888 loss_train: 1.3977 acc_train: 0.6200 loss_val: 0.4889 acc_val: 0.8640 time: 0.4250s\n",
            "Epoch: 0889 loss_train: 1.3536 acc_train: 0.6227 loss_val: 0.4858 acc_val: 0.8640 time: 0.4282s\n",
            "Epoch: 0890 loss_train: 1.4237 acc_train: 0.5987 loss_val: 0.4838 acc_val: 0.8640 time: 0.4205s\n",
            "Epoch: 0891 loss_train: 1.3732 acc_train: 0.6293 loss_val: 0.4817 acc_val: 0.8680 time: 0.4325s\n",
            "Epoch: 0892 loss_train: 1.5148 acc_train: 0.5867 loss_val: 0.4804 acc_val: 0.8760 time: 0.4325s\n",
            "Epoch: 0893 loss_train: 1.4859 acc_train: 0.5733 loss_val: 0.4792 acc_val: 0.8680 time: 0.4332s\n",
            "Epoch: 0894 loss_train: 1.3990 acc_train: 0.6227 loss_val: 0.4784 acc_val: 0.8680 time: 0.4222s\n",
            "Epoch: 0895 loss_train: 1.4115 acc_train: 0.6160 loss_val: 0.4789 acc_val: 0.8680 time: 0.4241s\n",
            "Epoch: 0896 loss_train: 1.4273 acc_train: 0.6000 loss_val: 0.4798 acc_val: 0.8680 time: 0.4313s\n",
            "Epoch: 0897 loss_train: 1.4443 acc_train: 0.6027 loss_val: 0.4807 acc_val: 0.8680 time: 0.4240s\n",
            "Epoch: 0898 loss_train: 1.3983 acc_train: 0.6080 loss_val: 0.4792 acc_val: 0.8680 time: 0.4206s\n",
            "Epoch: 0899 loss_train: 1.4381 acc_train: 0.6013 loss_val: 0.4777 acc_val: 0.8680 time: 0.4326s\n",
            "Epoch: 0900 loss_train: 1.4739 acc_train: 0.5880 loss_val: 0.4775 acc_val: 0.8680 time: 0.4308s\n",
            "Epoch: 0901 loss_train: 1.4727 acc_train: 0.5933 loss_val: 0.4778 acc_val: 0.8800 time: 0.4276s\n",
            "Epoch: 0902 loss_train: 1.4348 acc_train: 0.6013 loss_val: 0.4785 acc_val: 0.8800 time: 0.4287s\n",
            "Epoch: 0903 loss_train: 1.3447 acc_train: 0.6307 loss_val: 0.4795 acc_val: 0.8720 time: 0.4294s\n",
            "Epoch: 0904 loss_train: 1.4325 acc_train: 0.6093 loss_val: 0.4811 acc_val: 0.8640 time: 0.4356s\n",
            "Epoch: 0905 loss_train: 1.4091 acc_train: 0.6040 loss_val: 0.4850 acc_val: 0.8600 time: 0.4268s\n",
            "Epoch: 0906 loss_train: 1.4729 acc_train: 0.5893 loss_val: 0.4884 acc_val: 0.8520 time: 0.4192s\n",
            "Epoch: 0907 loss_train: 1.3940 acc_train: 0.6213 loss_val: 0.4893 acc_val: 0.8560 time: 0.4214s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 406.7883s\n",
            "Loading 806th epoch\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GAT(\n",
              "  (attention_0): GraphAttentionLayer (512 -> 8)\n",
              "  (attention_1): GraphAttentionLayer (512 -> 8)\n",
              "  (attention_2): GraphAttentionLayer (512 -> 8)\n",
              "  (attention_3): GraphAttentionLayer (512 -> 8)\n",
              "  (attention_4): GraphAttentionLayer (512 -> 8)\n",
              "  (attention_5): GraphAttentionLayer (512 -> 8)\n",
              "  (attention_6): GraphAttentionLayer (512 -> 8)\n",
              "  (attention_7): GraphAttentionLayer (512 -> 8)\n",
              "  (out_att): GraphAttentionLayer (64 -> 33)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    }
  ]
}